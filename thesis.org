#+LaTeX_CLASS: phd
#+LATEX_HEADER: \linespread{1.3}
#+LATEX_HEADER: \usepackage[top=0.75in, bottom=0.75in, left=1.5in, right=0.75in]{geometry}
#+LATEX_HEADER: \usepackage{etex}
#+LaTeX_CLASS_OPTIONS: [integrals, nointegrals,12pt]
#+OPTIONS: toc:nil
#+LATEX_HEADER: \usepackage{thesis-commands}
#+LATEX_HEADER: \usepackage{amsmath}
#+LATEX_HEADER: \usepackage{mathtools}
#+LATEX_HEADER: \usepackage{graphicx}
#+LATEX_HEADER: \usepackage{hyperref}
#+LATEX_HEADER: \usepackage[T1]{fontenc}
#+LATEX_HEADER: \usepackage[utf8]{inputenc}
#+LATEX_HEADER: \usepackage{lmodern}
#+LATEX_HEADER: \usepackage{fancyvrb}
#+LATEX_HEADER: \usepackage{tabularx}
#+LATEX_HEADER: \usepackage[normalem]{ulem}
#+LATEX_HEADER: \usepackage{theorem}
#+LATEX_HEADER: \usepackage{listings}
#+LATEX_HEADER: \usepackage{array}
#+LATEX_HEADER: \usepackage{caption}
#+LATEX_HEADER: \usepackage{subcaption}
#+LATEX_HEADER: \usepackage{msc/msc}
#+LATEX_HEADER: \usepackage{algorithm}
#+LATEX_HEADER: \usepackage[nounderscore]{syntax}
#+LATEX_HEADER: \usepackage[noend]{algpseudocode}
#+LATEX_HEADER: \usepackage[toc,page]{appendix}
#+LATEX_HEADER: \usepackage{multicol}
#+LATEX_HEADER: \usepackage{wasysym}
#+LATEX_HEADER: \usepackage{float}
#+LATEX_HEADER: \usepackage{multirow}
#+LATEX_HEADER: \usepackage{tikz}
#+LATEX_HEADER: \usepackage{pbox}
#+LATEX_HEADER: \usepackage{arydshln}
#+LATEX_HEADER: \usepackage{chngpage}
#+LATEX_HEADER: \setlength{\abovecaptionskip}{10pt plus 3pt minus 3pt}
#+LATEX_HEADER: \usetikzlibrary{arrows,shapes,shadows,tikzmark,calc}
#+LATEX_HEADER: \usetikzmarklibrary{listings}
#+LATEX_HEADER: \usepackage{rotating}
#+LATEX_HEADER: \degree{Doctor of Philosophy}
#+LATEX_HEADER: \department{Mathematical and Computer Sciences}
#+LATEX_HEADER: \gmonth{November}

#+TITLE: Reliable Massively Parallel Symbolic Computing: \linebreak /Fault Tolerance for a Distributed Haskell/
#+AUTHOR: Robert Stewart
#+OPTIONS: H:5

#+LATEX: {\small \begin{abstract}
As the number of cores in manycore systems grows exponentially, the
number of failures is also predicted to grow exponentially. Hence
massively parallel computations must be able to tolerate
faults. Moreover new approaches to language design and system
architecture are needed to address the resilience of massively
parallel heterogeneous architectures.

Symbolic computation has underpinned key advances in Mathematics and
Computer Science, for example in number theory, cryptography, and
coding theory. Computer algebra software systems facilitate symbolic
mathematics. Developing these at scale has its own distinctive set of
challenges, as symbolic algorithms tend to employ complex irregular
data and control structures. SymGridParII is a middleware for parallel
symbolic computing on massively parallel High Performance Computing
platforms. A key element of SymGridParII is a domain specific language
(DSL) called Haskell Distributed Parallel Haskell (HdpH). It is
explicitly designed for scalable distributed-memory parallelism, and
employs work stealing to load balance dynamically generated irregular
task sizes.

To investigate providing scalable fault tolerant symbolic computation
we design, implement and evaluate a reliable version of HdpH,
HdpH-RS. Its reliable scheduler detects and handles faults, using task
replication as a key recovery strategy. The scheduler supports load
balancing with a fault tolerant work stealing protocol. The reliable
scheduler is invoked with two fault tolerance primitives for implicit
and explicit work placement, and 10 fault tolerant parallel skeletons
that encapsulate common parallel programming patterns. The user is
oblivious to many failures, they are instead handled by the scheduler.

An operational semantics describes small-step reductions on states. A
simple abstract machine for scheduling transitions and task evaluation
is presented. It defines the semantics of supervised futures, and the
transition rules for recovering tasks in the presence of failure. The
transition rules are demonstrated with a fault-free execution, and
three executions that recover from faults.

The fault tolerant work stealing has been abstracted in to a Promela
model. The SPIN model checker is used to exhaustively search the
intersection of states in this automaton to validate a key resiliency
property of the protocol. It asserts that an initially empty
supervised future on the supervisor node will eventually be full in
the presence of all possible combinations of failures.

The performance of HdpH-RS is measured using five
benchmarks. Supervised scheduling achieves a speedup of 757 with
explicit task placement and 340 with lazy work stealing when executing
Summatory Liouville up to 1400 cores of a HPC architecture. Moreover,
supervision overheads are consistently low scaling up to 1400
cores. Low recovery overheads are observed in the presence of frequent
failure when lazy on-demand work stealing is used. A Chaos Monkey
mechanism has been developed for stress testing resiliency with random
failure combinations. All unit tests pass in the presence of random
failure, terminating with the expected results.
#+LATEX: \end{abstract} }

#+BEGIN_LATEX
\begin{dedication}
\vspace*{\fill}
\begin{center}
To Mum and Dad.
\end{center}
\vspace*{\fill}
\end{dedication}
#+END_LATEX

#+LATEX: \begin{acknowledgements}
Foremost, I would like to express my deepest thanks to my two
supervisors, Professor Phil Trinder and Dr Patrick Maier. Their
patience, encouragement, and immense knowledge were key motivations
throughout my PhD. They carry out their research with an objective and
principled approach to computer science. They persuasively conveyed an
interest in my work, and I am grateful for my inclusion in their
HPC-GAP project.

Phil has been my supervisor and guiding beacon through four years of
computer science MEng and PhD research. I am truly thankful for his
steadfast integrity, and selfless dedication to both my personal and
academic development. I cannot think of a better supervisor to
have. Patrick is a mentor and friend, from whom I have learnt the
vital skill of disciplined critical thinking. His forensic scrutiny of
my technical writing has been invaluable. He has always found the time
to propose consistently excellent improvements. I owe a great debt of
gratitude to Phil and Patrick.

I would like to thank Professor Greg Michaelson for offering thorough
and excellent feedback on an earlier version of this thesis. In
addition, a thank you to Dr Gudmund Grov. Gudmund gave feedback on
Chapter 4 of this thesis, and suggested generality improvements to my
model checking abstraction of HdpH-RS.

A special mention for Dr Edsko de Vries of Well Typed, for our
insightful and detailed discussions about network transport
design. Furthermore, Edsko engineered the network abstraction layer on
which the fault detecting component of HdpH-RS is built.

I thank the computing officers at Heriot-Watt University and the
Edinburgh Parallel Computing Centre for their support and hardware
access for the performance evaluation of HdpH-RS.
#+LATEX: \end{acknowledgements}

#+TOC: headlines 2

* Introduction
  
** Context                                                          

The manycore revolution is steadily increasing the performance and
size of massively parallel systems, to the point where system
reliability becomes a pressing concern. Therefore, massively parallel
compute jobs must be able to tolerate /failures/. Research in to High
Performance Computing (HPC) spans many areas including language
design and implementation, low latency network protocols and parallel
hardware. Popular languages for writing HPC
applications include Fortran or C with the Message
Passing Interface (MPI). New approaches to language design and system
architecture are needed to address the growing issue of massively
parallel heterogeneous architectures, where processing capability is
non-uniform and failure is increasingly common.

Symbolic computation has underpinned key advances in Mathematics and
Computer Science. Developing computer algebra systems at scale has its
own distinctive set of challenges, for example how to coordinate
symbolic applications that exhibit highly irregular parallelism. The
HPC-GAP project aims to coordinate symbolic computations in
architectures with $10^6$ cores \cite{hpc-gap}. At that scale, systems
are heterogeneous and exhibit non-uniform communication latency's, and
failures are a real issue. SymGridParII is a middleware that has been
designed for scaling computer algebra programs onto massively parallel
HPC platforms \cite{Maier_Stewart_Trinder_SAC2013}.

A core element of SymGridParII is a domain specific language (DSL)
called Haskell Distributed Parallel Haskell (HdpH). It supports both
implicit and explicit parallelism. The design of HdpH was informed by
the need for reliability, and the language has the potential for fault
tolerance. To investigate providing scalable fault tolerant symbolic
computation this thesis presents the design, implementation and
evaluation of a \textbf{R}eliable \textbf{S}cheduling version of HdpH,
HdpH-\textbf{RS}. It adds two new fault tolerant primitives and 10
fault tolerant algorithmic skeletons. A reliable scheduler has been
designed and implemented to support these primitives, and its
operational semantics are given. The SPIN model checker has been used
to verify a key fault tolerance property of the underlying work
stealing protocol.

** Contributions                                                    

The thesis makes the following research contributions:

1. *A critical review of fault tolerance in distributed
   systems*. This covers existing approaches to handling failures at
   various levels including fault tolerant communication layers,
   checkpointing and rollback, task and data replication, and fault
   tolerant algorithms (Chapter [[Related Work]]).

2. *A supervised workpool as a software reliability mechanism*
   \cite{DBLP:conf/sfp/StewartTM12}. The supervised fault tolerant
   workpool hides task scheduling, failure detection and task
   replication from the programmer. The fault detection and task
   replication techniques that support supervised workpools are
   prototypes for HdpH-RS mechanisms. Some benchmarks show low
   supervision overheads of between 2% and 7% in the absence of
   faults. Increased runtimes are between 8% and 10%, attributed to
   failure detection latency and task replication, when 1 node fails
   in a 10 node architecture (Appendix [[Supervised Workpools]]).

3. *The design of fault tolerant language HdpH extensions*. The
   HdpH-RS primitives ~supervisedSpawn~ and ~supervisedSpawnAt~
   provide fault tolerance by invoking supervised task
   scheduling. Their inception motivated the addition of ~spawn~ and
   ~spawnAt~ to HdpH \cite{comlan-special-issue}. The APIs of the
   original and fault tolerant primitives are identical, allowing the
   programmer to trivially opt-in to fault tolerant scheduling
   (Section [[HdpH-RS Programming Primitives]]).

4. *The design of a fault tolerant distributed scheduler*. To support
   the HdpH-RS primitives, a fault tolerant scheduler has been
   developed. The reliable scheduler algorithm is designed to support
   work stealing whilst tolerating random loss of single and
   simultaneous node failures. It supervises the location of
   supervised tasks, using replication as a recovery technique. Task
   replication is restricted to expressions with idempotent side
   effects i.e. side effects whose repetition cannot be
   observed. Failures are encapsulated with isolated heaps for each
   HdpH-RS node, so the loss of one node does not damage other nodes
   (Section [[Designing a Fault Tolerant Scheduler]]).

5. *An operational semantics for HdpH-RS*. The operational
   semantics for HdpH-RS extends that of HdpH, providing small-step
   reduction on states of a simple abstract machine. They provide a
   concise and unambiguous description of the scheduling transitions
   in the absence and presence of failure, and the states of
   supervised sparks and supervised futures. The transition rules are
   demonstrated with one fault-free execution, and three executions that
   recover and evaluate task replicas in the presence of
   faults (Section [[Operational Semantics]]).

6. *A validation of the fault tolerant distributed scheduler with the
   SPIN model checker*. The work stealing scheduling algorithm is
   abstracted in to a Promela model and is formally verified with the
   SPIN model checker. Whilst the model is an abstraction, it does
   model all failure combinations that may occur real architectures on
   which HdpH-RS could be deployed. The abstraction has an immortal
   supervising node and three mortal thieving nodes competing for a
   spark with the work stealing protocol. Any node holding a task
   replica can write to a future on the supervisor node. A key
   resiliency property of the model is expressed using linear temporal
   logic, stipulating that the initially empty supervised future on
   the supervisor node is eventually full despite node failures. The
   work stealing routines on the supervisor and three thieves are
   translated in to a finite automaton. The SPIN model checker is used
   to exhaustively search the model's state space to validate that the
   reliability property holds on all reachable states. This it does
   having searched approximately 8.22 million states of the HdpH-RS
   fishing protocol, at a reachable depth of 124 transitions (Chapter
   [[The Validation of Reliable Distributed Scheduling for HdpH-RS]]).

7. *The implementation of the HdpH-RS fault tolerant primitives and
   reliable scheduler*. The implementation of the spawn family of
   primitives and supervised futures are described. On top of the
   fault tolerant ~supervisedSpawn~ and ~supervisedSpawnAt~
   primitives, 10 algorithmic skeletons have been produced that provide
   high level fault tolerant parallel patterns of computation. All
   load-balancing and task recovery is hidden from the programmer. The
   fault tolerant spawn primitives honour the small-step operational
   semantics, and the reliable scheduler is an implementation of the
   verified Promela model. In extending HdpH, one module is added for
   the fault tolerant strategies, and 14 modules are modified. This
   amounts to an additional 1271 lines of Haskell code in HdpH-RS, an
   increase of 52%. The increase is attributed to fault detection,
   fault recovery and task supervision code (Chapter [[Implementing a
   Fault Tolerant Programming Language and Reliable Scheduler]]).

8. *An evaluation of fault tolerant scheduling performance*. The
   fault tolerant HdpH-RS primitives are used to implement five
   benchmarks. Runtimes and overheads are reported, both in the
   presence and absence of faults. The benchmarks are executed on a
   256 core Beowulf cluster \cite{Meredith:2003:EBC:767598.767641} and
   on 1400 cores of HECToR \cite{hector}, a national UK compute
   resource. The task supervision overheads are low at all scales up
   to 1400 cores. The scalability of the HdpH-RS scheduler design is
   demonstrated on massively parallel architectures using both flat
   and hierarchically nested supervision. Flat supervised scheduling
   achieves a speedup of 757 with explicit task placement and 340 with
   lazy work stealing when executing Summatory Liouville on HECToR
   using 1400 cores. Hierarchically nested supervised scheduling
   achieves a speedup of 89 with explicit task placement when
   executing Mandelbrot on HECToR using 560 cores.

   A Chaos Monkey failure injection mechanism \cite{chaos-monkey} is
   built-in to the reliable scheduler to simulate random node loss. A
   suite of eight unit tests are used to assess the resilience of
   HdpH-RS. All unit tests pass in the presence of random failures on
   the Beowulf cluster. Executions in the presence of random failure
   show that lazy on-demand scheduling is more suitable when failure
   is the common case, not the exception (Chapter [[Fault Tolerant
   Programming & Reliable Scheduling Evaluation]]).

9. *Other contributions* A new fault detecting transport layer has
   been implemented for HdpH and HdpH-RS. This was collaborative work
   with members of the Haskell community, including code and testing
   contributions of a new network transport API for distributed
   Haskells. (Section [[Fault Detecting Communications Layer]]). In the
   domain of reliable distributed computing, two related papers were
   produced. The first \cite{DBLP:conf/appt/StewartTL11} compares
   three high level MapReduce query languages for performance and
   expressivity. The other \cite{mr-vs-fj} compares the two
   programming models MapReduce and Fork/Join.

** Authorship & Collaboration                                        

*** Authorship

This thesis is closely based on the work reported in the following papers:

- *Supervised Workpools for Reliable Massively Parallel
  Computing* \cite{DBLP:conf/sfp/StewartTM12}. /Trends in
  Functional Programming/, 13th International Symposium, TFP 2012, St
  Andrews, UK. Springer. With Phil Trinder and Patrick Maier. This
  paper presents the supervised workpool described in Appendix
  [[Supervised Workpools]]. The fault detection and task replication
  techniques that support supervised workpools is a reliable
  computation prototype for HdpH-RS.
- *Reliable Scalable Symbolic Computation: The Design of
  SymGridPar2* \cite{Maier_Stewart_Trinder_SAC2013}. 28th /ACM
  Symposium On Applied Computing/, SAC 2013, Coimbra, Portugal. ACM
  Press. With Phil Trinder and Patrick Maier. The author contributed
  the fault detection, fault recovery and fault tolerant algorithmic
  skeleton designs for HdpH-RS. Supervision and recovery overheads for
  the Summatory Liouville application using the supervised workpool
  were presented.
- *Reliable Scalable Symbolic Computation: The Design of
  SymGridPar2* \cite{comlan-special-issue}. Submitted to Computer
  Languages, Systems and Structures. /Special Issue/. Revised Selected
  Papers from 28th ACM Symposium On Applied Computing 2013. With Phil
  Trinder and Patrick Maier. The SAC 2013 publication was extended
  with a more extensive discussion on SymGridParII fault
  tolerance. The HdpH-RS designs for task tracking, task duplication,
  simultaneous failure and a fault tolerant work stealing protocol
  were included.

Most of the work reported in the thesis is primarily my own, with
specific contributions as follows. The SPIN model checking in Chapter
[[The Validation of Reliable Distributed Scheduling for HdpH-RS]] is my
own work with some contribution from Gudmund Grov. The HdpH-RS
operational semantics in Chapter [[Designing a Fault Tolerant
Programming Language for Distributed Memory Scheduling]] extends the
HdpH operational semantics developed by Patrick Maier.

*** Collaboration

Throughout the work undertaken for this thesis, the author
collaborated with numerous development communities, in addition to the
the HPC-GAP project team \cite{hpc-gap}.

- /Collaboration with Edsko De Vries, Duncan Coutts and Jeff Epstein/
  on the development of a network abstraction layer for
  distributed Haskells \cite{network-transport}. The failure semantics
  for this transport layer were discussed \cite{nt-discussion}, and
  HdpH-RS was used as the first real-world case study to uncover and
  resolve numerous race conditions \cite{nt-tcp-fix} in the TCP
  implementation of the transport API.
- /Collaboration with Scott Atchley/, the technical lead on the Common
  Communications Interface (CCI). The author explored the adoption of
  the TCP implementation of CCI for HdpH-RS. This work uncovered a bug
  in the CCI TCP implementation \cite{cci-tcp-fix} that was later fixed.
- /Collaboration with Tim Watson/, the developer of the CloudHaskell
  Platform, which aims to mirror Erlang OTP in CloudHaskell. The
  author uncovered a bug in the ~Async~ API, and provided a test case
  \cite{chp-async-fix}.
- /Collaboration with Morten Olsen Lysgaard/ on a distributed hash
  table (DHT) for CloudHaskell. The author ported this DHT to
  CloudHaskell 2.0 (Section [[CloudHaskell 2.0]]) \cite{sirkel-port}.
- /Collaboration with Ryan Newton/ on the testing of a
  ChaseLev \cite{DBLP:conf/spaa/ChaseL05} work stealing deque for
  Haskell, developed by Ryan and Edward Kmett. The author uncovered a
  bug \cite{atomic-primops-bug} in the ~atomic-primops~ library
  \cite{atomic-primops} when used with TemplateHaskell for explicit
  closure creation in HdpH. This was identified as a GHC bug, and was
  fixed in the GHC 7.8 release \cite{ghc-atomics}.

* Related Work

This chapter introduces dependable distributed system concepts, fault
tolerance and causes of failures. Fault tolerance has been built-in to
different domains of distributed computing, including cloud computing,
high performance computing, and mobile computing.

Section [[Dependability of Distributed Systems]] classifies the types of
dependable systems and the trade-offs between availability and
performance. Section [[Fault Tolerance]] outlines a terminology of
reliability and fault tolerance concepts that is adopted throughout
the thesis. It begins with failure forecasts as architecture trends
illustrate a growing need for tolerating faults. Existing fault
tolerant mechanisms are described, followed by a summary of a well
known implementation of each (Section [[Classifications of Fault
Tolerance Implementations]]). Most existing fault tolerant approaches in
distributed architectures follow a checkpointing and rollback-recovery
approach, and new opportunities are being explored as alternative and
more scalable possibilities.

This chapter ends with a review of distributed Haskell technologies,
setting the context for the HdpH-RS design and implementation. Section
[[CloudHaskell]] introduces CloudHaskell, a domain specific language for
distributed programming in Haskell. Section [[SymGridParII]] introduces
HdpH in detail, the realisation of SymGridParII --- a middleware for
parallel distributed computing.


** Dependability of Distributed Systems


#+BEGIN_QUOTE
"Dependability is defined as that property of a computer system
such that reliance can justifiably be placed on the service it
delivers. A given system, operating in some particular environment,
may fail in the sense that some other system makes, or could in
principle have made, a judgement that the activity or
inactivity of the given system constitutes failure."
\cite{DBLP:conf/compsac/Laprie04}
#+END_QUOTE


*** Distributed Systems Terminology

Developing a dependable computing system calls for a combined
utilisation of methods that can be classified in to 4 distinct areas
\cite{Laprie85}. Fault /avoidance/ is the prevention of fault
occurrence. Fault /tolerance/ provides a service in spite of faults
having occurred. The /removal/ of errors minimises
the presence of latent errors. Errors can be /forecast/ through
estimating the presence, the creation, and the consequences of errors.

Definitions for availability, reliability, safety and maintainability
are given in \cite{DBLP:conf/compsac/Laprie04}. /Availability/ is the
probability that a system will be operational and able to deliver the
requested services at any given time . The /reliability/ of a system
is the probability of failure-free operation over a time period in a
given environment. The /safety/ of a system is a judgement of the
likelihood that the system will cause damage to people or its
environment. /Maintainable/ systems can be adapted economically to
cope with new requirements, with minimal infringement on the
reliability of the system. /Dependability/ is the ability to avoid
failures that are more frequent and more severe than is acceptable.

*** Dependable Systems

**** Highly Available Cloud Computing

Cloud computing service providers allow users to rent virtual
computers on which to run their own software applications. High
availability is achieved with the redundancy of virtual machines
hosted on commodity hardware. Cloud computing service quality is
promised by providers with /service level agreements/ (SLA). An SLA
specifies the availability level that is guaranteed and the penalties
that the provider will suffer if the SLA is violated. Amazon Elastic
Compute Cloud (EC2) is a popular cloud computing provider. The EC2 SLA
is:

#+BEGIN_QUOTE
AWS will use commercially reasonable efforts to make Amazon EC2
available with an Annual Uptime Percentage of at least 99.95% during
the Service Year. In the event Amazon EC2 does not meet the Annual
Uptime Percentage commitment, you will be eligible to receive a
Service Credit. \cite{amazon-ec2-sla}
#+END_QUOTE

**** Fault Tolerant Critical Systems

Failure occurrence in critical systems can result in significant
economic losses, physical damage or threats to human life
\cite{Sommerville10}. The failure in a /mission-critical/ system may
result in the failure of some goal-directed activity, such as a
navigational system for aircraft. A /business-critical/ system failure
may result in very high costs for the business using that system, such
as a computerised accounting system.

**** Dependable High Performance Computing

Future HPC architectures will require the simultaneous use and control
of millions of processing, storage and networking elements. The
success of massively parallel computing will depend on the ability to
provide reliability and availability at scale \cite{schroeder2007}. As
HPC systems continue to increase in scale, their mean time between
failure (MTBF, described in Section [[Attributes of Faults]]) decreases
respectively. The message passing interface (MPI) is the defacto
message passing library in HPC applications. These two trends have
motivated work on fault tolerant MPI implementations
\cite{DBLP:journals/ijhpca/BouteillerHKLC06}. MPI provides a rigid
fault model in which a process fault within a communication group
imposes failure to all processes within that communication group. An
active research area is fault tolerant MPI programming (Section [[Fault
Tolerant MPI]]), though that work has yet to be adopted by the broader
HPC community \cite{DBLP:conf/hoti/AtchleyDSGSBM11}.

The current state of practise for fault tolerance in HPC systems is
checkpointing and rollback (Section [[Fault Tolerance Mechanisms]]). With
the increasing error rates and increasing aggregate memory leaving
behind I/O capabilities, the checkpointing approach is becoming less
efficient \cite{litvinova10proactive}. Proactive fault tolerance
avoids failures through preventative measures, such as by migrating
processes away from nodes that are about to fail. A proactive
framework is described in \cite{litvinova10proactive}. It uses
environmental monitoring, event logging and resource monitoring to
analyse HPC system reliability and avoids faults through preventative
actions.

The need for HPC is no longer restricted to numerical computations
such as multiplying huge matrices filled with floating point
numbers. Many problems from the field of /symbolic computing/ can only
be tackled with the power provided by parallel computers
\cite{DBLP:conf/pdpta/BlochingerBH00}. In addition to the complexities
of irregular parallelism in symbolic computing (Section
[[SymGridParII]]), these applications often face extremely long runtimes
on HPC platforms.  So fault tolerance measures also need to be taken
in large scale symbolic computation frameworks.

** Fault Tolerance

*** Fault Tolerance Terminology

**** Attributes of Failures

The distinction between /failures/, /errors/ and /faults/ are made in
\cite{avizienis2000}. These three aspects of fault tolerance construct
a /fundamental chain/ \cite{avizienis2000}, shown in Figure
\ref{fig:fundamental-chain}. In this chain, a /failure/ is an event
that occurs when the system does not deliver a service as expected by
its users. A /fault/ is a characteristic of software that can lead to
a system error. An /error/ can lead to an erroneous system state
giving a system behaviour that is unexpected by system users.

#+CAPTION:    Fundamental Chain
#+LABEL:      fig:fundamental-chain
#+ATTR_LaTeX: :width 90mm
[[./img/chp2/intro/fundamental-chain.pdf]]

Dependability is grouped into three classes in \cite{avizienis2000},
shown in Figure \ref{fig:dependability-tree}. The /impairments/ to
dependability are undesired, but not unexpected. The /means/ of
dependability are the techniques for providing the ability to deliver
a service, and to reach confidence in this ability. The /attributes/
enable the properties which are expected from the system, and allow
the system quality to be addressed.

#+CAPTION:    Dependability Tree
#+LABEL:      fig:dependability-tree
#+ATTR_LaTeX: :width 70mm
[[./img/chp2/intro/dependability_tree.pdf]]

**** Attributes of Faults

A fault tolerant system detects and manages faults in such a
way that system failure does not
occur. Fault /recovery/ mechanisms (Section [[Fault Recovery]]) enable a
system to restore its state to a known safe state. This may be
achieved by correcting the damaged state with forward error recovery
or restoring the system to a previous state using
backward error recovery.

The use of verification techniques can be used for fault
/detection/. Fault detection mechanisms deal with either
/preventative/ and /retrospective/ faults. An example of a
preventative approach is to initialise fault detection prior to
committing a state change. If a potentially erroneous state is
detected, the state change is not committed. In contrast, a
retrospective approach initialises fault detection /after/ the system
state has changed, to check whether a fault has occurred. If a fault
is discovered, an exception is signalled and a repair mechanism is
used to recover from the fault.

Faults can occur for many reasons as shown in Figure
\ref{fig:fault-classes}. Faults can occur due to improper software
techniques, or development incompetence. This includes man made
/phenomenological causes/, and development or operational faults
during creation. The /capability/ or /capacity/ of a system may be the
cause of faults, e.g. an issue with memory management internally or a
lack of capacity for persistent storage.

#+CAPTION:    Elementary Fault Classes
#+LABEL:      fig:fault-classes
#+ATTR_LaTeX: :width 70mm
[[./img/chp2/intro/fault_classes.pdf]]

The availability of a system can be calculated as the probability that
it will provide the specified services within required bounds over a
specific time interval. A widely used calculation can be used to
derive steady-state availability of a system. The mean time between
failures (MTBF) and mean time to repair (MTTR) value are used to
derive steady-state availability of a system as $\alpha =
\frac{MTBF}{MTBF + MTTR}$. Non-repairable systems can fail only
once. In systems that do not recover from faults another measure is
used, mean time to failure (MTTF), which is the expected time to the
failure of a system.


*** Failure Rates

Unfortunately, obtaining access to failure data from modern
large-scale systems is difficult, since such data is often sensitive
or classified \cite{DBLP:conf/dsn/SchroederG06}. Existing studies of
failure are often based on only a few months of data
\cite{DBLP:conf/prdc/XuKI99}, and many commonly cited studies on
failure analysis stem from the early 1990's, when computer systems
were significantly different from today
\cite{DBLP:conf/srds/Gray86}. Failure root causes fall in one of the
following five high-level categories: /human/ error; /environmental/
e.g. power outages or A/C failures; /network/ failure, /software/
failure, and /hardware/ failure \cite{DBLP:conf/dsn/SchroederG06}.

**** Datasheet and Field Study Data Discrepancies

Studies have been carried out to calculate the MTTF values for
specific hardware components. As an example, the MTTF for CPU chips
have been calculated at 7 years in
\cite{DBLP:journals/micro/SrinivasanABR05}, and at 11 years in
\cite{wojciechowski2011thermal}. The MTTF for permanent hard drive
failure is investigated in
\cite{Schroeder:2007:DFR:1267903.1267904}. It compares discrepancies
between the datasheet reports for enterprise disks and actual failure
logs from field studies in HPC and internet service provider
clusters. The authors conclude that large-scale installation field
usage differs widely from nominal datasheet MTTF conditions. In
particular for five to eight year old drives, field replacement rates
were a factor of 30 more than the MTTF datasheet had suggested. The
study also reports the relative frequency of failure
across all hardware components in HPC and commodity-off-the-shelf
(COTS) environments from their field studies, and the results are in Table
\ref{tab:hpc-com-failure}.

#+BEGIN_LATEX
\begin{table}\small
\begin{center}
\begin{tabular}{|r|c|c||c|c|}
\hline
\multirow{2}{*}{Rank} & \multicolumn{2}{c||}{HPC} & \multicolumn{2}{c|}{COTS} \\
\cline{2-5}
 & Component & \% & Component & \% \\
\hline
\hline
1 & Hard drive & 30.6 & Power supply & 34.8 \\
2 & Memory          & 28.5 & Memory         & 20.1 \\
3 & Misc/Unk        & 14.4 & Hard drive     & 18.1 \\
4 & CPU             & 12.4 & Case           & 11.4 \\
5 & PCI motherboard &  4.9 & Fan            &  8.0 \\
6 & Controller      &  2.9 & CPU            &  2.0 \\
7 & QSW             &  1.7 & SCSI board     &  0.6 \\
8 & Power supply     &  1.6 & NIC card       &  1.2 \\
9 & MLB             &  1.0 & LV power board &  0.6 \\
10 & SCSI BP         &  0.3 & CPU heatsink   &  0.6 \\
\hline
\end{tabular}
\end{center}
\caption{Relative frequency of hardware component failure that required replacement}
\label{tab:hpc-com-failure}
\end{table}
#+END_LATEX

**** HECToR Case Study

HECToR (High End Computing Terascale Resource) \cite{hector} is a national
high-performance computing service for the UK academic community. The
service began in 2008, and is expected to operate
until 2014. An evaluation of executing HdpH-RS benchmarks on HECToR
are in Section [[Performance With No Failure]]. As of August 2013, it had
a theoretical capacity of over 800 Teraflops per second, and over
90,112 processing cores.

Monthly, quarterly and annual HECToR reports are published, detailing
time usage, disk usage, failures, MTBF and performance metrics. The
following statistics are published in the annual report from January
2012 to January 2013 \cite{hector-report-2012}. The overall MTBF was
732 hours, compared with 586 hours in 2011. There were 12
technology-attributed service failures. This comprised 4 instances
of late return to service following scheduled maintenance sessions, 3
PBS batch subsystem related failures, 1 cabinet blower failure, 1
Lustre filesystem failure, 2 HSN failures, and 1 acceptance testing
overrun. There were 166 single node failures over the 12 months. The
peak was 29 in October 2012, due to a software bug.


*** Fault Tolerance Mechanisms

There are many fault tolerance approaches in HPC and COTS
environments. Faults can be detected, recovered from, and prevented
(Section [[Fault Prevention]]). The tactics for fault tolerance detection,
recovery and prevention are shown in Figure
\ref{fig:availability-tactics} \cite{scott2009}.

#+CAPTION:    Availability Tactics
#+LABEL:      fig:availability-tactics
#+ATTR_LaTeX: :height 70mm :width 110mm
[[./img/chp2/availability_tactics/availability_tactics.pdf]]

**** Fault Detectors

A simple protocol for detecting failure is /ping-pong/. The messages
in a ping-pong are shown in Figure
\ref{fig:pingpong-protocol}. Process $P_i$ pings $P_j$ once every T
time units, and $P_j$ replies each time with a pong message. The
failure detection time of a failure is $2T$.

#+LATEX: \hspace{-1.5cm}
#+LATEX: \begin{minipage}{1.1\linewidth}
#+LATEX: \begin{minipage}{0.33\linewidth}

#+CAPTION:    Ping-Pong
#+LABEL:      fig:pingpong-protocol
#+ATTR_LATEX: :placement [H] :height 10mm
[[./img/chp2/failure_detectors/pingpong.pdf]]

#+LATEX: \end{minipage}
#+LATEX: \hspace{-0.6cm}
#+LATEX: \begin{minipage}{0.33\linewidth}

#+CAPTION:    Passive Heartbeats
#+LABEL:      fig:passive-heartbeat-protocol
#+ATTR_LaTeX: :placement [H] :height 10mm
[[./img/chp2/failure_detectors/passive-heartbeat.pdf]]

#+LATEX: \end{minipage}
#+LATEX: \hspace{-0.4cm}
#+LATEX: \begin{minipage}{0.33\linewidth}

#+CAPTION:    Active Heartbeats
#+LABEL:      fig:active-heartbeat-protocol
#+ATTR_LaTeX: :placement [H] :height 10mm
[[./img/chp2/failure_detectors/active-heartbeat.pdf]]

#+LATEX: \end{minipage}
#+LATEX: \end{minipage}

\vspace{0.6cm}

An alternative is the /heartbeat/ protocol. A /passive/ heartbeat
protocol is shown in Figure \ref{fig:passive-heartbeat-protocol}. In this
illustration, $P_j$ sends a heartbeat to $P_i$ every time unit
$T$. The heartbeat contains a sequence number, which is incremented
each time. When process $P_i$ receives a heartbeat message, it resets
a timer that ticks after $T$ time units. A latency delay $\alpha$ for
heartbeat arrival is tolerated. If a heartbeat is not receive within a
period of $T + \alpha$, $P_j$ is assumed to have failed. /Active/
heartbeats is a variant, shown in Figure
\ref{fig:active-heartbeat-protocol}. It can be used in connection oriented
transport implementations, when unsuccessful transmission attempts
throw exceptions.

/Monitors/ are components that can monitor many parts of a system,
such as processors, nodes, and network congestion. They may use
heartbeat or ping-pong protocols to monitor remote components in
distributed systems. In message passing distributed systems,
/timestamps/ can be used to detect or re-order incorrect event
sequences. The correctness of programs can be recorded using
/condition/ monitoring, such as computing checksums, or validating
assumptions made during designs processes.

**** Fault Recovery

***** Replication

Replication is a common fault tolerance recovery strategy (Section
[[Classifications of Fault Tolerance Implementations]]).

#+BEGIN_QUOTE
"Replication is a primary means of achieving high
availability in fault-tolerant distributed systems. Multicast or group
communication is a useful tool for expressing replicated algorithms
and constructing highly available systems. But programming language
support for replication and group communication is uncommon."
\cite{Cooper:1990:PLS:504136.504162}.
#+END_QUOTE

The authors of \cite{DBLP:conf/mascots/AguilarH00} define three
replication strategies: /passive, semi-active/ and /active/. In the
/passive/ approach, the replica task is a backup in case failure
occurs. The regular task is executed, and only if fails is the
corresponding replica scheduled for re-execution. The /semi-active/
approach /eagerly/ races the execution of both the regular and replica
tasks. The same is true for /active/ replication, though this time
both results are considered to ensure the results are equal. This is
more costly than the /semi-active/ approach, as consensus (Section
[[Distributed Algorithms for Reliable Computing]]) is required to accept a
final value.

A well known use of replication is in MapReduce frameworks (Section
[[MapReduce]]), such as Hadoop \cite{DBLP:books/daglib/0029284}. It
replicates tasks over a distributed runtime system, and data chunks
over a distributed filesystem. Data replication for fault tolerance is
also common in NoSQL databases and distributed hash tables (Section
[[Distributed Datastores]]).

***** Rollback and Checkpointing

Most existing work in fault tolerance for HPC systems is based on
checkpointing and rollback recovery. Checkpointing methods
\cite{DBLP:journals/tocs/ChandyL85} are based on periodically saving a
global or semi-global state to stable storage.

There are two broad distinctions --- /global synchronous/
checkpointing systems, and local or /semi-global asynchronous/
mechanisms. Global checkpointing simplifies the requirement of
satisfying safety guarantees, but is not a scalable
solution. Asynchronous checkpointing approaches have potential to
scale on larger systems, though encounter difficult challenges such
rollback propagation, domino effects , and loss of integrity through
incorrect rollback in dependency graphs
\cite{DBLP:journals/csur/ElnozahyAWJ02}.

Prior work on checkpointing storage for HPC has focused on two
areas. The first is node local checkpointing storage, and the second
involves centralised techniques that focus on high-performance
parallel systems \cite{node-local-storage}. The bandwidth between I/O
nodes in distributed systems is often regarded as the bottleneck in
distributed checkpointing.

Rollback recovery \cite{DBLP:journals/csur/ElnozahyAWJ02} is used when
system availability requirements can tolerate the outage of computing
systems during recovery. It offers a resource efficient way of
tolerating failure, compared to other techniques such as replication
or transaction processing \cite{DBLP:journals/tdsc/ElnozahyP04}. In
message passing distributed systems, messages induce interprocess
dependencies during failure-free operation. Upon process failure,
these dependencies may force some of the processes that did not fail
to roll back to a rollback line, which is called /rollback
propagation/ \cite{DBLP:journals/csur/ElnozahyAWJ02}. Once a good
state is reached, process execution resumes.

/Log/ based rollback-recovery \cite{DBLP:conf/ftcs/StromBY88} combines
checkpointing and logging to enable processes to replay their
execution after a failure beyond the most recent checkpoint. Log-based
recovery generally is not susceptible to the domino effect, thereby
allowing the processes to use uncoordinated checkpointing if
desired. Three main techniques of logging are /optimistic logging/,
/causal logging/, and /pessimistic logging/. /Optimistic/ logging
assumes that messages are logged, but part of these logs can be lost
when a fault occurs. Systems that implement this approach use either a
global coherent checkpoint to rollback the entire application, or they
assume a small number of fault at one time in the system. /Causal/
logging is an optimistic approach, checking and building an event
dependency graph to ensure that potential incoherence in the
checkpoint will not appear.  All processes record their "happened
before" activities in an antecedence graph, in addition to the logging
of sent messages. Antecedence graphs are asynchronously sent to a
stable storage with checkpoints and message logs. When failures occur,
all processes are rolled back to their last checkpoints, using
antecedence graphs and message logs. Lastly, /pessimistic/ logging is
a transaction log ensuring that no incoherent state can be reached
starting from a local checkpoint of processes, even with an unbounded
number of faults.

***** Other Mechanisms

Degrading operations \cite{Avizienis:1975:FFC:800027.808469} is a
tactic that suspends non-critical components in order to keep alive
the critical aspects of system functionality. Degradation can reduce
or eliminate fault occurrence by gracefully reducing system
functionality, rather than causing a complete system
failure. Shadowing \cite{DBLP:conf/vldb/BittonG88} is used when a
previously failed component attempts to rejoin a system. This
component will operate in a /shadow mode/ for a period of time, during
which its behaviour will be monitored for correctness and will
repopulate its state incrementally as confidence of its reliability
increases.

Retry tactics \cite{DBLP:journals/computer/GrottkeT07} can be an
effective way to recover from transient failures --- simply retrying
a failed operation may lead to success. Retry strategies have been
added to networking protocols in Section [[Fault Tolerant Networking
Protocols]], and also to language libraries such as the ~gen_server~
abstraction for Erlang in Section [[Erlang]].

**** Fault Prevention

The prevention of faults is a tactic to avoid or minimise fault
occurrence. The component removal tactic
\cite{DBLP:conf/woss/GeorgiadisMK02} involves the precautionary
removal of a component, /before/ failure is detected on it. This may
involve resetting a node or a network switch, in order to scrub latent
faults such as memory leaks, before the accumulation of faults amounts
to failure. This tactic is sometimes referred to as software
rejuvenation \cite{DBLP:conf/ftcs/HuangKKF95}.

Predictive modeling \cite{DBLP:conf/dsn/LiangZSJS06} is often used
with monitors (Section [[Fault Tolerance Mechanisms]]) to gauge the health
of components or to ensure that the system is operating within its
nominal operating parameters such as the load on processors or memory,
message queue size, and latency of ping-ack responses (Section [[Fault
Detectors]]). As an example, most motherboards contain temperature
sensors, which can be accessed via interfaces like ACPI
\cite{acpi00specification}, for monitoring nominal operating
parameters. Predictive modeling has been used in a fault tolerant MPI
\cite{DBLP:conf/hipc/ChakravortyMK06}, which proactively migrates
execution from nodes when it is experiencing intermittent failure.

*** Software Based Fault Tolerance

**** Distributed Algorithms for Reliable Computing

Algorithmic level fault tolerance is a high level fault tolerance
approach. Classic examples include leader election algorithms,
consensus through voting and quorums, and smoothing results with
probabilistic accuracy bounds.

Leader election algorithms are used to overcome the problem of crashes
and link failures in both synchronous and asynchronous distributed systems
\cite{DBLP:journals/tc/Garcia-Molina82}. They can be used in systems
that must overcome master node failure in master/slave
architectures. If a previously elected leader fails, a new election
procedure can unilaterally identify a new leader.

Consensus is the agreement of a system
status by the fault-free segment of a process population in spite of
the possible inadvertent or even malicious spread of disinformation by
the faulty segment of that population
\cite{DBLP:journals/csur/BarborakM93}. Processes propose
values, and they all eventually agree on one among these values. This
problem is at the core of protocols that handle synchronisation,
atomic commits, total order broadcasting, and replicated file systems.

Using quorums \cite{DBLP:journals/dc/MalkhiR98} is one way to enhance
the availability and efficiency of replicated data structures. Each
quorum can operate on behalf of the system to increase its
availability and performance, while an intersection property
guarantees that operations done on distinct quorum preserve
consistency \cite{DBLP:journals/dc/MalkhiR98}.

Probabilistic accuracy bounds are used to specify approximation limits
on smoothing results when some tasks have been lost due to partial
failure. A technique presented in \cite{DBLP:conf/ics/Rinard06}
enables computations to survive errors and faults while providing a
bound on any resulting output distortion. The fault tolerant approach
here is simple: tasks that encounter faults are discarded. By
providing probabilistic accuracy bounds on the distortion of the
output, the model allows users to confidently accept results in the
presence of failure, provided the distortion falls with acceptable
bounds.

A non-masking approach to fault tolerance is Algorithm Based Fault
Tolerance (ABFT) \cite{Kuang-Hua-Huang:1984:AFT:1310169.1310842}. The
approach consists of computing on data that is encoded with some level of
redundancy. If the encoded results drawn from successful computation
has enough redundancy, it remains possible to reconstruct the missing
parts of the results. The application of this technique is mainly
based on the use of parity checksum codes, and is widely used in HPC
platforms \cite{InProceedingsRoche.RRC_aftatp_09}.

#+BEGIN_QUOTE
"A system is self-stabilising when, regardless of its initial state, it 
is guaranteed to arrive at a /legitimate state/ in a finite number of 
steps." -- Edsger W. Dijkstra \cite{Dijkstra:1974:SSS:361179.361202}
#+END_QUOTE

Self stabilisation \cite{DBLP:journals/csur/Schneider93} provides a
non-masking approach to fault tolerance. A self stabilising algorithm
converges to some predefined set of /legitimate states/ regardless of
its initial state. Due to this property, self-stabilising algorithms
provide means for tolerating transient faults
\cite{DBLP:conf/podc/GhoshGHP96}. Self-stabilising algorithms such as
\cite{Dijkstra:1974:SSS:361179.361202} use forward recovery
strategies. That is, instead of being externally stopped and
rolled-back to a previous correct state, the algorithm continues its
execution despite the presence of faults, until the algorithm corrects
itself without external influence.

**** Fault Tolerant Programming Libraries

In the 1980's, programmers often had few alternatives faced with
choosing a programming language for writing fault tolerant distributed
software \cite{DBLP:journals/spe/Andrews82a}. At one end of the
spectrum were relatively low-level choices such as C, coupled with a
fault tolerance library such as ISIS
\cite{DBLP:conf/sosp/Birman85}. The ISIS system transforms abstract
type specifications into fault tolerance distributed implementations,
while insulating users from the mechanisms used to achieve fault
tolerance. The system itself is based on a small set of communication
primitives. Whilst such applications may have enjoyed efficient
runtime speeds, the approach forced the programmer to deal with the
complexities of distributed execution and fault tolerance in a
language that is fundamentally sequential.

At the other end of the spectrum were high-level languages
specifically intended for constructing fault tolerant application
using a given technique, such as Argus
\cite{DBLP:conf/ac/Liskov84}. These languages simplified the problems
of faults considerably, yet could be overly constraining if the programmer
wanted to use fault tolerance techniques other than the one supported
by the language.

Another approach is to add language extensions to support fault
tolerance. An example is FT-SR
\cite{Schlichting95programminglanguage}, an augmentation of the
general high-level distributed programming language SR
\cite{DBLP:journals/spe/Andrews82a}, augmented with fault tolerance
mechanisms. These include replication, recovery and failure
notification. FT-SR was implemented using the x-kernel
\cite{Hutchinson91thex-kernel:}, an Operating System designed for
experimenting with communication protocols.

** Classifications of Fault Tolerance Implementations

This section describes examples of fault tolerant distributed software
systems. This begins with a case study of a reliability extension to a
symbolic computation middleware in Section [[Fault Tolerance for DOTS
Middleware]]. The fault tolerance of the MapReduce programming model is
described in Section [[MapReduce]]. A discussion on fault tolerant
networking protocols is in Section [[Fault Tolerant Networking
Protocols]]. As MPI is prominently the current defacto standard for
message passing in High Performance Computing, Section [[Fault Tolerant
MPI]] details numerous fault tolerant MPI implementations. Supervision
and fault recovery tactics in HdpH-RS are influenced by Erlang,
described in Sections [[Erlang]] and [[Process Supervision in Erlang OTP]].

*** Fault Tolerance for DOTS Middleware

A reliability extension to the Distributed Object-Oriented Threads
System (DOTS) symbolic computation framework \cite{Blochinger99a.:an}
is presented in \cite{DBLP:conf/pdpta/BlochingerBH00}. DOTS was
originally intended for the parallelisation of application belonging
to the field of symbolic computation, but it has also been
successfully used in other application domains such as computer graphs
and computational number theory.

**** Programming Model

DOTS provides object-oriented asynchronous remote procedure calls
services accessible from C++ through a /fork/join/ programming API,
which also supports thread cancellation. Additionally, it supports
object-serialisation for parameter passing. The programming model
provides a uniform and high-level programming paradigm over
hierarchical multiprocessor systems. Low level details such as message
passing and shared memory threads within a single parallel application
are masked from the programmer
\cite{DBLP:conf/pdpta/BlochingerBH00}.

**** Fault Tolerance

The fault tolerance in the reliable extension to DOTS is achieved
through asynchronous checkpointing, realised by two orthogonal
approaches. The first is /implicit/ checkpointing. It is completely
hidden from the programmer, and no application code
modifications are necessary. The second is /explicit/ checkpointing by
adding three new API calls, allowing the programmer to explicitly
control the granularity needs of the application.

To realise the /implicit/ checkpointing approach, /function
memoization/ \cite{DBLP:conf/asplos/SchnarrL98} is used. A checkpoint
consists of a pair containing the argument and the computed result of
a thread --- whenever a thread has successfully finished its
execution, a checkpoint is taken. The checkpointed data is stored in a
/history table/. In the case of a restart of a thread, a replay of
communication and computation takes place. If the argument of the
thread is present in the history table, the result is taken from the
table and immediately sent back to the caller. This strategy is
feasible whenever the forked function is stateless.

*** MapReduce

MapReduce is a programming model and an associated implementation for
processing and generating large data sets
\cite{DBLP:journals/cacm/DeanG08}. Programs written in the functional
style are automatically parallelisable and can be run on large
clusters. Its fault tolerance model make implementations such as
Hadoop \cite{DBLP:books/daglib/0029284} a popular choice for running
on COTS architectures, where failure is more common than on HPC
architectures. The use of task re-execution (Section [[Replication]]) is
the fault tolerance mechanism. Programmers can adopt the MapReduce
model directly, or can use higher level data query languages
\cite{DBLP:conf/appt/StewartTL11}.

The MapReduce architecture involves one /master/ node, and all other
nodes are slave nodes. They are responsible both for executing tasks,
and hosting chunks of the MapReduce distributed filesystem. Fault
tolerance is achieved by replicating tasks and distributed filesystem
chunks, providing both task parallel and data parallel redundancy. A
comparison between Hadoop and HdpH-RS is made in Section [[Hadoop]].

*** Distributed Datastores

Distributed data structures such as distributed hash tables
\cite{DBLP:conf/iptps/ByersCM03} and
NoSQL databases \cite{Stonebraker:2010:SDV:1721654.1721659} often
provide flexible performance and fault tolerance
parameters. Optimising one parameter tends to put pressures on
others. Brewer's CAP theorem \cite{DBLP:conf/podc/Brewer00} states a
trade off between /consistency/, /availability/ and /partition
tolerance/ --- a user can only have two out of three. An important
property of a DHT is a degree of their /fault tolerance/, which is the
fraction of nodes that can fail without eliminating data or preventing
successful routing \cite{DBLP:conf/iptps/KaashoekK03}.

NoSQL databases do not require fixed table schemas, and are designed
to scale horizontally and manage huge amounts of data. One of the
first NoSQL databases was Google's proprietary BigTable
\cite{DBLP:conf/osdi/ChangDGHWBCFG06}, and subsequent open source
NoSQL databases have emerged including Riak \cite{riak}. Riak is a
scalable, highly-available, distributed key/value store built using
Erlang/OTP (Section [[Process Supervision in Erlang OTP]]). It is
resilient to failure through a quorum based eventual consistency
model, based on Amazon's Dynamo paper
\cite{DBLP:conf/sosp/DeCandiaHJKLPSVV07}.



*** Fault Tolerant Networking Protocols

There are many APIs for connecting and exchanging data between network
peers. In 1981, the Internet Protocol (IP) \cite{IP81} began the era
of ubiquitous networking. When processes on two different computer
communicate, the most often do so using the TCP protocol
\cite{DBLP:books/aw/Stevens94}.  It offers a convenient bi-directional
bytestream interface for communication. It also hides most
communication problems from the programmer, such as message losses and
message duplicates, overcome using sequence numbers and
acknowledgements.

TCP/IP uses the sockets interfaces supported by many Operating
Systems. It is the widely used protocol that
Internet services rely upon. It has a simple API that provides stream
and datagram modes, and is robust to failure. It does not provide the
collective communications or one-sided operations that MPI provides.

FT-TCP \cite{916715} is an architecture that allows a replicated
service to survive crashes without breaking its TCP connections. It is
a failover software implementation that wraps the TCP stack to
intercept all communication, and is based on message logging.

In the HPC domain, MPI is the dominant interface for inter-process
communication \cite{DBLP:conf/hoti/AtchleyDSGSBM11}. Designed for
maximum scalability, MPI has a richer though also more complex API
than sockets. The complications of fault tolerant MPI are detailed in
Section [[Fault Tolerant MPI]].

There are numerous highly specialised vendor APIs for distributed
network peer communication. Popular examples include Cray Portals
\cite{DBLP:conf/cluster/BrightwellHPRU05}, IBM's LAPI
\cite{DBLP:conf/ipps/ShahNMKHGGDB98} and Infiniband Verbs
\cite{Infiniband}. Verbs has support for two-sided and one-sided
asynchronous operations, and buffer management is left to the
application. Infiniband \cite{DBLP:conf/ics/LiuWKWP03} is a switched
fabric communications link commonly used in HPC clusters. It features
high throughput, low latency and is designed for scalability.

The performance of each interface varies wildly in terms of speed,
portability, robustness and complexity. The design of each balance the
trade-off between these performance metrics, e.g. trading portability
for high performance. Table \ref{tab:network-protocol-tradeoffs} lists
performance metrics for sockets, MPI and these specialised APIs.

#+BEGIN_LATEX
\begin{table}
{\small
\begin{center}
\begin{tabular}{|l||c|c|c|}
\hline
 & Sockets & MPI & Specialised APIs \\
\hline
\hline
Performance  & No      & Yes  & Yes              \\
Scalability  & No      & Yes  & Varies           \\
Portability  & Yes     & Yes  & No               \\
\textbf{Robustness} & \textbf{Yes} & \textbf{No} & \textbf{Varies}  \\
Simplicity   & Yes     & No   & Varies           \\
\hline
\end{tabular}
\end{center}
}
\caption{Metrics for Network Protocols}
\label{tab:network-protocol-tradeoffs}
\end{table}
#+END_LATEX

The Common Communication Interface (CCI)
\cite{DBLP:conf/hoti/AtchleyDSGSBM11} aims to address the problem of
using these specialised APIs, without having to understand the
complexities of each. The goal is to create a relatively simple
high-level communication interface with low barriers of adoption,
while still providing important features such as scalability,
performance, and resiliency for HPC and COTS architectures.

CCI uses connection-oriented semantics for peers to provide fault
isolation, meaning that a fault peer only affects that connection and
not all communication. Additionally, it also provides flexible
reliability and ordering semantics. CCI is an alternative to MPI
(Section [[Fault Tolerant MPI]]). If a HPC user needs tag matching and
communicators, then MPI is more suitable. If they do not need tag
matching and communicators, and their target is a low-latency,
high-throughput network with support for fault tolerance, then CCI may
be more suitable. 

Fault detection in CCI uses /keepalive/ timeouts that prevent a client
from connecting to a server, and then disappearing to a server
without noticing. If no traffic at all is received on
a connection within a timeout period, a keepalive event is raised
on that connection. The CCI implementation automatically sends control
heartbeats across an inactive connection, to reset the peer's
keepalive timer before it times out.

A level up from network protocols, orthogonal implementations target
message-oriented middleware. A popular example is ZeroMQ
\cite{zeromq}, an AMQP \cite{Vinoski:2006:AMQ:1187622.1187698}
interface. It provides patterns of message communication, including
publish-subscribe, pipelining and request-reply.  Recovery from faults
such as network outages and client disconnects are handled within the
middleware.

*** Fault Tolerant MPI

Most scientific applications are written either in C with MPI, or
Parallel Fortran \cite{DBLP:journals/software/KarpB88}. MPI provides
both two-sided semantics in MPI-1 \cite{Forum:1994:MMI:898758}, and
one-sided semantics in MPI-2 \cite{forum97mpi2}. It also provides tag
matching and collective operations.

Paradoxically however, fault tolerant MPI programming for users is
challenging, and they are often forced to handle faults
programmatically. This will become increasingly problematic as MPI is
scaled to massively parallel exascale systems as soon as 2015
\cite{kogge:darpa08}. The original MPI standards specify very limited
features related to reliability and fault tolerance
\cite{DBLP:journals/ijhpca/GroppL04}.  Based on the early standards,
an entire application is shut down when one of the executing
processors experiences a failure, as fault handling is per
communicator group, and not be peer. The default behaviour in MPI is
that any fault will typically bring down the entire communicator.

This section describes several approaches to achieve fault tolerance
in MPI programs including the use of checkpointing, modifying the
semantics of existing MPI functions to provide more fault-tolerant
behaviour, or defining extensions to the MPI specification.

**** Fault Tolerant MPI Implementations

A typical MPI software stack is shown in Figure
\ref{fig:mpi-layers}. MPI uses network hardware via network
abstraction layers, such as Verbs over Infiniband, or sockets over
TCP/IP based Ethernet.

#+CAPTION:    MPI Network Stack
#+LABEL:      fig:mpi-layers
#+ATTR_LaTeX: :width 60mm
[[./img/chp2/cci/transport-layers-mpi.pdf]]


Many fault tolerant MPI implementations adopt the checkpointing
approach (Section [[Rollback and Checkpointing]]). LAM-MPI
\cite{sankaran05:_lam_mpi_check_restar_framew} provides a variety of
fault tolerance mechanisms including an MPI coordinated
checkpointing interface. A checkpoint is initiated either by a user
or a batch scheduler, which is propagated to all processes
in an MPI job.

For the checkpoint and restart components of LAM-MPI, three abstract
actions are defined: /checkpoint/, /continue/ and /restart/. The
/checkpoint/ action is invoked when a checkpoint is initiated. Upon
receiving checkpoint requests, all the MPI processes interact with
each other to ensure a globally consistent state --- defined by
process states and the state of communication channels. The /continue/
action is activated after a successful checkpoint, which may
re-establish communication channels if processes have moved from
failed nodes to a new node in the system. The /restart/ action is
invoked after an MPI process has been restored from a prior
checkpoint. This action will almost always need to re-discover its MPI
process peers and re-establish communication channels to them.

The MPICH-V \cite{DBLP:conf/sc/BouteillerCHKLM03} environment
encompasses a communication library based on MPICH
\cite{DBLP:journals/pc/GroppLDS96}, and a runtime
environment. MPICH-V provides an automatic volatility tolerant MPI
environment based on uncoordinated checkpointing and rollback and
distributed message logging. Unlike other checkpointing systems such
as LAM-MPI, the checkpointing in MPICH-V is /asynchronous/, avoiding
the risk that some nodes may unavailable during a checkpointing
routine.

Cocheck \cite{Stellner:1996:CCP:645606.660853} is an independent
application making an MPI parallel application fault tolerant. Cocheck
sits at the runtime level on top of a message passing library,
providing a consistency at a level above the message passing
system. Cocheck coordinates the application processes checkpoints and
flushes the communication channels of the target applications using a
Chandy-Lamport’s algorithm \cite{snapshot-cl-algorithm}. The program
itself is not aware of checkpointing or rollback routines. The
checkpoint and rollback procedures are managed by a centralised
coordinator.

FT-MPI \cite{DBLP:conf/pvm/DewolfsBSF06} takes an alternative
approach, by modifying the MPI semantics in order to recover from
faults. Upon detecting communication failures, FT-MPI marks the
associated node as having a /possible/ error. All other nodes involved
with that communicator are informed. It extends the MPI communicator
states, and default MPI process sates \cite{DBLP:conf/pvm/FaggD00}.

LA-MPI \cite{Aulwes03networkfault} has numerous fault tolerance
features including application checksumming, message re-transmission
and automatic message re-routing. The primary motivation for LA-MPI
is fault tolerance at the data-link and transport levels. It
reliably delivers messages in the presence of I/O bus, network card
and wire-transmission errors, and so guarantees delivery of in-flight
message after such failures.

*** Erlang

Erlang is a distributed functional programming language and is
becoming a popular solution for developing highly concurrent,
distributed soft real-time systems. The Erlang approach to failures is
/let it crash and another process will correct the error/
\cite{DBLP:journals/cacm/Armstrong10}.

New processes are created with a ~spawn~ primitive. The return value
of a ~spawn~ call is a process ID (~PID~). One process can monitor
another by calling ~monitor~ on its ~PID~. When a process dies, A
~DOWN~ message is sent to all monitoring processes. For convenience,
the function ~spawn_monitor~ atomically spawns a new process and
monitors it. An example of using ~spawn_monitor~ is shown in Listing
\ref{lst:erlang-spawn-monitor}. The ~time_bomb/1~ function ticks down
from $N$ every second. When $N$ reaches 0, the process dies. The
~repeat_explosion/2~ function recursively spawns ~time_bomb/1~ $R$
times, monitoring each process. The execution of ~repeat_explosion/2~
in Listing \ref{lst:erlang-spawn-monitor-execution} demonstrates that
the death of spawned processes executing ~time_bomb/1~ does not impact
on the program execution.

#+BEGIN_LATEX
\begin{Code}
\begin{erlangcode}{Illustration of Fault Tolerant Erlang using Monitors}{lst:erlang-spawn-monitor}
-module(ammunition_factory).
-export([repeat_explosion/2,time_bomb/1]).

repeat_explosion(0,_) -> ok;
repeat_explosion(R,N) ->
    {_Pid,_MonitorRef} = spawn_monitor(fun() -> time_bomb(N) end),
    receive X -> io:format("Message: ~p\n",[X]),
    repeat_explosion(R-1,N) end.

time_bomb(0) -> exit("boom!");
time_bomb(N) -> timer:sleep(1000), time_bomb(N-1).
\end{erlangcode}
\end{Code}

\begin{Code}
{\scriptsize
\begin{lstlisting}[captionpos=b,basicstyle=\ttfamily\footnotesize, caption=Execution of \texttt{repeat\_explosion/2 from Listing \ref{lst:erlang-spawn-monitor}},label=lst:erlang-spawn-monitor-execution]
1> c(ammunition_factory).
{ok,ammunition_factory}
2> ammunition_factory:repeat_explosion(3,1).
Message: {'DOWN',#Ref<0.0.0.60>,process,<0.39.0>,"boom!"}
Message: {'DOWN',#Ref<0.0.0.62>,process,<0.40.0>,"boom!"}
Message: {'DOWN',#Ref<0.0.0.64>,process,<0.41.0>,"boom!"}
ok
\end{lstlisting}
}
\end{Code}
#+END_LATEX

The second way to handle faults in Erlang is by linking
processes. Bidirectional links are created between processes with the
~link~ function. As with ~monitor~, a ~PID~ argument is used to
identify the process being linked to. For convenience, the function
~spawn_link~ atomically spawns a new process and creates a
bidirectional link to it. Erlang OTP fault tolerance (Section [[Process
Supervision in Erlang OTP]]) is implemented in a simple way using the
nature of these links.

*** Process Supervision in Erlang OTP

Erlang OTP \cite{logan10} provides library modules that
implement common concurrent design patterns. Behind the
scenes, and without the programmer having to be aware of it, the library
modules ensure that errors and specific cases are handled in a
consistent way. OTP behaviours are a formalisation of process
design patterns. These library modules do all of the generic process
work and error handling. The application specific user code is placed
in a separate module and called through a set of predefined callback
functions.

OTP behaviours include /worker/ processes, which do the actual
processing. Supervisors monitor their children, both workers
and other supervisors --- creating a
/supervision tree/, shown in Figure \ref{fig:erlang-supervision-tree}.

#+CAPTION:    Supervision Trees in Erlang OTP
#+LABEL:      fig:erlang-supervision-tree
#+ATTR_LaTeX: :width 60mm
[[./img/chp2/erlang/erlang-supervision-tree.pdf]]

When constructing an OTP supervision tree, the programmer defines
processes with supervisor and child specifications. The supervisor
specification describes how the supervisor should react when a child
terminates. An ~AllowedRestarts~ parameter specifies the maximum
number of abnormal terminations the supervisor is allowed to handle in
~MaxSeconds~ seconds. A ~RestartStrategy~ parameter determines how
other children are affected if one of their siblings terminates. The
child specifications provide the supervisor with the properties of
each of its children, including instructions on how to start it. A
~Restart~ parameter defines the restart strategy for a child ---
either transient, temporary or permanent.

** CloudHaskell

CloudHaskell \cite{DBLP:conf/haskell/EpsteinBJ11} emulates Erlang's
approach of isolated process memory with explicit message passing, and
provides process linking. It explicitly targets distributed-memory
systems and it implements all parallelism extensions entirely in
Haskell. No extensions to the GHC \cite{ghc} runtime system are needed
. CloudHaskell inherits the language features of Haskell, including
purity, types, and monads, as well as the multi-paradigm concurrency
models in Haskell. CloudHaskell includes a mechanism for serialising
function closures, enabling higher order functions to be used in
distributed computing environments.

*** Fault Tolerance in CloudHaskell

Fault tolerance in CloudHaskell is based on ideas from Erlang (Section
[[Erlang]]). If a monitored process terminates, the monitoring process
will be notified. Ascertaining the origin of the failure and recover
from it are left to the application. The process linking and
monitoring in CloudHaskell is shown in Listing
\ref{lst:cloudhaskell-ft-api}. As CloudHaskell has borrowed the fault
tolerance ideas from Erlang, the ~repeat_explosion/2~ Erlang example
in Listing \ref{lst:erlang-spawn-monitor} could similarly be
constructed with CloudHaskell using ~spawnMonitor~ in
\ref{lst:cloudhaskell-ft-api}. At a higher level, another fault
tolerance abstraction is redundant distributed data structures. The
author has contributed \cite{sirkel-port} to the development of a
fault tolerant Chord-based distributed hash table in CloudHaskell.

#+BEGIN_LATEX
\begin{Code}
\begin{haskellcode}{CloudHaskell Fault Tolerant Primitives}{lst:cloudhaskell-ft-api}
-- * Monitoring and linking
link       :: ProcessId -> Process ()
monitor    :: ProcessId -> Process MonitorRef

-- * Advanced monitoring
spawnLink       :: NodeId -> Closure (Process ()) -> Process ProcessId
spawnMonitor    :: NodeId -> Closure (Process ()) -> Process (ProcessId, MonitorRef)
spawnSupervised :: NodeId -> Closure (Process ()) -> Process (ProcessId, MonitorRef)
\end{haskellcode}
\end{Code}
#+END_LATEX


*** CloudHaskell 2.0

Following on from the release of CloudHaskell as reported in
\cite{DBLP:conf/haskell/EpsteinBJ11}, a second version of CloudHaskell
was developed \cite{ch2-hiw2012}, keeping the APIs largely the
same. It was engineered by the WellTyped company \cite{welltyped},
and has since been maintained through a community effort.

The CloudHaskell 2.0 framework is de-coupled into multiple layers,
separating the process layer, transport layer, and transport
implementations. The network transport layer is inspired by the CCI
software stack, separating the transport API from protocol
implementations. The software stack as illustrated in Figure
\ref{fig:distributed-haskell-layers} is designed to encourage
additional middlewares other than CloudHaskell (e.g. HdpH) for
distributed computing, and for alternative network layer
implementations other than TCP (Section [[Fault Tolerant Networking
Protocols]]).

#+CAPTION:    Distributed Haskell Software Layers
#+LABEL:      fig:distributed-haskell-layers
#+ATTR_LaTeX: :width 80mm
[[./img/chp2/distributed-haskell/layers.pdf]]

** SymGridParII

Symbolic computation is an important area of both Mathematics and
Computer Science, with many large computations that would benefit from
parallel execution. Symbolic computations are, however, challenging to
parallelise as they have complex data and control structures, and both
dynamic and highly irregular parallelism. In contrast to many of the
numerical problems that are common in HPC applications, symbolic
computation cannot easily be partitioned into many subproblems of
similar type and complexity.

The SymGridPar (SGP) framework \cite{ISSAC} was developed to address
these challenges on small-scale parallel architectures.  However, the
multicore revolution means that the number of cores and the number of
failures are growing exponentially, and that the communication
topology is becoming increasingly complex. Hence an improved parallel
symbolic computation framework is required. SymGridParII is a
successor to SGP that is designed to provide scalability onto 10$^6$
cores, and hence also provide fault tolerance.

The main goal in developing SGPII \cite{Maier_Stewart_Trinder_SAC2013}
as a successor to SGP is scaling symbolic computation to
architectures with $10^6$ cores. This scale necessitates a number of
further design goals, one of which is /fault tolerance/, to cope with
increasingly frequent component failures (Section [[Failure Rates]]).

** HdpH

The realisation of SGPII is Haskell Distributed Parallel Haskell
(HdpH). The language is a shallowly embedded parallel extension of
Haskell that supports high-level implicit and explicit parallelism.

To handle the complex nature of symbolic applications, HdpH supports
dynamic and irregular parallelism. Task placement in SGPII should avoid
explicit choice wherever possible.  Instead, choice should be
semi-explicit, so the programmer decides which tasks are suitable for
parallel execution and possibly at what distance from the current
node they should be executed. HdpH therefore
provides high-level /semi-explicit/ parallelism. The programmer is not
required to explicitly place tasks on specific node, instead idle
nodes seek work automatically. The HdpH implementation continuously
manages load, ensuring that all nodes are utilised effectively.

*** HdpH Language Design

HdpH supports two parallel programming models. The first is a
continuation passing style \cite{sus75}, when the language primitives
are used directly. This supports a dataflow programming style. The
second programming model is through the use of higher order parallel
skeletons. Algorithmic skeletons have been developed on top of the
HdpH primitives to provide higher order parallel skeletons, inspired
by the /Algorithms + Skeletons = Parallelism/ paper
\cite{DBLP:journals/jfp/TrinderHLJ98}.

The HdpH language is strongly influenced by two Haskell libraries, that
lift functionality normally provided by a low-level RTS to the Haskell
level.

- Par Monad \cite{DBLP:conf/haskell/MarlowNJ11} :: A shallowly
                    embedded domain specific language (DSL) for
     deterministic shared-memory parallelism. The HdpH primitives
     extend the ~Par~ monad for /distributed memory/ parallelism.
- Closure serialisation in CloudHaskell :: HdpH extends the closure
     serialisation techniques from CloudHaskell (Section
     [[CloudHaskell]]) to support polymorphic closure transformation,
     which is used to implement high-level coordination abstractions.
     
*** HdpH Primitives

The API in Figure \ref{lst:hdph-api} expose scheduling primitives
for both /shared/ and /distributed/ memory parallelism. The shared
memory primitives are inherited from the ~Par~ monad. The ~Par~ type
constructor is a monad for encapsulating a parallel computation. To
communicate the results of computation (and to block waiting for their
availability), threads employ \texttt{IVar}s, which are essentially mutable
variables that are writable exactly once. A user can create an IVar
with ~new~, write to them with ~put~, and blocking read from them with
~get~.

#+BEGIN_LATEX
\begin{Code}
\begin{haskellcode}{HdpH Primitives}{lst:hdph-api}
-- * Types
type Par
data NodeId
data IVar a
data GIVar a

-- * Locality
myNode   :: Par NodeId
allNodes :: Par [NodeId]

-- * Scheduling
fork   :: Par () -> Par ()
spark  :: Closure (Par ()) -> Par ()
pushTo :: Closure (Par ()) -> NodeId -> Par ()

-- * Closure construction
mkClosure :: ExpQ -> ExpQ

-- * IVar Operations
new :: Par (IVar a)
put :: IVar a -> a -> Par ()
get :: IVar a -> Par a
tryGet :: IVar a -> Par (Maybe a)
probe  :: IVar a -> Par Bool
glob :: IVar (Closure a)  -> Par (GIVar (Closure a))
rput :: GIVar (Closure a) -> Closure a -> Par ()
\end{haskellcode}
\end{Code}
#+END_LATEX

The basic primitive for shared memory scheduling is ~fork~, which
forks a new thread and returns nothing. The two primitives for
distributed memory parallelism are ~spark~ and ~pushTo~. The former
operates much like ~fork~, generating a computation that /may/ be
executed on another node. The ~pushTo~ primitive is similar except
that it eagerly pushed a computation to a target node, where it
eagerly unwrapped from its closured form and executed. In contrast to
this scheduling strategy, ~spark~ just stores its argument in a local
/sparkpool/, where it sits waiting to be distributed, or scheduled by
an on-demand work-stealing scheduler, described in Section [[HdpH
Implementation]]. Closures are constructed with the ~mkClosure~
function. It safely converts a quoted thunk i.e. an expression in
Template Haskell's quotation brackets ~[|..|]~ into a quoted ~Closure~
\cite{Maier_Trinder_IFL2011}.


*** Programming Example with HdpH

A HdpH implementation of the Fibonacci micro-benchmark is shown in
Listing \ref{lst:hdph-fib-code}. It uses ~spark~ to allow the lazy
work stealing HdpH scheduling to (maybe) evaluate \texttt{fib~n-1},
\texttt{fib~n-2}, \texttt{fib~n-3} etc\ldots in parallel.

#+BEGIN_LATEX
\begin{Code}
\begin{haskellcode}{Fibonacci in HdpH}{lst:hdph-fib-code}
-- |Sequential Fibonacci micro-benchmark 
fib :: Int -> Integer
fib n | n == 0 = 0
      | n == 1 = 1
      | otherwise = fib (n-1) + fib (n-2)

-- |Fibonacci in HdpH
hdphFib :: RTSConf -> Int -> IO Integer
hdphFib conf x = fromJust <$> runParIO conf (hdphFibEval x)

-- |Fibonacci Par computation
hdphFibEval :: Int -> Par Integer
hdphFibEval x
    | x == 0 = return 0
    | x == 1 = return 1
    | otherwise = do
        @\label{lbl:lbl-example}@v <- new
        gv <- glob v
        spark $(mkClosure [| hdphFibRemote (x, gv) |])
        y <- hdphFibEval (x - 2)
        clo_x <- get v
        force $ unClosure clo_x + y

-- |Function closure
hdphFibRemote :: (Int, GIVar (Closure Integer)) -> Par ()
hdphFibRemote (n, gv) = hdphFibEval (n - 1) >>= force >>= rput gv . toClosure
\end{haskellcode}
\end{Code}
#+END_LATEX


*** HdpH Implementation

The HdpH system architecture is shown in Figure
\ref{fig:hdph-architecture}. Each node runs several thread schedulers,
typically one per core. Each scheduler owns a dedicated /threadpool/
that may be accessed by other schedulers for stealing work. Each node
runs a message handler, which shares access to the /sparkpool/ with
the schedulers. Inter-node communication is abstracted into a
/communication layer/, that provides startup and shutdown
functionality, node IDs, and peer-to-peer message delivery between
nodes. The communication layer in the version of HdpH presented in
\cite{Maier_Trinder_IFL2011} is based on MPI.

#+CAPTION:    HdpH System Architecture
#+LABEL:      fig:hdph-architecture
#+ATTR_LaTeX: :width 120mm
[[./img/chp2/hdph_architecture/hdph_architecture.pdf]]

** Fault Tolerance Potential for HdpH

HdpH was designed to have /potential/ for fault tolerance. The
language implementation isolates the heaps of each distributed node,
and hence has the potential to tolerate individual node failures. This
thesis realises this fault tolerance potential as a reliable
scheduling extension called HdpH-RS.

Symbolic computation is often distinguished from numerical analysis by
the observation that symbolic computation deals with exact
computations, while numerical analysis deals with approximate
quantities \cite{DBLP:conf/europar/HammondZCPT07}. Therefore, trading
precision in HdpH-RS is not a possibility, because solutions must be
necessarily exact. This chapter has classified numerous dependable
system types (Section [[Dependability of Distributed Systems]]) and
existing approaches to fault tolerance (Section [[Fault Tolerance
Mechanisms]]). Self stabilisation techniques and probabilistic accuracy
bounds can, for example, be used with stochastic simulations to trade
precision for reliability by simply discarding lost computations or by
smoothing information between neighbouring nodes (Section [[Distributed
Algorithms for Reliable Computing]]). Due to the necessary precision of
symbolic applications, these mechanisms are unsuitable for HdpH-RS.

HdpH-RS uses a TCP-based transport layer. In contrast to FT-TCP
(Section [[Fault Tolerant Networking Protocols]]), the resilience in
HdpH-RS is implemented at a higher level than TCP --- TCP is used to
detect node failure, but /recovery/ is implemented in the HdpH-RS
scheduler.

A collection of algorithmic parallel fault tolerance abstractions is
built on top of the HdpH-RS primitives ~supervisedSpawn~ and
~supervisedSpawnAt~, influenced by how the supervision behaviour in
Erlang OTP (Section [[Process Supervision in Erlang OTP]]) abstracts over
the lower level ~link~ primitive. The distinction between fault
tolerance in CloudHaskell (Section [[CloudHaskell]]) and HdpH-RS is how
failures are handled. Although CloudHaskell provides the necessary
primitives for fault tolerance i.e. process linking and monitoring
(Section [[Fault Tolerance in CloudHaskell]]), parent processes must be
programmed to recover from detected failures. In contrast, the HdpH-RS
scheduler handles and recovers from faults --- the programmer does not
need to program with faults in mind.

* Designing a Fault Tolerant Programming Language for Distributed Memory Scheduling
  
This chapter presents the design of a reliability extension to HdpH,
HdpH-RS. The design of HdpH-RS is sensitive to the complexities of
asynchronous distributed systems --- tracking task migration,
detecting failure, and identifying at-risk tasks in the presence of
failure. The design forms a foundation for the validation and
implementation of a reliable distributed scheduler for HdpH-RS in
Chapters [[The Validation of Reliable Distributed Scheduling for
HdpH-RS]] and [[Implementing a Fault Tolerant Programming Language and
Reliable Scheduler]]. The design of HdpH-RS has three elements:

1. *Fault tolerant programming primitives* The HdpH API is extended
   with the spawn family of primitives for /fault tolerant/ task
   scheduling, introduced in Section [[HdpH-RS Programming Primitives]].
2. *A small-step operational semantics* An operational semantics for a
   simplified definition for the HdpH-RS primitives is given in
   Section [[Operational Semantics]]. It describes the small-step
   reduction semantics on states for each primitive. Executions
   through these small-step transitions is in Section [[Execution of
   Transition Rules]].
3. *A reliable distributed scheduler* The algorithms for reliable
   scheduling is in Section [[Fault Tolerant Scheduling Algorithm]], and
   are then illustrated with examples in Section [[Fault Recovery
   Examples]]. HdpH-RS is resilient to single node and simultaneous
   failure (Section [[Simultaneous Failure]]), such as network
   partitioning where multiple nodes become detached from the root
   node at once. The scheduler detects network connection failures
   due to failures of nodes, networking hardware or software
   crashes. Failures are encapsulated with isolated heaps for each
   node, so the loss of one node does not damage other nodes in the
   architecture.

Why is such a rigorous design processes needed? Would not a carefully
constructed reliable scheduler suffice, with human intuition as the
verification step in its design? Asynchronous message passing in
distributed systems make it extremely difficult maintain a correct
understanding of task location, occurrence of task evaluation, and
node failure. The work stealing system architecture of HdpH
complicates matters further --- when tasks are created they can
migrate between nodes before they are either evaluated or lost in the
presence of failure. Marrying a verified scheduler model and a
formalised operational semantics as a design process adds confidence
that the HdpH-RS scheduler will evaluate tasks correctly and be
resilient to failure.

#+CAPTION: Abstraction of Design, Validation & Implementation of HdpH-RS
#+LABEL:      fig:design-abstraction
#+ATTR_LaTeX: :width 80mm
[[./img/chp3/design-abstraction/design-abstraction.pdf]]

The relationship between the HdpH-RS primitives and scheduler design
(Section [[HdpH-RS Programming Primitives]]), reliable scheduling
properties verified with model checking (Chapter [[The Validation of
Reliable Distributed Scheduling for HdpH-RS]]), the operational
semantics (Section [[Operational Semantics]]) and Haskell realisation
(Chapter [[Implementing a Fault Tolerant Programming Language and
Reliable Scheduler]]) are depicted in Figure
\ref{fig:design-abstraction}. The implementation honours the Promela
model verified by SPIN, and the small-step operational
semantics. Therefore, the scheduling algorithms in Section [[Fault
Tolerant Scheduling Algorithm]], the Promela model in Chapter [[The
Validation of Reliable Distributed Scheduling for HdpH-RS]], and
implementation in Chapter [[Implementing a Fault Tolerant Programming
Language and Reliable Scheduler]] are frequently cross-referenced to
show a consistency from design, through validation, to implementation.

** Supervised Workpools Prototype

A supervised workpools prototype is a strong influence on the HdpH-RS
fault tolerant programming primitives and reliable scheduling
design. The design and implementation has been published
\cite{DBLP:conf/sfp/StewartTM12}, and is described in detail in
Appendix [[Supervised Workpools]]. The concept of exposing the fault
tolerant programming primitives ~supervisedSpawn~ and
~supervisedSpawnAt~ is influenced by the ~supervisedWorkpoolEval~
primitive. All three
allow the user to opt-in to reliable scheduling of tasks. The workpool
hides task scheduling, failure detection and task replication from the
programmer. Moreover, workpools can be nested to form fault-tolerant
hierarchies, which is essential for scaling up to massively parallel
platforms.

On top of the supervised workpool, two algorithmic skeletons were
produced, encapsulating parallel-map and divide-and-conquer
patterns. The technique of abstracting fault tolerant parallel
patterns in this way has been elaborated in HdpH-RS, which exposes 10
fault tolerant algorithmic skeletons (Section [[Fault Tolerant Parallel
Skeletons]]). The supervised workpool achieved fault tolerance using
three techniques --- keeping track of where tasks are sent, detecting
failure, and replicating tasks that may be lost because of
failure. These techniques are extended in the HdpH-RS implementation.

The main limitation of the supervised workpool prototype is its
scheduling capability. Tasks are scheduled eagerly and
preemptively. There is no opportunity for load balancing between
overloaded and idle nodes. The challenge of fault tolerance is greatly
increased when a distributed scheduler is required to supervise tasks
that can migrate to idle nodes. The HdpH-RS design and implementation
is a refined elaboration of the supervised workpools prototype. It
supports work stealing, spark location tracking,
failure detection, and spark and thread replication. The HdpH-RS
implementation feature set matches the HdpH language and scheduler,
with the exception of fish hopping (Section [[Task Locality]]).
Failure-free runtime overheads are comparative to HdpH, even when
scaling to 1400 cores (Section [[Runtime & Speed Up]]).

** Introducing Work Stealing Scheduling

To minimise application completion times, load-balancing algorithms
try to keep all nodes busy performing application related work. If
work is not equally distributed between nodes, then some nodes may be
idle while others may hold more work than they can immediately
execute, wasting the potential performance that a parallel
environment offers. The efficiency gained with load balancing are
set against communication costs. The distribution of tasks causes
communication among the nodes in addition to
task execution \cite{DBLP:conf/ppopp/NieuwpoortKB01}. Minimising both the idle time
and communication overhead are conflicting goals, so load-balancing
algorithms have to carefully balance communication-related overhead
and processor idle time \cite{DBLP:journals/computer/ShivaratriKS92}.

Parallel symbolic computation is a good example where load balancing
is needed. Symbolic computations often exhibit irregular parallelism,
which in turn leads to less common patterns of parallel behaviour
\cite{DBLP:conf/europar/HammondZCPT07}. These applications pose
significant challenges to achieving scalable performance on
large-scale multicore clusters, often require ongoing dynamic load
balancing in order to maintain efficiency
\cite{DBLP:conf/sc/DinanLSKN09}.

Load balancing algorithms have been designed for single site clusters
e.g. \cite{DBLP:journals/tpds/ZainTML08} and wide area networks
e.g. \cite{DBLP:journals/concurrency/NieuwpoortMBKV00}. There are many
design possibilities for load balancing in wide area networks to
consider. These include cluster-aware hierarchical stealing and
cluster-aware load balanced stealing
\cite{DBLP:conf/ppopp/NieuwpoortKB01}. For single-site clusters, two
common load balancing methods are work /pushing/ initiated by an
overloaded node, and work /stealing/ initiated by an idle node.

With random work /pushing/, a node checks after creating or receiving
a task whether the task queue length exceeds a certain threshold
value. If this is the case, a task from the queue's tail is pushed to
a randomly chosen node. This approach aims at minimising node idle
time because tasks are pushed ahead of time, before they are actually
needed. Random work /stealing/ attempts to steal a job from a randomly
selected node when a node finds its own work queue empty, repeating
steal attempts until it succeeds. This approach minimises
communication overhead at the expense of idle time. No communication
is performed until a node becomes idle, but then it has to wait for a
new task to arrive. When the system load is high, no communication is
needed, causing the system to behave well under high loads
\cite{DBLP:conf/ppopp/NieuwpoortKB01}.

The HdpH-RS language supports both work /pushing/ and work /stealing/,
thanks to the inherited design and implementation of the HdpH
scheduler (Section [[HdpH]]). The language supports implicit task
placement with ~spawn~, and explicit placement with ~spawnAt~,
described in Section [[HdpH-RS Programming Primitives]]. /Sparkpools/ hold
sparks (closured expressions) that can migrate between nodes in the
HdpH architecture. When a node becomes idle, the scheduler looks for
work. If the local sparkpool is empty, it will fish for work from
remote nodes. If the local sparkpool holds a spark, it is
unpacked and added to a local /threadpool/ to be evaluated. In this
form, threads cannot migrate (again) to other nodes.

This chapter adopts a common terminology for work stealing schedulers
\cite{DBLP:conf/focs/Blumofe94}. In work stealing strategies, nodes
assume dynamically inherited scheduling roles during
runtime. /Thieves/ are idle nodes with few tasks to compute, or no
tasks at all. They steal from /victims/ that host more work than they
can immediately execute. A thief will randomly select a victim,
avoiding deterministic work stealing loops between only a subset of
nodes. /Fishing/ is the act of a thief attempting to steal work from a
chosen victim.  The HdpH-RS terminology is described in Section
[[HdpH-RS Terminology]], for now a /spark/ can be regarded as a
computation that can migrate between nodes. Two examples of the HdpH
fishing protocol are shown in Figure
\ref{fig:hdph-workstealing}. Thieving node C attempts to steal a spark
from victim node B with a fish message, and is successful. Thieving
node D is unsuccessful in its fishing message to victim node B.

#+CAPTION: Two HdpH Work Stealing Scenarios
#+LABEL:      fig:hdph-workstealing
#+ATTR_LaTeX: :width 80mm
[[./img/chp3/msc/hdph-fishing.pdf]]

** Reliable Scheduling for Fault Tolerance

The fault tolerance is realised in HdpH-RS with two new programming
primitives, and a resilient scheduler.

- Fault tolerance API :: The HdpH-RS API provides four new
     primitives. They provide implicit and explicit parallelism. They
     are ~spawn~ and ~spawnAt~, and two fault tolerant primitives
     ~supervisedSpawn~ and ~supervisedSpawnAt~. The definition of a
     spawned computation defines a close pairing between tasks and
     values, described in Section [[HdpH-RS Programming Primitives]].
- Reliable scheduler :: To support the two fault tolerance primitives,
     the HdpH-RS scheduler includes 7 additional RTS messages,
     compared to the HdpH scheduler. The recovery of supervised tasks
     is described algorithmically in Section [[Fault Tolerant Scheduling
     Algorithm]], and the operational semantics for recovery is given in
     Section [[Operational Semantics]]. The additional runtime system
     (RTS) messages in the reliable scheduler are described in Section
     [[RTS Messages to Support the Work Stealing Protocol]].

The language design provides opt-in reliability. The original plan for
the HdpH-RS API was to use the existing HdpH scheduling API, and
adding a scheduler flag to indicate that fault tolerant work stealing
should be used. An operational semantics (Section [[Operational
Semantics]]) was attempted for the fault tolerant versions of the HdpH
task creation primitives ~spark~ and ~pushTo~. The semantics were
complicated by a lack of enforced coupling between tasks and values
(\texttt{IVar}s). A simpler set of task creation primitives, the spawn
family, were designed as a result.  The spawn family of primitives
(Section [[HdpH-RS Programming Primitives]]) enforce a one-to-one
relationship between task expressions and \texttt{IVar}s.

*** HdpH-RS Terminology

Table \ref{tab:hdphrs-terminology} defines a consistent terminology to
describe tasks, values, and nodes. The terminology is tied closely to
the spawn family of primitives, introduced in Section [[HdpH-RS
Programming Primitives]].

#+BEGIN_LATEX
\begin{table}\footnotesize
\begin{center}
\begin{tabular}{|l|p{10cm}|}
\hline
\textbf{Term} & \textbf{Description} \\
\hline
\hline
\multicolumn{2}{|l|}{Tasks and values} \\
\hline
Future                & A variable that initially holds no value. It will later be filled by evaluating a task expression. \\
Task Expression       & A pure Haskell expression that is packed in to a spark or thread. \\
Spark                 & A lazily scheduled task expression to be evaluated and its value written to its corresponding  future. \\
Thread                & An eagerly scheduled task expression to be evaluated and its value written to its corresponding  future. \\
\hline
\hline
\multicolumn{2}{|l|}{Supervision of futures and sparks} \\
\hline
Supervised Future     & Same as a future, with an additional guarantee of being eventually filled by its associated task expression even in the presence of remote node failure. \\
Supervised Spark      & A lazily scheduled spark that has its location monitored. A replica of this supervised spark will be re-scheduled by its creator (supervisor) when a previous copy is lost in the presence of failure. \\
\hline
\hline
\multicolumn{2}{|l|}{Node roles for work stealing} \\
\hline
Thief                 & Node that attempts to steal a spark or supervised spark from a chosen victim. \\
Victim                & Node that holds a spark or supervised spark and has been targeted by a thief. \\
Supervisor            & Creator of a supervised future \& corresponding supervised spark or thread. \\
Task Location Tracker & State on a supervisor that knows the location of supervised sparks or threads corresponding to supervised futures that it hosts. \\
\hline
\end{tabular}
\caption{HdpH-RS Terminology}
\label{tab:hdphrs-terminology}
\end{center}
\end{table}
#+END_LATEX


*** HdpH-RS Programming Primitives

Listing \ref{lst:hdphrs-api} shows the HdpH-RS API. The fault
tolerance primitives ~supervisedSpawn~ and ~supervisedSpawnAt~ in
HdpH-RS on lines \ref{code:supervisedSpawn} and
\ref{code:supervisedSpawnAt} share the same API as their non-fault
tolerant counterparts ~spawn~ and ~spawnAt~. This minimises the pain
of opting in to (and out of) fault tolerant scheduling. All four of
these primitives take an expression as an argument, and return a
/future/ \cite{DBLP:journals/toplas/Halstead85}. A future can be
thought of as placeholder for a value that is set to contain a real
value once that value becomes known, by evaluating the
expression. Futures are implemented with HdpH-RS using a modified
version of \texttt{IVar}s from HdpH, described in Section [[Implementing
Futures]]. In HdpH-RS, the creation of \texttt{IVar}s and the writing of
values to \texttt{IVar}s is hidden from the programmer, which is
instead performed by the spawn primitives. A visualisation of dataflow
graph construction using ~spawn~ and ~spawnAt~ is shown in Figure
\ref{fig:spawn-visualisation}. It is an distributed-memory extension
of the dataflow graph scheduling for single nodes presented in the
monad-par paper \cite{DBLP:conf/haskell/MarlowNJ11}.

#+CAPTION: Dataflow Parallelism with \texttt{spawn},\texttt{spawnAt} and \texttt{get} 
#+LABEL:      fig:spawn-visualisation
#+ATTR_LaTeX: :width 130mm
[[./img/chp3/hdphrs-dataflow/hdphrs-dataflow.pdf]]

#+BEGIN_LATEX
\begin{Code}
\begin{haskellcode}{HdpH-RS Primitives.}{lst:hdphrs-api}
-- * Lazy work placement
spawn             :: Closure (Par (Closure a)) -> Par (IVar (Closure a))
supervisedSpawn   :: Closure (Par (Closure a)) -> Par (IVar (Closure a)) @\label{code:supervisedSpawn}@

-- * Eager work placement
spawnAt           :: Closure (Par (Closure a)) -> NodeId -> Par (IVar (Closure a))
supervisedSpawnAt :: Closure (Par (Closure a)) -> NodeId -> Par (IVar (Closure a)) @\label{code:supervisedSpawnAt}@

-- * IVar operations
get    :: IVar a -> Par a
tryGet :: IVar a -> Par (Maybe a)
probe  :: IVar a -> Par Bool
\end{haskellcode}
\end{Code}
#+END_LATEX

The following list defines four HdpH-RS primitives at a high level. The
naming convention for operations on ~IVar~ futures in HdpH-RS, namely
~spawn~ and ~get~, is inspired by the monad-par library
\cite{DBLP:conf/haskell/MarlowNJ11}.

- ~spawn~ :: Takes a closured expression to be filled with the value
             of the evaluated spark. ~IVar~ operations ~new~ and
             ~rput~ are hidden from the
             programmer. It returns an empty ~IVar~, and schedules the
             spark.
- ~supervisedSpawn~ :: Same as ~spawn~, but in this case it invokes
     the reliable scheduler, which guarantees the execution of the
     supervised spark.
- ~spawnAt~ :: Takes a closured expression, and a ~NodeId~ parameter
               specifying which node that will be eagerly allocated
               the thread. Again, the ~IVar~ operations ~new~ and
               ~rput~ are hidden from the programmer.
- ~superviseSpawnAt~ :: Same as ~spawnAt~, but in this case it invokes
     the reliable scheduler, which guarantees the execution of the
     supervised thread. For fault tolerance, the task tracking of the
     supervised thread is simpler than ~supervisedSpawn~. Once the task has been
     transmitted, the scheduler knows that if the target node dies
     /and/ the ~IVar~ is empty, then the supervised thread needs
     recovering.

Regrettably, naming conventions across programming libraries that
support futures do not share a common naming convention of primitives
for creating and reading futures, and task scheduling. This is
explored in greater detail in Appendix [[Programming with Futures]], which
compares the use of functional futures in the APIs for monad-par
\cite{DBLP:conf/haskell/MarlowNJ11}, Erlang RPC
\cite{DBLP:books/daglib/0022920}, the CloudHaskell Platform
\cite{ch-p} and HdpH-RS.


** Operational Semantics

This section gives a small-step operational semantics
\cite{DBLP:journals/jlp/Plotkin04} of the key HdpH-RS primitives
outlined in Section [[HdpH-RS Programming Primitives]]. The operational
semantics is a small-step reduction semantics on states. It also
introduces a number of internal scheduling transitions in HdpH-RS that
are used for migrating sparks and writing values to \texttt{IVar}s,
and recovering lost sparks and threads in the presence of failure.

An operational semantics specifies the behaviour of a programming
language by defining a simple abstract machine for it. There are other
approaches to formalising language semantics, though the treatment of
non-determinism and concurrency in more abstract denotational
semantics make them unsuitable in this context
\cite{DBLP:books/daglib/0005958}.

Section [[Semantics of the Host Language]] outlines the semantics of the
host language, Haskell. Section [[HdpH-RS Core Syntax]] describes a
simplified HdpH-RS syntax, and Section [[Small Step Operational
Semantics]] defines the operational semantics of these
primitives. Section [[Execution of Transition Rules]] simulates the
execution of transition rules over the global state to demonstrate the
execution and recovery of sparks and threads.

*** Semantics of the Host Language

Being an embedded DSL, the operational semantics of HdpH-RS builds on
the semantics of the host language, Haskell. For the sake of
presenting a semantics, the host language is assumed to be a
standard call-by-name lambda calculus with a fixed-point combinator
and some basic data types, like booleans, integers, pairs, and lists.

Figure \ref{fig:host_lang} presents syntax and operational semantics
for this host language. Note that data types (constructors, projections
and case analysis) have been omitted for brevity.

\begin{figure}
\small%
\begin{gather*}
\begin{array}{l@{\enspace}r@{~}c@{~}l}
\textit{Variables}
& x & & \text{general variable}
\\[1ex]
\textit{Terms}
& L, M, N & \bnfdef & x \mid \fun x . L \mid M\,N \mid \fix M 
\\[1ex]
\textit{WHNF values}
& V & \bnfdef & \fun x . L \mid x\,\bar{L}
\end{array}
\\[1ex]
\begin{aligned}
\text{[whnf]}\enspace
& \displaystyle\frac{}{V \bg V}
& \qquad
\text{[fix]}\enspace
& \displaystyle\frac{M\,(\fix M) \bg V}{\fix M \bg V}
\\[2ex]
\text{[beta]}\enspace
& \displaystyle\frac{M \bg \fun x . L \qquad L\subst{x}{N} \bg V}{M\,N \bg V}
& \qquad
\text{[head]}\enspace
& \displaystyle\frac{M \bg x\,\bar{L}}{M\,N \bg x\,\bar{L}\,N}
\end{aligned}
\end{gather*}
\caption{Syntax and Big-Step Semantics of Host Language.}
\label{fig:host_lang}
\end{figure}

The syntax of terms is standard, including the usual convention that
applications associate to the left, and the scopes of lambda
abstractions extend as far as possible to the right.  The notation
$\bar{L}$ is a shorthand for a sequence of applications $L_1\,L_2
\dots L_n$, for some $n \geq 0$.  We write $L\subst{x}{N}$ for the
term that arises from $L$ by (capture-avoiding) substitution of all
free occurrences of $x$ with $N$.

The big-step operational semantics is also standard for a call-by-name
lambda calculus.  By induction on derivations, it can be shown that
the big-step reduction relation $\bg$ is a partial function the
results of which are values in weak head normal form
\cite{Abramsky90thelazy}.

*** HdpH-RS Core Syntax

Listing \ref{lst:hdphrs-simplified-syntax} shows the core primitives
of HdpH-RS. For the purpose of simplifying the semantics, the language
presented in Listing \ref{lst:hdphrs-simplified-syntax} deviates from
the HdpH-RS API (Listing \ref{lst:hdphrs-api}) in a number of ways. It
ignores the issues with closure serialisation. So, there is no
~Closure~ type constructor and instead all expressions are assumed
serialisable. The syntax does not allow to the programmer to /fork/
threads. Although this could be easily added, the focus is on
scheduling and recovering remote tasks with the ~spawn~ family. Read
access (~get~) is still restricted to locally hosted
\texttt{IVar}s. The pure ~at~ operation is lifted into the ~Par~
monad. The ~rput~ operation is not part of the HdpH-RS API, but is an
operation in the semantics, hence its inclusion. Pure operations would
complicate the semantics needlessly, hence why ~at~ is
monadic. Lastly, the definition of ~eval~ does not allow ~IO~ actions
to be lifted to ~Par~ computations, so only pure computations can be
lifted.

#+BEGIN_LATEX
\begin{Code}
\begin{haskellcode}{Simplified HdpH-RS Primitives}{lst:hdphrs-simplified-syntax}
-- * Types
data Par a    -- Par monad
data NodeId   -- explicit locations (shared-memory nodes)
data IVar a   -- remotely writable, locally readable one-shot buffers

-- * IVar operations
at    :: IVar a -> Par Node    -- query host
probe :: IVar a -> Par Bool    -- non-blocking local test
get   :: IVar a -> Par a       -- blocking local read
rput  :: IVar a -> a -> Par () -- put value to IVar (hidden primitive)

-- * Task scheduling
-- spawn
spawn             :: Par a  -> Par (IVar a)
supervisedSpawn   :: Par a  -> Par (IVar a)

-- spawnAt
spawnAt           :: NodeId -> Par a -> Par (IVar a) 
supervisedSpawnAt :: NodeId -> Par a -> Par (IVar a)

-- * Task evaluation
eval :: a -> Par a  -- evaluate argument to WHNF
\end{haskellcode}
\end{Code}
#+END_LATEX

*** Small Step Operational Semantics

This section describes small-step operational semantics of
HdpH-RS. They extend the HdpH semantics \cite{hdph-semantics} with a
set of new states for futures and faults, and transition rules for the
spawn family of primitives & spark and thread recovery.
**** Semantics of HdpH-RS

Figure \ref{fig:dsl_syn} presents the syntax of the HdpH-RS DSL for
the purpose of this semantics. They are an extension of the HdpH
semantics, with the spawn family of primitives added, and ~spark~,
~pushTo~ and ~new~ removed. For simplicity, types are elided. However,
all terms are assumed well-typed according to the type signatures
given in Listing \ref{lst:hdphrs-simplified-syntax}. Figure
\ref{fig:dsl_syn} also introduces evaluation contexts
\cite{DBLP:conf/pldi/FlanaganSDF93} as Par monad terms with a hole,
which may be followed by a continuation.  Put differently, an
evaluation context is a Par monad term, possibly to the left of a
bind.

The HdpH-RS semantics is influenced by the monad-par semantics
\cite{DBLP:conf/haskell/MarlowNJ11}, which in turn takes its
presentation from the papers on concurrency and asynchronous
exceptions in Haskell \cite{Jones02tacklingthe}. A vertical bar $\p$
is used to join threads, sparks and \texttt{IVar}s into a program
state. For example, $i\thr{\E[\get\,M]}{n}\, \p\, j\gv{N}{n}$ is a
program state with a thread labelled $i$ whose next ~Par~ action is
~get~, joined with a full \texttt{IVar} labelled $j$ (Table
\ref{tab:hdph-atomic-states}). The following rule is used for getting
values from full \texttt{IVar}s, shown later in Figure
\ref{fig:hdphrs-dsl_smallstep_sched-futures}.

#+BEGIN_LATEX
\begin{center}
\begin{equation*}
\begin{array}{p{.75\textwidth}}
[get$_i$]\enspace$\displaystyle%
  \frac{M \bg j \qquad}%
       {i\thr{\E[\get\,M]}{n} \p j\gv{N}{n} \p S
        \enspace\sm{\Tpe}\enspace
        i\thr{\E[\ret\,N]}{n} \p j\gv{N}{n} \p S}$
\end{array}
\end{equation*}
\end{center}
#+END_LATEX

This rule says that if the next ~Par~ action in thread $i$ is
\texttt{get~M} where $M$ is reduced to label $j$, and this thread is
parallel with an \texttt{IVar} named $j$ containing $N$, then the
action \texttt{get~M} can be replaced by \texttt{return N}.

#+BEGIN_LATEX
\begin{figure}
\begin{equation*}
\begin{array}{l@{\enspace}r@{~}c@{~}ll}
\textit{Par monad terms}
& P,Q & \bnfdef & \multicolumn{2}{@{}l}{P \bind \fun x . Q \mid
                                        \ret\,M \mid \eval\,M} \\
&& | & \multicolumn{2}{@{}l}{\spawn\,P \mid \spawnAt\,M\,P} \\
&& | & \multicolumn{2}{@{}l}{\supervisedSpawn\,P \mid \supervisedSpawnAt\,M\,P} \\
&& | & \multicolumn{2}{@{}l}{\at\,M \mid
                             \probe\,M \mid \get\,M \mid \rputi\,M\,P}
\\[1ex]
\textit{Evaluation contexts}
& \E & \bnfdef & \multicolumn{2}{@{}l}{[{\cdot}] \mid \E \bind M}
\end{array}
\end{equation*}
\caption{Syntactic categories required for HdpH-RS small-step semantics.}
\label{fig:dsl_syn}
\end{figure}
#+END_LATEX

**** States

The operational semantics of HdpH-RS is a small-step reduction
semantics on states. A /state/ $S$ is a finite partial map from an
infinite set of /labels/ $\names$ to threads, sparks, and
\texttt{IVar}s.  We denote the finite set of labels defined on $S$ by
$\dom(S)$, and the empty state (i.e. the state defining no labels) by
$\emptyset$.  Two states $S_1$ and $S_2$ can be composed into a new
state $S_1 \p S_2$, provided that they do not overlap, i.e. $\dom(S_1)
\inter \dom(S_2) = \emptyset$.  Thus, states with composition $\p$ and
identity $\emptyset$ form a commutative partial monoid, that is $\p$
is an associative and commutative operation, and $\emptyset$ is
neutral with respect to $\p$.

Short hand $i\gv{?}{n}$ is used to denote an IVar that is either empty
or full. Transition rules that use this IVar state do not need to know
whether or not the IVar has been written to. The notation $|{x,y,z}| =
3$ denotes three pairwise distinct elements \emph{x, y} and
\emph{z}. Ultimately, a state is either the special state *Error* or
is constructed by composition from /atomic states/. It consists of four
types of atomic states inherited from HdpH in Table
\ref{tab:hdph-atomic-states}, and 4 types of new atomic states for
HdpH-RS in Table \ref{tab:hdphrs-atomic-states}.

#+BEGIN_LATEX
\begin{table}
\begin{tabular}{|l|l|}
\hline
Threads & $i\thr{M}{n}$ maps label $i$ to a \emph{thread} computing $M$ on node $n$. \\
Sparks & $i\spk{M}{n}$ maps label $i$ to a \emph{spark} to compute $M$, currently on node $n$. \\
Empty IVars & $i\gv{}{n}$ maps label $i$ to an \emph{empty IVar} on node $n$. \\
Full IVars & $i\gv{M}{n}$ maps label $i$ to a \emph{full IVar} on node $n$, filled with $M$. \\
\hline
\end{tabular}
\caption{HdpH Atomic States}
\label{tab:hdph-atomic-states}
\end{table}
#+END_LATEX

#+BEGIN_LATEX
\begin{table}
\begin{tabular}{|l|p{10.5cm}|}
\hline
Supervised spark & $i \supspk{M}{n}$ maps label $i$ to a supervised spark on node $n$. \\
Supervised futures & $i\gv{j \supspk{M}{n'}}{n}$ maps label $i$ to an empty IVar on node $n$, to be filled by the result of the supervised spark $j$ . \\
Threaded futures & $i\gv{j \thr{M}{n'}}{n}$ maps label $i$ to an empty IVar on nodes $n$, to be filled by the result of the supervised thread $j$ . \\
Faults & $i:dead_n$ maps label $i$ to a notification that node $n$ has died. \\
\hline
\end{tabular}
\caption{HdpH-RS Atomic States}
\label{tab:hdphrs-atomic-states}
\end{table}
#+END_LATEX

**** Transitions

The small-step transition rules embodying HdpH-RS scheduling policy
are presented in three parts. First, explicit rules are presented for
four primitives in the spawn family and ~IVar~ operations. Next, the
migration and conversion of sparks and supervised sparks are
presented. Lastly, the transition rules for killing nodes, and
recovering sparks and threads are presented. The transition rules are
summarised in Table \ref{tab:transitions-extensions}, which also
identifies the new rules for HdpH-RS i.e. are extensions of HdpH.


#+BEGIN_LATEX
\begin{table}
\begin{center}
{\footnotesize
\begin{tabular}{|l|c|}
\hline
\multirow{2}{*}{\textbf{Transition Rules}} & \multirow{2}{*}{\pbox{1.5cm}{\relax\ifvmode\centering\fi HdpH-RS \\ Extension}} \\
  & \\
\hline
\hline
\multicolumn{2}{|l|}{Primitives \& Evaluation Contexts, Figure \ref{fig:hdphrs-dsl_smallstep_primitives}.} \\
\hline
spawn & \tick \\
supervisedSpawn & \tick \\
spawnAt & \tick \\
supervisedSpawnAt & \tick \\
eval & \\
bind & \\
normalize & \\
\hline
\hline
\multicolumn{2}{|l|}{IVar Operations, Figure \ref{fig:hdphrs-dsl_smallstep_sched-futures}.} \\
\hline
rput\_empty & \\
rput\_empty\_supervised\_threaded\_future & \tick \\
rput\_empty\_supervised\_sparked\_future & \tick \\
rput\_full & \\
get & \\
get\_error & \\
probe\_empty & \\
probe\_empty\_supervised\_threaded\_future & \tick \\
probe\_empty\_supervised\_sparked\_future & \tick \\
probe\_full & \\
probe\_error & \\
\hline
\hline
\multicolumn{2}{|l|}{Spark Scheduling, Figure \ref{fig:hdph-dsl_smallstep_sched}.} \\
\hline
migrate\_spark & \\
migrate\_supervised\_spark & \tick \\
convert\_spark & \\
convert\_supervised\_spark & \tick \\
\hline
\hline
\multicolumn{2}{|l|}{Failure \& Recovery, Figure \ref{fig:hdphrs-dsl_smallstep_recovery}.} \\
\hline
kill\_node & \tick \\
kill\_spark & \tick \\
kill\_supervised\_spark & \tick \\
kill\_thread & \tick \\
kill\_ivar & \tick \\
recover\_supervised\_spark & \tick \\
recover\_supervised\_thread & \tick \\
\hline
\end{tabular}
}
\caption{Summary of Transition Rules}
\label{tab:transitions-extensions}
\end{center}
\end{table}
#+END_LATEX

***** Explicit HdpH-RS Primitive Rules

#+BEGIN_LATEX
\begin{figure}
\footnotesize%
\begin{equation*}
\renewcommand{\arraystretch}{2.0}
\begin{array}{p{.95\textwidth}}

[spawn$_i$]\enspace$\displaystyle%
  \frac{j \notin \dom(S) \qquad k \notin \dom(S) \qquad |\{i,j,k\}| = 3 }%
       {i\thr{\E[\spawn\,N]}{n} \p S
        \enspace\sm{\Tpe}\enspace
        i\thr{\E[\ret\, j]}{n} \p j\gv{ }{n} \p k \spk{N \bind \rputi\, j}{n} \p S}$
\\[2ex]

[supervisedSpawn$_i$]

\enspace$\displaystyle%
  \frac{j \notin \dom(S) \qquad k \notin \dom(S) \qquad |\{i,j,k\}|=3 }%
       {i\thr{\E[\supervisedSpawn\,N]}{n} \p S
        \enspace\sm{\Tpe}\enspace
        i\thr{\E[\ret\, j]}{n} \p j\gv{ k \supspk{N \bind \rputi\, j}{n}}{n}
        \p k \supspk{N \bind \rputi\, j}{n} \p S}$

\\[2ex]

[spawnAt$_i$]\enspace$\displaystyle%
  \frac{M \bg n' \qquad j \notin \dom(S) \qquad k \notin \dom(S) \qquad |\{i,j,k\}|=3 }%
       {i\thr{\E[\spawnAt\,M\,N}{n} \p S
        \enspace\sm{\Tpe}\enspace
        i\thr{\E[\ret\, j]}{n} \p j\gv{ }{n} \p k \thr{N \bind \rputi\, j}{n'} \p S}$
\\[2ex]

[supervisedSpawnAt$_i$]

\enspace$\displaystyle%
  \frac{M \bg n' \qquad j \notin \dom(S) \qquad k \notin \dom(S) \qquad |\{i,j,k\}|=3}%
       {i\thr{\E[\supervisedSpawnAt\,M\,N]}{n} \p S
        \enspace\sm{\Tpe}\enspace
        i\thr{\E[\ret\, j]}{n} \p j\gv{ k
          \thr{N \bind \rputi\, j}{n'} }{n} \p k \thr{N \bind \rputi\, j}{n'} \p S}$
\\[5ex]

[eval$_i$]\enspace$\displaystyle%
  \frac{M \bg V}%
       {i\thr{\E[\eval\,M]}{n} \p S
        \enspace\sm{\Tpe}\enspace
        i\thr{\E[\ret\,V]}{n} \p S}$
\\[2ex]
[bind$_i$]\enspace%
  $i\thr{\E[\ret\,N \bind M]}{n} \p S
   \enspace\sm{\Tpe}\enspace
   i\thr{\E[M\,N]}{n} \p S$
\\[2ex]
[normalize$_i$]\enspace$\displaystyle%
  \frac{M \bg V \qquad M \not\equiv V}%
       {i\thr{\E[M]}{n} \p S
        \enspace\sm{\Tpe}\enspace
        i\thr{\E[V]}{n} \p S}$

\end{array}
\end{equation*}
\caption{Small-step transition rules embodying HdpH-RS Primitives \& Evaluation Contexts.}
\label{fig:hdphrs-dsl_smallstep_primitives}
\end{figure}
#+END_LATEX

The semantics for the spawn family of HdpH-RS primitives are shown in
Figure \ref{fig:hdphrs-dsl_smallstep_primitives}. The first rule
~spawn~ takes a task ~N~, and two new atomic states are added to the
state ~S~. The first is an empty ~IVar~ $j\gv{ }{n}$ and the second $k
\spk{N \bind \rputi\, j}{n}$ is a spark that, when converted to a
thread, will evaluate ~N~ to normal form, and write the value to
/j/. The empty ~IVar~ /j/ is returned immediately, so ~spawn~ is
non-blocking. ~IVar~ $j$ will always reside on $n$, and spark $k$ will
initially reside on $n$.

The ~supervisedSpawn~ rule is identical but with one important
distinction, the ~IVar~ /j/ and spark /k/ are more tightly coupled in
$j\gv{ k \supspk{N \bind \rputi\, j}{n}}{n}$, stating that /j/ will be
filled by ~rput~ the value of evaluating ~N~ to normal form in
/k/. The ~spawnAt~ rule shows that the primitive takes two arguments,
~M~ and ~N~. Reducing ~M~ to weak head normal form is $n'$ indicating
the node to which the expression $N \bind \rputi\, j$ will be
sent. Once again ~spawnAt~ is non-blocking, and an empty ~IVar~ /j/ is
returned immediately. The ~supervisedSpawnAt~ couples the supervised
future /j/ and supervised thread /k/, enforcing that the value of ~N~
reduced to normal form will be written to ~IVar~ /j/ with ~rput~.

***** Explicit HdpH-RS Transitions on Futures

#+BEGIN_LATEX
\begin{figure}
\small%
\begin{equation*}
\renewcommand{\arraystretch}{2.0}
\begin{array}{p{.95\textwidth}}
[rput\_empty$_i$]\enspace$\displaystyle%
  \frac{M \bg j}%
       {i\thr{\E[\rputi\,M\,N]}{n'} \p j\gv{}{n} \p S
        \enspace\sm{\Tpe}\enspace
        i\thr{\E[\ret\,\unit]}{n'} \p j\gv{N}{n} \p S}$
\\[2ex]

[rput\_empty\_supervised\_threaded\_future$_i$]

\enspace$\displaystyle%
  \frac{M \bg j}%
       {i\thr{\E[\rputi\,M\,N]}{n'} \p j\gv{ k \thr{M}{n''}}{n} \p S
        \enspace\sm{\Tpe}\enspace
        i\thr{\E[\ret\,\unit]}{n'} \p j\gv{N}{n} \p S}$
\\[2ex]

[rput\_empty\_supervised\_sparked\_future$_i$]

\enspace$\displaystyle%
  \frac{M \bg j }%
       {i\thr{\E[\rputi\,M\,N]}{n'} \p j\gv{ k \supspk{M}{n''}}{n} \p S
        \enspace\sm{\Tpe}\enspace
        i\thr{\E[\ret\,\unit]}{n'} \p j\gv{N}{n} \p S}$
\\[2ex]

[rput\_full$_i$]\enspace$\displaystyle%
  \frac{M \bg j}%
       {i\thr{\E[\rputi\,M\,N]}{n'} \p j\gv{N'}{n} \p S
        \enspace\sm{\Tpe}\enspace
        i\thr{\E[\ret\,\unit]}{n'} \p j\gv{N'}{n} \p S}$
\\[2ex]


[get$_i$]\enspace$\displaystyle%
  \frac{M \bg j \qquad}%
       {i\thr{\E[\get\,M]}{n} \p j\gv{N}{n} \p S
        \enspace\sm{\Tpe}\enspace
        i\thr{\E[\ret\,N]}{n} \p j\gv{N}{n} \p S}$
\\[2ex]
[get\_error$_i$]\enspace$\displaystyle%
  \frac{M \bg j \qquad n' \neq n}%
       {i\thr{\E[\get\,M]}{n'} \p j\gv{?}{n} \p S
        \enspace\sm{\Tpe}\enspace
        \error}$
\\[2ex]

[probe\_empty$_i$]\enspace$\displaystyle%
  \frac{M \bg j}%
       {i\thr{\E[\probe\,M]}{n} \p j\gv{ }{n} \p S
        \enspace\sm{\Tpe}\enspace
        i\thr{\E[\ret\;\false]}{n} \p j\gv{ }{n} \p S}$
\\[2ex]

[probe\_empty\_supervised\_threaded\_future$_i$]

\enspace$\displaystyle%
  \frac{M \bg j}%
       {i\thr{\E[\probe\,M]}{n} \p j\gv{k \thr{M}{n'}}{n} \p S
        \enspace\sm{\Tpe}\enspace
        i\thr{\E[\ret\;\false]}{n} \p j\gv{k \thr{M}{n'}}{n} \p S}$
\\[2ex]

[probe\_empty\_supervised\_sparked\_future$_i$]

\enspace$\displaystyle%
  \frac{M \bg j}%
       {i\thr{\E[\probe\,M]}{n} \p j\gv{k \supspk{M}{n'}}{n} \p S
        \enspace\sm{\Tpe}\enspace
        i\thr{\E[\ret\;\false]}{n} \p j\gv{k \supspk{M}{n'}}{n} \p S}$
\\[2ex]

[probe\_full$_i$]\enspace$\displaystyle%
  \frac{M \bg j}%
       {i\thr{\E[\probe\,M]}{n} \p j\gv{N}{n} \p S
        \enspace\sm{\Tpe}\enspace
        i\thr{\E[\ret\;\true]}{n} \p j\gv{N}{n} \p S}$
\\[2ex]

[probe\_error$_i$]\enspace$\displaystyle%
  \frac{M \bg j \qquad n' \neq n}%
       {i\thr{\E[\probe\,M]}{n'} \p j\gv{?}{n} \p S
        \enspace\sm{\Tpe}\enspace
        \error}$

\end{array}
\end{equation*}
\caption{Small-Step Transition Rules Embodying Future Operations.}
\label{fig:hdphrs-dsl_smallstep_sched-futures}
\end{figure}
#+END_LATEX

The transitions on futures are in Figure
\ref{fig:hdphrs-dsl_smallstep_sched-futures}. The ~get~ rule takes an
argument ~M~, which is reduced to ~IVar~ /j/ that holds the value
~M~. This is a blocking operation, and does not return until /j/ has
been filled with a value. When value ~N~ is written to the ~IVar~, it
is returned to the ~get~ function caller. For completeness, a rule
~get_error~ is included. This rule states that if the ~IVar~ resides
on a different node to the ~get~ function caller, then the program
will exit with an *Error*. This is because the model of HdpH-RS is
that \texttt{IVar}s always stay on the node that created them, and
reading a ~IVar~ remotely violates the model.

One difference between HdpH and HdpH-RS is that ~rput~ is hidden in
the HdpH-RS API. Nevertheless, it is used internally by the four spawn
rules, hence its inclusion in the HdpH-RS transition rules. It takes
two arguments $M$ and $N$. $M$ reduces to a ~IVar~ label /j/. $N$ is
the value to be written to the ~IVar~. In the ~rput_empty~ rule, the
future is filled with $N$ in thread $i$. In the
~rput_supervised_empty~ rule, the supervised future $j$ is either to
be filled with $N$, the value of executing supervised spark $k
\supspk{V}{n'}$ or thread $k \thr{V}{n'}$. The
/post/-state is a full future holding $N$.

The rule ~rput_full~ is triggered when an ~rput~ attempt is made to a
~IVar~ that already holds a value. The rule shows how this write
attempt is silently ignored, an important property for fault
tolerance. Section [[Duplicate Sparks]] explains how identical
computations may be raced against one another, with the first write
attempt to the ~IVar~ succeeding. The write semantics in monad-par is
different. If a second write attempt is made to an ~IVar~ in
monad-par, the program exits with an error because monad-par insists
on determinism.

The ~probe~ primitive is used by the scheduler to establish which
\texttt{IVar}s are empty at the point of node failure i.e. identifying
the sparks and threads that need replicating. In ~probe_empty~,
~probe~ takes an argument ~M~ reduced to /j/, an ~IVar~ label. If /j/
is empty i.e.  $j\gv{}{n}$, $j\gv{k \thr{V}{n'}}{n}$ or $j\gv{k
\supspk{V}{n'}}{n}$, then ~False~ is returned. The ~probe_full~ rule
is triggered when the ~IVar~ $j\gv{N}{n}$ is full, and ~True~ is
returned. As with ~get_error~, if a node attempts to probe a ~IVar~
that is does not host, an *Error* is thrown.

***** Spark Migration & Conversion

#+BEGIN_LATEX
\begin{figure}
\small%
\begin{equation*}
\renewcommand{\arraystretch}{2.0}
\begin{array}{p{.95\textwidth}}

[migrate\_spark$_j$]\enspace$\displaystyle%
  \frac{n' \neq n}%
    {j \spk{N}{n} \p S
       \enspace\sm{\Tpe}\enspace
       j \spk{N}{n'} \p S}$
\\[2ex]

[migrate\_supervised\_spark$_j$]\enspace$\displaystyle%
  \frac{n \neq n''}%
    {j \supspk{N}{n} \p i \gv{ j \supspk{N}{n} }{n'} \p S
       \enspace\sm{\Tpe}\enspace
       j \supspk{N}{n''} \p i \gv{ j \supspk{N}{n''} }{n} \p S}$
\\[2ex]

[convert\_spark$_j$]\enspace$\displaystyle%
  \frac{}%
   {j \spk{M}{n} \p S
   \enspace\sm{\Tpe}\enspace
   j \thr{M}{n} \p S}$
\\[2ex]

[convert\_supervised\_spark$_j$]\enspace$\displaystyle%
  \frac{}%
   {j \supspk{M}{n} \p S
   \enspace\sm{\Tpe}\enspace
   j \thr{M}{n} \p S}$

\end{array}
\end{equation*}
\caption{Small-Step Transition Rules For Spark Migration \& Conversion.}
\label{fig:hdph-dsl_smallstep_sched}
\end{figure}
#+END_LATEX

The spark migration and conversion transition rules are shown in
Figure \ref{fig:hdph-dsl_smallstep_sched}. The ~migrate_spark~
transition moves a supervised spark $j \spk{N}{n}$ from node $n$ to
$n'$. The ~migrate_supervised_spark~ transition modifies two
states. First, the supervised spark $j \supspk{N}{n}$ migrates to node
$n''$. Second, the state of its corresponding supervised future $i$ on
node $n'$ is modified to reflect this migration. Constraining the
migration of obsolete supervised spark replicas is in rule
~migrate_supervised_spark~. The $j$ label on the left hand side of the
rule ensures that only the youngest replica of a spark can
migrate. The ~convert_spark~ rule shows that the spark $j$ is
converted to a thread.

***** Fault Tolerance Rules

#+BEGIN_LATEX
\begin{figure}
\footnotesize%
\begin{equation*}
\renewcommand{\arraystretch}{2.0}
\begin{array}{p{.95\textwidth}}

[kill\_node$_n$]\enspace$\displaystyle%
  \frac{}%
      {S \enspace\sm{\Tpe}\enspace i:dead_n \p S}$
\\[2ex]

[kill\_spark$_i$]\enspace$\displaystyle%
  \frac{}%
      {i \spk{M}{n} \p j\:dead_n \p S \enspace\sm{\Tpe}\enspace j\:dead_n \p S}$
\\[2ex]

[kill\_supervised\_spark$_i$]\enspace$\displaystyle%
  \frac{}%
      {i \supspk{M}{n} \p j\:dead_n \p S \enspace\sm{\Tpe}\enspace j\:dead_n \p S}$
\\[2ex]

[kill\_thread$_i$]\enspace$\displaystyle%
  \frac{}%
      {i \thr{M}{n} \p j:dead_n \p S \enspace\sm{\Tpe}\enspace j:dead_n \p S}$
\\[2ex]

[kill\_ivar$_i$]\enspace$\displaystyle%
  \frac{}%
      {i \gv{?}{n} \p j:dead_n \p S \enspace\sm{\Tpe}\enspace j:dead_n \p S}$
\\[5ex]

[recover\_supervised\_spark$_j$]\enspace$\displaystyle%
  \frac{n' \neq n \qquad k \notin \dom(S) \qquad |\{i,j,k,p\}|=4}%
      {p:dead_{n'} \p  i \gv{ j \supspk{N}{n'} }{n} \p S
        \enspace\sm{\Tpe}\enspace
         p:dead_{n'} \p i \gv{ k \supspk{N}{n} }{n} \p k \supspk{N}{n} \p S}$
\\[1.5ex]

[recover\_supervised\_thread$_j$]\enspace$\displaystyle%
  \frac{n' \neq n \qquad k \notin \dom(S) \qquad |\{i,j,k,p\}|=4}%
      {p:dead_{n'} \p  i \gv{ j \thr{N}{n'} }{n} \p S
        \enspace\sm{\Tpe}\enspace
         p:dead_{n'} \p i \gv{ k \thr{N}{n} }{n} \p k \thr{N}{n} \p S}$
\end{array}
\end{equation*}
\caption{Small-step transition rules embodying HdpH-RS Task Recovery.}
\label{fig:hdphrs-dsl_smallstep_recovery}
\end{figure}
#+END_LATEX

The rules for killing nodes, losing sparks, threads and
\texttt{IVar}s, and recovering sparks and threads are shown in Figure
\ref{fig:hdphrs-dsl_smallstep_recovery}. Nodes can fail at any
moment. Thus, there are no conditions for triggering the ~kill_node~
rule. It adds a new state $i:dead_n$ indicating node $n$ is dead. When
a node fails, all state on that node is also removed from state
$S$. The presence of $dead_n$ means that sparks, threads and
\texttt{IVar}s on $n$ are lost with rules ~kill_spark~, ~kill_thread~
and ~kill_ivar~.

The transition rules for fault tolerance in HdpH-RS are
~recover_supervised_spark~ and ~recover_supervised_thread~, shown in
Figure \ref{fig:hdphrs-dsl_smallstep_recovery}. They are internal
transitions, and fault recovery is done by the scheduler, not the
programmer. There are two considerations for recovering sparks and
threads.

1) The candidate tasks for replication are those whose most recent
   tracked location was the failed node.
2) Of these tasks, they are rescheduled if not yet evaluated
   i.e. its corresponding ~IVar~ is empty.

The ~recover_supervised_spark~ rule is triggered if node $n'$ has
died, and a ~IVar~ /j/ is empty, shown as $j \gv{ \spk{N}{n'}
}{n}$. As node $n'$ is dead, ~IVar~ /j/ will only be filled if the
task is rescheduled. The post-state includes a new supervised spark
/k/ that will write to /j/. The spark is allocated initially into the
sparkpool on node $n$, the node that hosts the ~IVar~.

The same pre-conditions apply for triggering ~recover_thread~. That
is, a thread needs re-scheduling if it has been pushed to node $n'$
with ~supervisedSpawnAt~, and the corresponding ~IVar~ /j/ is
empty. The post-state is a thread /k/ in the threadpool of node $n$
that also hosts /j/. This thread cannot migrate to another node.

*** Execution of Transition Rules

This section uses the transition rules from Section [[Small Step
Operational Semantics]] to execute 4 simple programs. The first shows a
sequence of transitions originating from ~supervisedSpawn~ in the
absence of faults. The second shows the execution of ~supervisedSpawn~
in the presence of a node failure. The third shows the execution of
~supervisedSpawnAt~ in the presence of a node failure. The fourth
shows another execution of ~supervisedSpawn~ in the presence of
failure, and demonstrates the non-deterministic semantics of multiple
~rput~ attempts.

The execution of $3+3$ using ~supervisedSpawn~ is shown in Figure
\ref{fig:supervisedSpawn-execution-nofaults}. It creates ~IVar~ with
the label $2$, and spark with the label $3$. The spark migrates to
node $n'$, where it is converted to a thread. The $3+3$ expression is
evaluated and bound. The ~rput_empty~ rule then fills the ~IVar~ $2$
with the value $6$.

#+BEGIN_LATEX
\begin{figure}
\begin{align*}
&\rightarrow 1 \thr{ supervisedSpawn\; (eval\; (3+3)) }{n} \\
\shortintertext{[supervisedSpawn$_1$]}
&\rightarrow 1 \thr{ \ret\;2 }{n} \p 2\gv{ 3 \supspk{eval\; (3+3) \bind \rputi\, 2}{n}}{n} \p 3 \supspk{eval\; (3+3) \bind \rputi\, 2}{n} \\
\shortintertext{[migrate\_supervised\_spark$_3$]}
&\rightarrow 1 \thr{ \ret\;2 }{n} \p 2\gv{ 3 \supspk{eval\; (3+3) \bind \rputi\, 2}{n'}}{n} \p 3 \supspk{eval\; (3+3) \bind \rputi\, 2}{n'} \\
\shortintertext{[convert\_supervised\_spark$_3$]}
&\rightarrow 1 \thr{ \ret\;2 }{n} \p 2\gv{ 3 \supspk{eval\; (3+3) \bind \rputi\, 2}{n'}}{n} \p 3 \thr{eval\; (3+3) \bind \rputi\, 2}{n'} \\
\shortintertext{[eval$_3$]}
&\rightarrow 1 \thr{ \ret\;2 }{n} \p 2\gv{ 3 \supspk{eval\; (3+3) \bind \rputi\, 2}{n'}}{n} \p 3 \thr{\ret\, 6 \bind \rputi\, 2}{n'} \\
\shortintertext{[bind$_3$]}
&\rightarrow 1 \thr{ \ret\;2 }{n} \p 2\gv{ 3 \supspk{eval\; (3+3) \bind \rputi\, 2}{n'}}{n} \p 3 \thr{\rputi\, 2\; 6}{n'} \\
\shortintertext{[rput\_empty\_supervised\_sparked\_future$_3$]}
&\rightarrow 1 \thr{ \ret\;2 }{n} \p 2\gv{ 6 }{n} \p 3 \thr{\ret\,\unit}{n'}
\end{align*}
\caption{Migration \& Execution of Supervised Spark in Absence of Faults}
\label{fig:supervisedSpawn-execution-nofaults}
\end{figure}
#+END_LATEX

The execution in Figure \ref{fig:supervisedSpawn-execution-faults}
evaluates $2+2$ with ~supervisedSpawn~ and includes a failure. As in Figure
\ref{fig:supervisedSpawn-execution-nofaults}, spark $3$ is migrated to
node $n'$. At this point, node $n'$ fails. The ~kill_supervised_spark~
also removes spark $3$, which resided on $n'$ when it failed. The
~recover_supervised_spark~ rule is triggered, creating a new spark $5$
hosted on node $n$. Spark $5$ migrates to node $n''$. Here, it is
converted to a thread. The expression $2+2$ is evaluated to 4. This
value is ~rput~ to future $2$, from thread $5$ on $n''$.

#+BEGIN_LATEX
\begin{figure}
\begin{align*}
&\rightarrow 1 \thr{ supervisedSpawn\; (eval\; (2+2)) }{n} \\
\shortintertext{[supervisedSpawn$_1$]}
&\rightarrow 1 \thr{ \ret\;2 }{n} \p 2\gv{ 3 \supspk{eval\; (2+2) \bind \rputi\, 2}{n}}{n} \p 3 \supspk{eval\; (2+2) \bind \rputi\, 2}{n} \\
\shortintertext{[migrate\_supervised\_spark$_3$]}
&\rightarrow 1 \thr{ \ret\;2 }{n} \p 2\gv{ 3 \supspk{eval\; (2+2) \bind \rputi\, 2}{n'}}{n} \p 3 \supspk{eval\; (2+2) \bind \rputi\, 2}{n'} \\
\shortintertext{[kill\_node$_{n'}$]}
&\rightarrow 1 \thr{ \ret\;2 }{n} \p 2\gv{ 3 \supspk{eval\; (2+2) \bind \rputi\, 2}{n'}}{n} \p 3 \supspk{eval\; (2+2) \bind \rputi\, 2}{n'} \p 4:dead_{n'} \\
\shortintertext{[kill\_supervised\_spark$_3$]}
&\rightarrow 1 \thr{ \ret\;2 }{n} \p 2\gv{ 3 \supspk{eval\; (2+2) \bind \rputi\, 2}{n'}}{n} \p 4:dead_{n'} \\
\shortintertext{[recover\_supervised\_spark$_3$]}
&\rightarrow 1 \thr{ \ret\;2 }{n} \p 2\gv{ 5 \supspk{eval\; (2+2) \bind \rputi\, 2}{n}}{n} \p 4:dead_{n'} \p 5 \supspk{eval\; (2+2) \bind \rputi\, 2}{n} \\
\shortintertext{[migrate\_supervised\_spark$_5$]}
&\rightarrow 1 \thr{ \ret\;2 }{n} \p 2\gv{ 5 \supspk{eval\; (2+2) \bind \rputi\, 2}{n''}}{n} \p 4:dead_{n'} \p 5 \supspk{eval\; (2+2) \bind \rputi\, 2}{n''} \\
\shortintertext{[convert\_supervised\_spark$_5$]}
&\rightarrow 1 \thr{ \ret\;2 }{n} \p 2\gv{ 5 \supspk{eval\; (2+2) \bind \rputi\, 2}{n''}}{n} \p 4:dead_{n'} \p 5 \thr{eval\; (2+2) \bind \rputi\, 2}{n''} \\
\shortintertext{[eval$_5$]}
&\rightarrow 1 \thr{ \ret\;2 }{n} \p 2\gv{ 5 \supspk{eval\; (2+2) \bind \rputi\, 2}{n''}}{n} \p 4:dead_{n'} \p 5 \thr{\ret\, 4 \bind \rputi\, 2}{n''} \\
\shortintertext{[bind$_5$]}
&\rightarrow 1 \thr{ \ret\;2 }{n} \p 2\gv{ 5 \supspk{eval\; (2+2) \bind \rputi\, 2}{n''}}{n} \p 4:dead_{n'} \p 5 \thr{\rputi\, 2\; 4}{n''} \\
\shortintertext{[rput\_empty\_supervised\_sparked\_future$_5$]}
&\rightarrow 1 \thr{ \ret\;2 }{n} \p 2\gv{ 4 }{n} \p 4:dead_{n'} \p 5 \thr{\ret\,\unit}{n''}
\end{align*}
\caption{Migration \& Execution of Supervised Spark in Presence of a Fault}
\label{fig:supervisedSpawn-execution-faults}
\end{figure}
#+END_LATEX

The execution in Figure \ref{fig:supervisedSpawnAt-execution-faults}
evaluates $9+2$ with ~supervisedSpawnAt~ and includes a failure. The
~supervisedSpawnAt~ rule creates a future $2$, and thread $3$ on node
$n'$. The $9+2$ expression is evaluated. The next transition is
~kill_node~ in relation to node $n''$, where the thread resides. The
~kill_supervised_thread~ rule removes thread $3$ from the abstract
state machine. The thread is replicated as thread $5$ on the same node
$n$ as the future. The $9+2$ expression is once again evaluated. The
future $2$ is filled with value $11$ from thread $5$, also on node $n$.

#+BEGIN_LATEX
\begin{figure}
\begin{align*}
&\rightarrow 1 \thr{ supervisedSpawnAt\; (eval\; 9+2)\; n' }{n} \\
\shortintertext{[supervisedSpawnAt$_1$]}
&\rightarrow 1 \thr{ \ret\;2 }{n} \p 2\gv{ 3 \thr{eval\; (9+2) \bind \rputi\, 2}{n'}}{n} \p 3 \thr{eval\; (9+2) \bind \rputi\, 2}{n'} \\
\shortintertext{[eval$_3$]}
&\rightarrow 1 \thr{ \ret\;2 }{n} \p 2\gv{ 3 \supspk{eval\; (9+2) \bind \rputi\, 2}{n'}}{n} \p 3 \thr{\ret\, 11 \bind \rputi\, 2}{n'} \\
\shortintertext{[kill\_node$_{n'}$]}
&\rightarrow 1 \thr{ \ret\;2 }{n} \p 2\gv{ 3 \supspk{eval\; (9+2) \bind \rputi\, 2}{n'}}{n} \p 3 \thr{\ret\, 11 \bind \rputi\, 2}{n'} \p 4:dead_{n'} \\
\shortintertext{[kill\_supervised\_thread$_3$]}
&\rightarrow 1 \thr{ \ret\;2 }{n} \p 2\gv{ 3 \supspk{eval\; (9+2) \bind \rputi\, 2}{n'}}{n} \p 4:dead_{n'} \\
\shortintertext{[recover\_supervised\_thread$_3$]}
&\rightarrow 1 \thr{ \ret\;2 }{n} \p 2\gv{ 5 \thr{eval\; (9+2) \bind \rputi\, 2}{n}}{n} \p 4:dead_{n'} \p 5 \thr{eval\; (9+2) \bind \rputi\, 2}{n} \\
\shortintertext{[eval$_5$]}
&\rightarrow 1 \thr{ \ret\;2 }{n} \p 2\gv{ 5 \thr{eval\; (9+2) \bind \rputi\, 2}{n}}{n} \p 4:dead_{n'} \p 5 \thr{\ret\, 11 \bind \rputi\, 2}{n} \\
\shortintertext{[bind$_5$]}
&\rightarrow 1 \thr{ \ret\;2 }{n} \p 2\gv{ 5 \thr{eval\; (9+2) \bind \rputi\, 2}{n}}{n} \p 4:dead_{n'} \p 5 \thr{\rputi\, 2\; 11}{n} \\
\shortintertext{[rput\_empty\_supervised\_threaded\_future$_5$]}
&\rightarrow 1 \thr{ \ret\;2 }{n} \p 2\gv{ 11 }{n} \p 4:dead_{n'} \p 5 \thr{\ret\,\unit}{n}
\end{align*}
\caption{Migration \& Execution of Supervised Thread in Presence of Fault}
\label{fig:supervisedSpawnAt-execution-faults}
\end{figure}
#+END_LATEX

The execution in Figure
\ref{fig:supervisedSpawn-execution-faults-multiple-rputs} demonstrates
the non-deterministic semantics of multiple ~rput~ attempts. This
occurs when an intermittently faulty node executes an ~rput~ call. The
node's failure may have been detected, though the sparks and threads
it hosts may not necessarily be lost immediately. It presents a
scenario where a supervised spark is replicated. An obsolete replica
is an old copy of a spark. The semantics guarantee that obsolete
replicas cannot be migrated, in the ~migrate_supervised_spark~
rule. The implementation that ensures obsolete replicas are not
migrated is described in Section [[Supervised Empty Future State]].

The ~supervisedSpawn~ primitive is used to create an empty future $2$
and spark $3$ on node $n$. Converting and executing the spark will
write the value of expression $2+2$ to future $2$. Spark $3$ is
migrated to node $n'$. Next, a failure of node $n'$ is perceived,
triggering the ~kill_node~ rule. Spark $3$ is recovered by replicating
it as spark $5$, again on node $n$. The label for the spark inside the
future $2$ has changed from $3$ to $5$ by the
~recover_supervised_spark~ rule. This ensures that the
~migrate_supervised_spark~ rule can no longer be triggered for the
obsolete spark $3$ on node $n'$.

Spark $5$ migrates to a 3rd node $n''$. Here, it is converted to a
thread and the $2+2$ expression evaluated to $4$. Despite the failure
detection on node $n'$, it has not yet completely failed. Thus, spark
$3$ on node $n'$ has not yet been killed with ~kill_spark~. It is
converted to a thread and the $2+2$ expression evaluated to $4$. The
~rput_empty~ rule is triggered for the ~rput~ call in thread $3$ on
node $n'$. The future $2$ is now full with the value $4$. The ~rput~
call is then executed in thread $5$. This triggers the ~rput_full~
rule. That is, future $2$ is already full, so the ~rput~ attempt on
node $n''$ is silently ignored on node $n$. The non-deterministic
semantics of ~rput_empty~ and ~rput_full~ require the side effect of
expressions in sparks and threads to be idempotent
\cite{DBLP:conf/popl/RamalingamV13} i.e. side effects whose repetition
cannot be observed. The scenario in Figure
\ref{fig:supervisedSpawn-execution-faults-multiple-rputs} uses a pure
$2+2$ expression (i.e. with idempotent side effects) to demonstrate
spark recovery.

#+BEGIN_LATEX
\begin{figure}
\begin{align*}
\; &\rightarrow 1 \thr{ supervisedSpawn\; (eval\; (2+2)) }{n} \\
\shortintertext{[supervisedSpawn$_1$]}
&\rightarrow 1 \thr{ \ret\;2 }{n} \p 2\gv{ 3 \supspk{eval\; (2+2) \bind \rputi\, 2}{n}}{n} \p 3 \supspk{eval\; (2+2) \bind \rputi\, 2}{n} \\
\shortintertext{[migrate\_supervised\_spark$_3$]}
&\rightarrow 1 \thr{ \ret\;2 }{n} \p 2\gv{ 3 \supspk{eval\; (2+2) \bind \rputi\, 2}{n'}}{n} \p 3 \supspk{eval\; (2+2) \bind \rputi\, 2}{n'} \\
\shortintertext{[kill\_node$_{n'}$]}
&\rightarrow 1 \thr{ \ret\;2 }{n} \p 2\gv{ 3 \supspk{eval\; (2+2) \bind \rputi\, 2}{n'}}{n} \p 3 \supspk{eval\; (2+2) \bind \rputi\, 2}{n'} \p 4:dead_{n'} \\
\shortintertext{[recover\_supervised\_spark$_3$]}
&\rightarrow 1 \thr{ \ret\;2 }{n} \p 2\gv{ 5 \supspk{eval\; (2+2) \bind \rputi\, 2}{n}}{n} \p 3 \supspk{eval\; (2+2) \bind \rputi\, 2}{n'} \p 4:dead_{n'} \\
&\qquad \p 5 \supspk{eval\; (2+2) \bind \rputi\, 2}{n} \\
\shortintertext{[migrate\_supervised\_spark$_5$]}
&\rightarrow 1 \thr{ \ret\;2 }{n} \p 2\gv{ 5 \supspk{eval\; (2+2) \bind \rputi\, 2}{n''}}{n} \p 3 \supspk{eval\; (2+2) \bind \rputi\, 2}{n'} \p 4:dead_{n'} \\
&\qquad \p 5 \supspk{eval\; (2+2) \bind \rputi\, 2}{n''} \\
\shortintertext{[convert\_supervised\_spark$_5$]}
&\rightarrow 1 \thr{ \ret\;2 }{n} \p 2\gv{ 5 \supspk{eval\; (2+2) \bind \rputi\, 2}{n''}}{n} \p 3 \supspk{eval\; (2+2) \bind \rputi\, 2}{n'} \p 4:dead_{n'} \\
&\qquad \p 5 \thr{eval\; (2+2) \bind \rputi\, 2}{n''} \\
\shortintertext{[eval$_5$]}
&\rightarrow 1 \thr{ \ret\;2 }{n} \p 2\gv{ 5 \supspk{eval\; (2+2) \bind \rputi\, 2}{n''}}{n} \p 3 \supspk{eval\; (2+2) \bind \rputi\, 2}{n'} \p 4:dead_{n'} \\
&\qquad \p 5 \thr{\ret\, 4 \bind \rputi\, 2}{n''} \\
\shortintertext{[bind$_5$]}
&\rightarrow 1 \thr{ \ret\;2 }{n} \p 2\gv{ 5 \supspk{eval\; (2+2) \bind \rputi\, 2}{n''}}{n} \p 3 \supspk{eval\; (2+2) \bind \rputi\, 2}{n'} \p 4:dead_{n'} \\
&\qquad \p 5 \thr{\rputi\, 2\; 4}{n''} \\
\shortintertext{[convert\_supervised\_spark$_3$]}
&\rightarrow 1 \thr{ \ret\;2 }{n} \p 2\gv{ 5 \supspk{eval\; (2+2) \bind \rputi\, 2}{n''}}{n} \p 3 \thr{eval\; (2+2) \bind \rputi\, 2}{n'} \p 4:dead_{n'} \\
&\qquad \p 5 \thr{eval\; (2+2) \bind \rputi\, 2}{n''} \\
\shortintertext{[eval$_3$]}
&\rightarrow 1 \thr{ \ret\;2 }{n} \p 2\gv{ 5 \supspk{eval\; (2+2) \bind \rputi\, 2}{n''}}{n} \p 3 \thr{\ret\, 4 \bind \rputi\, 2}{n'} \p 4:dead_{n'} \\
&\qquad \p 5 \thr{\ret\, 4 \bind \rputi\, 2}{n''} \\
\shortintertext{[bind$_3$]}
&\rightarrow 1 \thr{ \ret\;2 }{n} \p 2\gv{ 5 \supspk{eval\; (2+2) \bind \rputi\, 2}{n''}}{n} \p 3 \thr{\rputi\, 2\; 4}{n'} \p 4:dead_{n'} \\
&\qquad \p 5 \thr{\rputi\, 2\; 4}{n''} \\
\shortintertext{[rput\_empty\_supervised\_sparked\_future$_3$]}
&\rightarrow 1 \thr{ \ret\;2 }{n} \p 2\gv{ 4 }{n} \p 3 \thr{\ret\,\unit}{n'} \p 4:dead_{n'} \p 5 \thr{\rputi\, 2\; 4}{n''}
\shortintertext{[rput\_full$_5$]}
&\rightarrow 1 \thr{ \ret\;2 }{n} \p 2\gv{ 4 }{n} \p 3 \thr{\ret\,\unit}{n'} \p 4:dead_{n'} \p 5 \thr{\ret\,\unit}{n''}
\end{align*}
\caption{Duplicating Sparks \& \texttt{rput} Attempts}
\label{fig:supervisedSpawn-execution-faults-multiple-rputs}
\end{figure}
#+END_LATEX


** Designing a Fault Tolerant Scheduler

*** Work Stealing Protocol

This section presents the protocol for fault tolerant work
stealing. It adds resiliency to the scheduling of sparks and
threads. These are non-preemptive tasks that are load balanced between
overloaded and idle nodes. Non-preemptive tasks are tasks that cannot
be migrated once evaluation has begun, in contrast to a partially
evaluated preemptive task that /can/ be migrated
\cite{DBLP:journals/computer/ShivaratriKS92}.

The fishing protocol in HdpH involves a victim and a thief. The
HdpH-RS /fault tolerant/ fishing protocol involves a third node --- a
/supervisor/. A supervisor is the node where a supervised spark was
created. The runtime system messages in HdpH-RS serve two
purposes. First, to schedule sparks from heavily loaded nodes to idle
nodes. Second, to allow supervisors to track the location of
supervised sparks as they migrate between nodes.

The UML Message Sequence Notation (MSC) is used extensively in this
section. An MSC \LaTeX macro package \cite{MB02} has been extended to
draw all MSC figures. The fault oblivious fishing protocol in HdpH is
shown in Figure \ref{fig:hdph-workstealing-protocol}. The fault
tolerant fishing protocol in HdpH-RS is shown in Figure
\ref{fig:hdphrs-pessimistic-workstealing-protocol}. In this
illustration, an idle /thief/ node C targets a /victim/ node B by
sending a ~FISH~ message. The victim requests a scheduling
authorisation from the /supervisor/ with ~REQ~. The supervisor grants
authorisation with ~AUTH~, and a spark is scheduled from the victim to
the thief in a ~SCHEDULE~ message. When the thief receives this, it
sends an ~ACK~ to the supervisor.

#+CAPTION: Fault Oblivious Fishing Protocol in HdpH
#+LABEL:      fig:hdph-workstealing-protocol
#+ATTR_LaTeX: :width 80mm
[[./img/chp3/msc/hdph-fishing-simple.pdf]]

#+CAPTION: Fault Tolerant Fishing Protocol in HdpH-RS
#+LABEL:      fig:hdphrs-pessimistic-workstealing-protocol
#+ATTR_LaTeX: :width 70mm
[[./img/chp3/msc/hdphrs-fishing-protocol-no-tracking.pdf]]

**** RTS Messages to Support the Work Stealing Protocol

The HdpH-RS RTS messages are described in Table
\ref{tab:hdphrs-scheduling-messages}. The /Message/ header is the
message type, the /From/ and /To/ fields distinguish a supervisor node
(S) and worker nodes (W), and /Description/ shows the purpose of the
message. The use of each message are described in the scheduling
algorithms in Section [[Fault Tolerant Scheduling Algorithm]].

#+BEGIN_LATEX
\begin{table}\footnotesize
\begin{center}
\begin{tabular}{|l|c|c|p{9cm}|}
\hline
\textbf{Message} & \textbf{From} & \textbf{To} & \textbf{Description} \\
\hline
\hline
\multicolumn{4}{|l|}{RTS Messages inherited from HdpH} \\
\hline
\texttt{FISH} \emph{thief}               & W    & W  & Fishing request from a thief. \\
\texttt{SCHEDULE} \emph{spark victim}    & W    & W  & Victim schedules a spark to a thief. \\
\texttt{NOWORK}                     & W    & W  & Response to \texttt{FISH}: victim informs thief that it either does not hold a spark, or was not authorised to schedule a spark. \\
\hline
\hline
\multicolumn{4}{|l|}{HdpH-RS RTS Messages for task supervision \& fault detection} \\
\hline
\texttt{REQ} \emph{ref seq victim thief} & W    & S  & Victim requests authorisation from supervisor to schedule spark to a thief. \\
\texttt{DENIED} \emph{thief}             & S    & W  & Supervisor denies a scheduling request with respect to \texttt{REQ}. \\
\texttt{AUTH} \emph{thief}          & S    & W  & Supervisor authorises a scheduling request with respect to \texttt{REQ}. \\
\texttt{OBSOLETE} \emph{thief}           & S    & W  & In response to \texttt{REQ}: the task waiting to be scheduled by victim is an obsolete task copy. Victim reacts to \texttt{OBSOLETE} by discarding task and sending \texttt{NOWORK} to thief. \\
\texttt{ACK} \emph{ref seq thief}              & W    & S  & Thief sends an \texttt{ACK} to the supervisor of a spark it has received. \\
\texttt{DEADNODE} \emph{node}            & S    & S  & Transport layer informs supervisor to reschedule sparks that \emph{may} have been lost on failed node. \\
\texttt{DEADNODE} \emph{node}            & W    & W  & Transport layer informs thief to stop waiting for a reply to a \texttt{FISH} sent to failed victim. \\
\hline
\end{tabular}
\caption{HdpH-RS RTS Messages}
\label{tab:hdphrs-scheduling-messages}
\end{center}
\end{table}
#+END_LATEX

*** Task Locality

For the supervisor to determine whether a spark is lost when a remote
node has failed, the migration of a supervised spark needs to be
tracked. This is made possible from the RTS messages ~REQ~ and ~ACK~
described in Section [[RTS Messages]].

#+CAPTION: Migration Tracking with Message Passing
#+LABEL:      fig:migration-modification
#+ATTR_LaTeX: :width 90mm
[[./img/chp3/msc/hdphrs-fishing-protocol.pdf]]

Task migration tracking is in shown in Figure
\ref{fig:migration-modification}. The messages ~REQ~ and ~ACK~ are
received by the supervising node A to keep track of a spark's
location. Sparks and threads can therefore be identified by their
corresponding globalised ~IVar~. If a spark is created with
~supervisedSpawn~, then the supervised spark's structure is composed
of three parts. First, an identifier corresponding the IVar that will
be filled by evaluating the task expression in the spark.  Second, a
replica number of the spark. Third, the task expression inside the
spark to be evaluated. In Figure \ref{fig:migration-modification} the
~IVar~ is represented as ~iX~ e.g. ~i2~, and the replica number as
~rX~ e.g. ~r2~. They are used by the victim node B to request
scheduling authorisation, and by thieving node C to acknowledge the
arrival of the supervised spark.

The message ~REQ~ is used to request authorisation to schedule the
spark to another node. If the supervisor knows that it is in the
sparkpool of a node (i.e. \texttt{OnNode~thief}) then it will authorise
the fishing request with ~AUTH~. If the supervisor believes it is
in-flight between two nodes (i.e. \texttt{InTransition~victim~thief})
then it will deny the request with ~DENIED~. An example of this is
shown in Figure \ref{fig:denied-workstealing}. It is covered by
Algorithm \ref{alg:handleReq} later in Section [[Fault Tolerant
Scheduling Algorithm]].


#+CAPTION:    Failed Work Stealing Attempt When Spark Migration is Not Yet \texttt{ACK}d
#+LABEL:      fig:denied-workstealing
#+ATTR_LaTeX: :width 120mm
[[./img/chp3/msc/req-denied.pdf]]

A slightly more complex fishing scenario is shown in Figure
\ref{fig:denied-workstealing}. It is covered by Algorithm
\ref{alg:handleFish}. Node C has sent a fish to node B, prompting an
authorisation request to the supervisor, node A. During this phase,
node D sends a fish to node B. As B is already a victim of node C, the
fish is rejected with a ~NOWORK~ response.

#+CAPTION:    Fish Rejected from a Victim of Another Thief
#+LABEL:      fig:normal-workstealing
#+ATTR_LaTeX: :width 110mm
[[./img/chp3/msc/fish-rejected.pdf]]


**** Tracking the Migration of Supervised Tasks

Section [[Task Locality]] has so far described the migration tracking of
one task. In reality, many calls of the spawn family of primitives per
node are likely. A reference for a supervised spark
or thread is identified by the globalised ~IVar~ that it will fill.

#+BEGIN_LATEX
\begin{Code}
\begin{haskellcode}{Par Computation That Modifies Local Registry on Node A}{lst:registry-modification-example}
-- | Par computation that generates registry entries in Table @\ref{tab:local-registry}@
foo :: Int -> Par Integer
foo x = do
    ivar1 <- supervisedSpawnAt $(mkClosure [| f x |]) nodeB
    ivar2 <- supervisedSpawn   $(mkClosure [| h x |])
    ivar2 <- supervisedSpawn   $(mkClosure [| j x |])
    ivar2 <- supervisedSpawn   $(mkClosure [| k x |])
    ivar2 <- supervisedSpawnAt $(mkClosure [| m x |]) nodeD
    x <- get ivar1
    y <- get ivar2
    {- omitted -}
\end{haskellcode}
\end{Code}
#+END_LATEX

#+CAPTION: Local Registry on Node A When Node B Fails
#+LABEL:   tab:local-registry
#+ATTR_LaTeX: :mode table :align |l||c|c|c|
|-----------------------------------+----------------+--------------------+------------|
| Function Call on A                | IVar Reference | Location           | Vulnerable |
|-----------------------------------+----------------+--------------------+------------|
| $supervisedSpawnAt\; (f\,x)\; B$  |              1 | ~OnNode B~         | $\star$    |
| $supervisedSpawn\quad\:\, (h\,x)$ |              2 | ~InTransition D C~ |            |
| $supervisedSpawn\quad\:\, (j\,x)$ |              3 | ~InTransition B C~ | $\star$    |
| $supervisedSpawn\quad\:\, (k\,x)$ |              4 | ~OnNode A~         |            |
| $supervisedSpawnAt\; (m\,x)\; D$  |              5 | ~OnNode D~         |            |
|-----------------------------------+----------------+--------------------+------------|

The migration trace for a supervised spark or thread is held within
the state of its associated empty ~IVar~. A simple local ~IVar~
registry is shown in Table \ref{tab:local-registry}, generated be
executing the code fragment in Listing
\ref{lst:registry-modification-example} is executed on node A. Five
futures (\texttt{IVar}s) have been created on node A. Node A expects
~REQ~ and ~ACK~ messages about each future it supervises, from victims
and thieves respectively. The registry is used to identify which
sparks and threads need recovering. Table \ref{tab:local-registry}
shows which tasks are at-risk when the failure of node B is detected
by node A. If node A receives a \texttt{DEADNODE B} message from the
transport layer, the tasks for \texttt{IVar}s 1 and 3 are
replicated. The replication of task 3 /may/ lead to duplicate copies
if it has arrived at node C before node B failed. The management of
duplicate sparks is described in Section [[Duplicate Sparks]].

**** Fishing Hops

The HdpH scheduler supports fishing hops, but the HdpH-RS scheduler
does not. Hops allow a thief to scrutinise a number of potential
victim targets for work, before it gives up and attempts again after a
random delay. A thief will transmit a fish message to a random
target. If that target does not have sparks to offer, the victim will
forward the fish on behalf of the thief. Each fish request is given a
maximum /hop count/. That is, the number of times it will be forwarded
before a ~NOWORK~ message is returned to the thief.

Idle nodes proactively fish for sparks on other nodes. When they send
a fish, they do not send another until they receive a reply from a
targeted node. The expected reply is ~SCHEDULE~ with a spark, or
~NOWORK~. Algorithm \ref{alg:handleDeadnode} shows how a node will
stop waiting for a reply when a ~DEADNODE~ message is received about
the chosen victim.

#+CAPTION:    (Hypothetical) Fishing Reply Deadlock
#+LABEL:      fig:fishing-reply-block
#+ATTR_LaTeX: :width 120mm
[[./img/chp3/msc/hops-deadlock.pdf]]

If hops were supported, thieves may potentially be deadlocked while
waiting for a fishing reply. A simple scenario is shown in Figure
\ref{fig:fishing-reply-block}. A spark is fished from node A to
B. Node D sends a fish to C, which is forwarded to node B. Node B
receives the fish, but fails before sending a reply to D. Although D
/will/ receive a \texttt{DEADNODE~B} message, it will not reset the
fishing lock. It is expecting a reply from C, and is oblivious to the
fact that the fish was forwarded to B. While there could be mechanisms
for supporting hops, for example fishing with timeouts, it would
complicate both the work stealing algorithms (Section [[Fault Tolerant
Scheduling Algorithm]]) and also the formal verification of the protocol
in Chapter [[The Validation of Reliable Distributed Scheduling for
HdpH-RS]].

**** Guard Posts

The fishing protocol actively involves the supervisor in the migration
of a spark. When a victim is targeted, it sends an authorisation
request to the supervisor of a candidate local spark. The response
from the supervisor indicates whether the candidate spark should be
scheduled to the thief, or whether it is an obsolete spark.

The /guard post/ is node state with capacity to hold one
spark. Candidate sparks are moved from the sparkpool to the guard post
while authorisation is pending. From there, one of three things can
happen. First, the candidate spark may be scheduled to a thief if the
victim is authorised to do so. Second, the candidate may be moved back
into the sparkpool if authorisation is denied. Third, it may be
removed from the guard post and discarded if the supervisor identifies
it as an obsolete spark.

*** Duplicate Sparks

In order to ensure the safety of supervised sparks, the scheduler makes
pessimistic assumptions that tasks have been lost when a node
fails. If a supervisor is certain that a supervised spark was on the
failed node, then it is replicated. If a supervisor believes a
supervised spark to be in-flight either towards or away from the
failed node during a fishing operation, again the supervised spark is
replicated. The consequence is that the scheduler may create
duplicates.

Duplicates of the same spark can co-exist in a distributed environment
with one constraint. Older obsolete spark replicas are not permitted
to migrate through work stealing, as multiple migrating copies with
the same reference may potentially lead to inconsistent location
tracking (Section [[Task Locality]]). However, they /are/ permitted to
transmit results to \texttt{IVar}s using ~rput~. Thanks to
idempotence, this scenario is indistinguishable from the one where the
obsolete replica has been lost.

#+BEGIN_QUOTE
"Idempotence is a correctness criterion that requires the system to
tolerate duplicate requests, is the key to handling both communication
and process failures efficiently. Idempotence, when combined with
retry, gives us the essence of a workflow, a fault tolerant composition
of atomic actions, for free without the need for distributed
coordination". \cite{DBLP:conf/popl/2013}
#+END_QUOTE

#+CAPTION: Pessimistic Scheduling that Leads to Spark Duplication
#+LABEL:      fig:pessimistic-task-duplication
#+ATTR_LaTeX: :width 110mm
[[./img/chp3/msc/spark-duplication.pdf]]

This possibility is illustrated in Figure
\ref{fig:pessimistic-task-duplication}. A supervised spark has been created with
~supervisedSpawn~ on node A. It has been fished by node B. During a fishing
phase between B and C, B fails. Supervising node A has not yet
received an ~ACK~ from C, and pessimistically replicates the spark
locally. In this instance, the original spark did survive the
failure of node B. There are now two copies of the same supervised
spark in the distributed environment.

Location tracking for a task switches between two states, ~OnNode~ and
~InTransition~, when a supervisor receives either a ~ACK~ and ~REQ~
message about the spark. The spark is identified by a pointer to an
~IVar~. The strategy of replicating an in-transition task is
pessimistic --- the task may survive a node failure depending on which
came first: a successful ~SCHEDULE~ transmission, or node failure
detection. This requires a strategy for avoiding possible task
tracking race conditions, if there exists more than one replica.

**** Replica Numbers in \texttt{ACK} Messages

#+CAPTION:    Replication of a Spark, ACK for Obsolete Ignored.
#+LABEL:      fig:task-sequences-ack-ignored
#+ATTR_LaTeX: :width 90mm
[[./img/chp3/msc/ack-with-replica.pdf]]

The handling of ~ACK~ messages relating to obsolete replicas is
illustrated in Figure \ref{fig:task-sequences-ack-ignored}. Node B
holds $spark_0$, which is successfully fished to node C. The
supervisor of $spark_0$ receives notification that B has failed
before an ~ACK~ has been received from C. The pessimistic
recovery strategy replicates the spark on node A as $spark_1$. When
the ~ACK~ from C is eventually received on A about $spark_0$, it is
simply ignored.

**** Replica Numbers in \texttt{REQ} Messages

#+CAPTION:    Obsolete Replicas are Discarded on Scheduling Attempts
#+LABEL:      fig:task-sequences-req-obsolete
#+ATTR_LaTeX: :width 130mm
[[./img/chp3/msc/req-with-replica.pdf]]

The handling of ~REQ~ messages relating to obsolete replicas is
illustrated in Figure \ref{fig:task-sequences-req-obsolete}. Due to
two node failures that have occurred prior to this message sequence,
there have been 3 replicas created from the original. Two still
remain, replica $2$ on node B, and replica $3$ on node A. Node C
successfully steals $spark_3$ from A. Node D targets B as a fishing
victim. Node B requests authorisation from A, using $r2$ to identify
$spark_2$ it holds. The supervisor determines that the younger
$spark_3$ is in existence, and so the scheduling request for $spark_2$
is denied with an ~OBSOLETE~ reply. The victim node C replies to the
thief node D with a ~NOWORK~ message, and discards $spark_2$. The
HdpH-RS implementation of task replica numbers is later described in
Section [[Implementing Futures]].


*** Fault Tolerant Scheduling Algorithm

This section presents the algorithms for supervised spark scheduling
and fault recovery. It describes message handling for the fault
tolerant fishing protocol (Section [[Work Stealing Protocol]]), how
obsolete sparks are identified and discarded (Section [[Duplicate
Sparks]]), and how task migration is tracked (Section [[Task
Locality]]). Each node is proactive in their search for work with
fishing (Algorithm \ref{alg:sendFish}), triggering a sequence of
message sequences between the victim, thieving and supervising nodes
as shown in Figure [[fig:ft-fishing-algorithm-interaction]].

#+CAPTION: Algorithm Interaction in Fault Tolerant Algorithm
#+LABEL:      fig:ft-fishing-algorithm-interaction
#+ATTR_LaTeX: :width 60mm
[[./img/chp3/msc-states/msc-states.pdf]]

#+BEGIN_LATEX
\begin{algorithm}
{\footnotesize
\caption{Algorithm for Proactive Fishing from a Thief}\label{alg:sendFish}
\begin{algorithmic}[1]
  \Function{fish}{}
   \Loop
    \If{$not\; fishing$} \Comment{is there an outstanding fish}
      \State $isFishing \gets True$ \Comment{blocks fishing until victim responds}
      \State $thief \gets myNode$ \Comment{this node is thief}
      \State $victim \gets randomNode$
      \State $msg \gets FISH\; thief$
      \State $\textbf{send}\; target\; msg$ \Comment{send to dead target will fail}
    \EndIf
   \EndLoop
  \EndFunction
\end{algorithmic}
}
\end{algorithm}
#+END_LATEX

Algorithm \ref{alg:handleFish} shows how a node that receives a ~FISH~
has been targeted by a thief. A condition on line
\ref{alg-code:waitingAuth} checks that this node has not already been
targeted by another thief and is waiting for authorisation. If it
is waiting for authorisation, then a ~NOWORK~ reply is sent to the
thief. If it is not waiting for an authorisation, then the local
sparkpool is checked for sparks. If a spark is present, then it is
moved in to the guard post and an authorisation request is sent to the
supervisor of that spark. Otherwise if the sparkpool is empty, a
~NOWORK~ is sent to the thief.

#+BEGIN_LATEX
\begin{algorithm}
{\footnotesize
\caption{Algorithm for Handling \texttt{FISH} Messages by a Victim}\label{alg:handleFish}
\begin{algorithmic}[1]
  \Require Thief (\emph{fisher}) is looking for work.
  \Function{handle}{$FISH\; thief$}
    \State $actioned \gets False$
    \If {$not\; waiting for auth$} \label{alg-code:waitingAuth} \Comment{is there an outstanding authorisation request}
      \If {$sparkpool\; not\; empty$}
        \State $spark \gets pop\; sparkpool$ \Comment{pop spark from local sparkpool}
        \State $guardPost \gets push\; spark$ \Comment{add spark to guard post}
        \State $msg \gets REQ\; spark.ref\; spark.replica\; myNode\; thief$
        \State $\textbf{send}\; spark.supervisor\; msg$ \Comment{authorisation reqest to supervisor}
        \State $actioned \gets True$ \Comment{local spark guarded}
      \EndIf
    \EndIf

    \If {$not\; actioned$}
        \State $msg \gets NOWORK$
        \State $\textbf{send}\; thief\; msg$ \Comment{inform thief of no work}
    \EndIf
  \EndFunction
\end{algorithmic}
}
\end{algorithm}
#+END_LATEX

If a victim has sparks that could be scheduled to a thief, it sends an
authorisation request to a supervisor, shown in Algorithm
\ref{alg:handleFish}. The handling of this request is shown in
Algorithm \ref{alg:handleReq}. A guarded spark is one held in the
guard post. If the location of the guarded spark is known to be in a
sparkpool of the victim (Section [[Task Locality]]), the request is
granted with ~AUTH~. Otherwise, if the task is believed to be in
transition between two nodes, the request is rejected with
~DENIED~. If the spark is obsolete, then the victim is instructed to
discard it with an ~OBSOLETE~ message.

#+BEGIN_LATEX
\begin{algorithm}
{\footnotesize
\caption{Algorithm for Handling \texttt{REQ} Messages by a Supervisor}\label{alg:handleReq}
\begin{algorithmic}[1]
  \Require A schedule request is sent from a victim to this (supervising) node.
  \Function{handle}{$REQ\; ref\; replica\; victim\; thief$}
    \State $replicaSame \gets compare\; (replicaOf\; ref)\; replica$
    \If {$replicaSame$} \Comment{remote task is most recent copy}
      \State $location \gets locationOf\; ref$
      \If {$location == OnNode$} \Comment{supervisor knows task is in a sparkpool}
        \State $\textbf{update}\; location\; (InTransition\; victim\; thief)$ 
        \State $msg \gets AUTH\; thief$ \Comment{authorise the request}
      \ElsIf {location == InTransition}
        \State $msg \gets DENIED\; thief$ \Comment{deny the request}
      \EndIf
    \Else
      \State $msg \gets OBSOLETE\; thief$ \Comment{remote task is old copy, ask that it is discarded}
    \EndIf
  \State $\textbf{send}\; victim\; msg$ 
  \EndFunction
\end{algorithmic}
}
\end{algorithm}
#+END_LATEX

When a victim that holds a spark is targeted, then it requests
scheduling authorisation in Algorithm \ref{alg:handleFish}. If the
request is granted in Algorithm \ref{alg:handleReq}, then a victim
will receive an ~AUTH~ message. The handling of an authorisation is
shown in Algorithm \ref{alg:handleAuth}. It takes the spark from the
guard post on line \ref{alg-code:popGuardPost}, and sends it to a
thief in a ~SCHEDULE~ message on line \ref{alg-code:sendSchedule}.

#+BEGIN_LATEX
\begin{algorithm}
{\footnotesize
\caption{Algorithm for Handling \texttt{AUTH} Messages by a Victim}\label{alg:handleAuth}
\begin{algorithmic}[1]
  \Require Location state on supervisor was \texttt{OnNode}.  
  \Function{handle}{$AUTH\; ref\; thief$}
    \State $spark \gets pop\; GuardPost$ \label{alg-code:popGuardPost}
    \State $msg \gets SCHEDULE\; spark$
    \State $\textbf{send}\; thief\; msg$ \label{alg-code:sendSchedule} \Comment{send thief the spark}  
  \EndFunction
  \Ensure Thief will receive spark in the \texttt{SCHEDULE} message.
\end{algorithmic}
}
\end{algorithm}
#+END_LATEX

However, if a victim is informed with ~OBSOLETE~ that the spark in its
guard post is an obsolete copy (Algorithm \ref{alg:handleObsolete}),
it empties the guard post on line \ref{alg-code:obsoleteSpark}, and
informs the thief that no work is available on line
\ref{alg-code:nowork}.

#+BEGIN_LATEX
\begin{algorithm}
{\footnotesize
\caption{Algorithm for Handling \texttt{OBSOLETE} Messages by a Victim}\label{alg:handleObsolete}
\begin{algorithmic}[1]
  \Require The guarded spark was obsolete.
  \Function{handle}{$OBSOLETE\; thief$}
    \State $obsoleteSpark \gets pop\; GuardPost$ \label{alg-code:obsoleteSpark} \Comment{discard spark in guard post}
    \State $\textbf{remove}\; obsoleteSpark$
    \State $msg \gets NOWORK$
    \State $\textbf{send}\; thief\; msg$ \label{alg-code:nowork} \Comment{inform thief of no work}
  \EndFunction
  \Ensure Guarded spark is discarded, thief will receive \texttt{NOWORK}.
\end{algorithmic}
}
\end{algorithm}
#+END_LATEX

When a victim is granted permission to send a spark to a thief,
then a thief will receive a ~SCHEDULE~ holding the spark. The handler
for scheduled sparks is shown in Algorithm
\ref{alg:handleSchedule}. It adds the spark to its own sparkpool on
line \ref{alg-code:insertSpark}, and sends an acknowledgement of its
arrival to its supervisor on line \ref{alg-code:sendAck}.

#+BEGIN_LATEX
\begin{algorithm}
{\footnotesize
\caption{Algorithm for Handling \texttt{SCHEDULE} Messages by a Thief}\label{alg:handleSchedule}
\begin{algorithmic}[1]
  \Require A Victim was authorised to send this node a spark in a \emph{SCHEDULE}.
  \Function{handle}{$SCHEDULE\; spark$}
    \State $\textbf{insert}\; spark\; sparkpool$ \Comment{add spark to sparkpool} \label{alg-code:insertSpark}
    \State $msg \gets ACK\; spark.ref\; spark.replica\; myNode$
    \State $\textbf{send}\; spark.supervisor\; msg$ \Comment{send \texttt{ACK} to spark's supervisor} \label{alg-code:sendAck}
  \EndFunction
  \Ensure Supervisor of spark will receive an \emph{ACK} about this spark.
\end{algorithmic}
}
\end{algorithm}
#+END_LATEX

A thief sends an acknowledgement of a scheduled spark to its
supervisor. The reaction to this ~ACK~ is shown in Algorithm
\ref{alg:handleAck}. It updates the migration tracking for this spark
to ~OnNode~, a state that will allow another thief to steal from the
new host of the spark.

#+BEGIN_LATEX
\begin{algorithm}
{\footnotesize
\caption{Algorithm for Handling \texttt{ACK} Messages by a Supervisor}\label{alg:handleAck}
\begin{algorithmic}[1]
  \Require Thief receives a spark.
  \Function{handle}{$ACK\; ref\; thief$}
   \State $\textbf{update}\; (locationOf\; ref)\; (OnNode\; thief)$ \Comment{set spark location to OnNode}
  \EndFunction
  \Ensure Location state updated to \texttt{OnNode}.
\end{algorithmic}
}
\end{algorithm}
#+END_LATEX

In the scenario where a ~REQ~ is received about a spark before an
~ACK~ (i.e. its migration state is ~InTransition~), then the request
is denied. This scenario is depicted in Figure
\ref{fig:denied-workstealing}. When a victim is denied a scheduling
request, it informs the thief that no work can be offered, on line
\ref{alg-code:sendNoWork} of Algorithm \ref{alg:handleDenied}.

#+BEGIN_LATEX
\begin{algorithm}
{\footnotesize
\caption{Algorithm for Handling \texttt{DENIED} Messages by a Victim}\label{alg:handleDenied}
\begin{algorithmic}[1]
  \Require location state on supervisor was \texttt{InTransition}.
  \Function{handle}{$DENIED\; thief$}
    \State $spark \gets popGuardPost$
    \State $\textbf{insert}\; spark\; sparkpool$ \Comment{put spark back into sparkpool}
    \State $msg \gets NOWORK$
    \State $\textbf{send}\; thief\; msg$ \label{alg-code:sendNoWork} \Comment{inform thief of no work}
  \EndFunction
  \Ensure fisher is given no work and can fish again.
\end{algorithmic}
}
\end{algorithm}
#+END_LATEX

A thief will fail to steal from its chosen victim in one of two
circumstances. First, because the victim has no sparks to offer
(Algorithm \ref{alg:handleFish}). Second, because the supervisor has
denied the request (Algorithm \ref{alg:handleReq}). A thief's reaction
when a ~NOWORK~ message is received is shown in Algorithm
\ref{alg:handleNowork}. It removes the block that prevented the
scheduler to perform on-demand fishing (Algorithm
\ref{alg:sendFish}). The thief can start fishing again.

#+BEGIN_LATEX
\begin{algorithm}
{\footnotesize
\caption{Algorithm for Handling \texttt{NOWORK} Messages by a Thief}\label{alg:handleNowork}
\begin{algorithmic}[1]
  \Require The victim was denied its authorisation request.
  \Function{handle}{NOWORK}
    \State $isFishing \gets False$
  \EndFunction
  \Ensure This thief can fish again.
\end{algorithmic}
}
\end{algorithm}
#+END_LATEX

Finally, Algorithm \ref{alg:handleDeadnode} shows the action of a node
when a ~DEADNODE~ message is received. It corresponds to the Haskell
implementation in Appendix [[Handling Dead Node Notifications]]. There are
four checks performed by every node when a remote node fails. First,
it checks if it is waiting for a fishing reply from the dead node
(line \ref{alg-code:waitingFishiReply}). Second, whether the dead node
is the thief of the spark it has requested authorisation for (line
\ref{alg-code:thiefDead}). Third, it identifies the supervised sparks
are at-risk due to the remote node failure (line
\ref{alg-code:vulnerableSparks}).  Fourth, it identifies the
supervised threads are at-risk due to the remote node failure (line
\ref{alg-code:vulnerableThreads}).

If a node is waiting for a fishing reply, it removes this block and no
longer waits (line \ref{alg-code:waitingFishiReply}). It is free to
fish again. If the node is a fishing victim of the failed node (line
\ref{alg-code:thiefDead}), then the spark in the guard post is popped
back in to the sparkpool. All at-risk (Section [[Task Locality]]) sparks
are replicated and added to the local sparkpool. These duplicates can
be fished again for load-balancing (line
\ref{alg-code:replicateSpark}). All at-risk threads are replicated and
are converted and executed locally (line
\ref{alg-code:replicateThread}). The Haskell implementation of spark
and thread replication is in Appendix [[Replicating Sparks and Threads]].

#+BEGIN_LATEX
\begin{algorithm}
{\footnotesize
\caption{Algorithm for Handling \texttt{DEADNODE} Messages by All Nodes}\label{alg:handleDeadnode}
\begin{algorithmic}[1]
  \Require A remote node has died.
  \Function{handle}{$DEADNODE\; deadNode$}
    \State $\textbf{remove}\; deadNode\; from\; distributed\; VM$
    \If {$waitingFishReplyFrom == deadNode$} \label{alg-code:waitingFishiReply}
      \State $isFishing \gets False$ \Comment{stop waiting for reply from dead node}
    \EndIf
    \If {$thiefOfGuardedSpark == deadNode$} \label{alg-code:thiefDead}
      \State $spark \gets pop\; guardPost$
      \State $\textbf{insert}\; spark\; sparkpool$ \Comment{put spark back in to sparkpool}    
    \EndIf
    \State $VulnerableSparks \gets (supervised\; sparks\; on\; deadNode)$ \Comment{at-risk sparks} \label{alg-code:vulnerableSparks}
    \State $VulnerableThreads \gets (supervised\; threads\; on\;deadNode)$ \Comment{at-risk threads} \label{alg-code:vulnerableThreads}
    \ForAll {$s \in VulnerableSparks$}
      \State $\textbf{insert}\; s\; sparkpool$ \Comment{Replicate potentially lost supervised spark} \label{alg-code:replicateSpark}
    \EndFor
    \ForAll {$t \in VulnerableThreads$}
      \State $\textbf{insert}\; t\; threadpool$ \Comment{Replicate potentially lost thread: convert \& execute locally} \label{alg-code:replicateThread}
    \EndFor
  \EndFunction
  \Ensure All at-risk supervised sparks and threads are recovered.
\end{algorithmic}
}
\end{algorithm}
#+END_LATEX

*** Fault Recovery Examples

This section introduces the fault tolerance mechanisms of the HdpH-RS
scheduler. The task replication and failure detection techniques are
similar to those used in the supervised workpools, demonstrated in
Appendix [[Use Case Scenarios]]. When the ~supervisedSpawn~ and
~supervisedSpawnAt~ primitives are used (Section [[HdpH-RS Programming
Primitives]]), the HdpH-RS scheduler guarantees task execution, provided
that the caller and the root node (if they are distinct) do not
die. This section presents a series of diagrammatic explanations of
scheduling behaviour in the presence of faults. Section [[Operational
Semantics]] describes the operational semantics that formalise these
behaviours, and Section [[Message Handling]] models the message passing in
the scheduler that implements the scheduling.

A simple HdpH-RS system architecture is shown in Figure
\ref{fig:sched-base-case}. This depicts a supervisor node and three
worker nodes. Each node has a sparkpool and a threadpool. Every node
has tasks residing in both the sparkpool and the threadpool. Tasks in
sparkpools can migrate to other sparkpools using load balancing. Tasks
are moved into threadpools through one of two actions --- either they
have been remotely scheduled with ~supervisedSpawnAt~ or a spark has
been converted to a thread by the scheduler (see the transition
~convert_spark~ in Section [[Operational Semantics]]).

#+CAPTION: Simple HdpH-RS System Architecture
#+LABEL:      fig:sched-base-case
#+ATTR_LaTeX: :width 70mm
[[./img/chp3/scheduling-behaviours/base-case.pdf]]

**** Recovering Sparks

This section describes the scenario of the ~supervisedSpawn~ primitive
being used to schedule 6 sparks, with a network of 4 nodes. After
spawning there will be 6 sparks in the /sparkpool/ of the supervisor
node, shown in Figure \ref{fig:sched-spawn1}. For the purposes of
illustration, the 3 workers fish continually in order to hold more
than one spark.

Work stealing balances the sparks across the architecture, shown in
Figure \ref{fig:sched-spawn2}. Worker 1 fails, which is detected by
the supervisor. The recovery action of the supervisor is to replicate
sparks $3$ and $6$, and add them to the local sparkpool, shown in
Figure \ref{fig:sched-spawn3}. Finally, these recovered sparks may be
once again fished away by the two remaining worker nodes, shown in
Figure \ref{fig:sched-spawn4}.

#+CAPTION: Six tasks are lazily scheduled by the supervisor node
#+LABEL:      fig:sched-spawn1
#+ATTR_LaTeX: :width 70mm
[[./img/chp3/scheduling-behaviours/sched-spawn1.pdf]]

#+CAPTION: Six tasks are fished equally by 3 worker nodes
#+LABEL:      fig:sched-spawn2
#+ATTR_LaTeX: :width 70mm
[[./img/chp3/scheduling-behaviours/sched-spawn2.pdf]]

#+CAPTION: A worker node fails, and copies of lost tasks are rescheduled by the supervisor
#+LABEL:      fig:sched-spawn3
#+ATTR_LaTeX: :width 70mm
[[./img/chp3/scheduling-behaviours/sched-spawn3.pdf]]

#+CAPTION: The rescheduled tasks are once again fished away
#+LABEL:      fig:sched-spawn4
#+ATTR_LaTeX: :width 70mm
[[./img/chp3/scheduling-behaviours/sched-spawn4.pdf]]

**** Recovering Threads

The scenario in Figure \ref{fig:sched-spawnAt1} shows a supervisor
node eagerly distributing 6 tasks across 3 worker nodes with round
robin scheduling with ~supervisedSpawnAt~. The tasks are never placed
in the supervisor's sparkpool or threadpool. When worker 1 dies, the
supervisor immediately replicates a copy of threads $1$ and $4$ into
its own threadpool, to be evaluated locally, depicted in Figure
\ref{fig:sched-spawnAt2}.


#+CAPTION: Six tasks are eagerly scheduled as threads to 3 worker nodes
#+LABEL:      fig:sched-spawnAt1
#+ATTR_LaTeX: :width 70mm
[[./img/chp3/scheduling-behaviours/sched-spawnAt1.pdf]]

#+CAPTION: Copies of the two lost tasks are converted to threads on the supervisor node
#+LABEL:      fig:sched-spawnAt2
#+ATTR_LaTeX: :width 70mm
[[./img/chp3/scheduling-behaviours/sched-spawnAt2.pdf]]
**** Simultaneous Failure

There are failure scenarios whereby the connection between the root
node and multiple other nodes may be lost. Communication links may
fail by crashing, or by failing to deliver messages. Combinations of
such failures may lead to partitioning failures
\cite{DBLP:journals/csur/DavidsonG85}, where nodes in a partition may
continue to communicate with each other, but no communication can
occur between sites in different partitions.

There are two distinguished connected graphs in the distributed
HdpH-RS virtual machine. First is the networking hardware between
hosts. Sockets over TCP/IP is used on Ethernet networking
infrastructures in HdpH-RS (Section [[Fault Detecting Communications
Layer]]) to send and receive messages. The maximum size of this
connected graph is fixed, hosts cannot be added during runtime.

The second graph connects tasks to futures. Recall from Section
[[HdpH-RS Programming Primitives]] that futures are created with the spawn
family of primitives. A task may be decomposed in to smaller tasks,
creating futures recursively.

A simple graph expansion of futures is shown in Figure
\ref{fig:simple-future-graph-expansion}. All nodes in a dotted area
are connected in a network. Nodes A and B are connected. Node A takes
the role of the root node and starts the computation, which splits the
program into 30 tasks. During runtime, 5 unevaluated tasks are stolen
by node B over the network connection. These 5 tasks are expanded in
to 10, so node B holds 10 tasks. The program in total has therefore
expanded to a graph of 35 futures. The key point is that in both the
absence and presence of faults, this program will always expand to 35
futures.

#+CAPTION:    Graph Expansion of Tasks
#+LABEL:      fig:simple-future-graph-expansion
#+ATTR_LaTeX: :width 30mm
[[./img/chp3/simultaneous-failure/task-expansion-example2.pdf]]

A slightly more complicated decomposition is in Figure
\ref{fig:future-graph-expansion-no-failure}. This program is expanded
to a graph of 54 futures in total, over 5 nodes including the root
node.

#+CAPTION:    Graph Expansion of Tasks over 5 Nodes
#+LABEL:      fig:future-graph-expansion-no-failure
#+ATTR_LaTeX: :width 50mm
[[./img/chp3/simultaneous-failure/no-faults2.pdf]]

The failure scenario is shown in Figure
\ref{fig:graph-expansion-network-partition}. The network is split
in to two partitions, one comprised of nodes A, B and C, and the other
with nodes D and E. A /lost/ partition is one that no longer includes
the root node A. The static network connection from the root node A to
both D and E are lost, and so D and E are zombie nodes. Despite the
fact that the connection with the root node was lost by D and E
simultaneously, notification of connection loss from these nodes will
arrive sequentially at A, B and C. Nodes D and E will receive
notification that connection with root node A has been lost, and will
terminate as they know they can no longer play a useful role in the
execution of the current job.

#+CAPTION:    Network Partition, Leaving Nodes D & E Isolated
#+LABEL:      fig:graph-expansion-network-partition
#+ATTR_LaTeX: :width 50mm
[[./img/chp3/simultaneous-failure/two-faults3.pdf]]

The recovery of tasks is shown in Figure
\ref{fig:graph-expansion-network-partition-recovery}. Nodes A, B and C
check whether these lost connections affect the context of the futures
they host locally. Node A concludes that it is completely unaffected
by the connection loss. Node B concludes that it is affected by the
loss of node D, and must recover 3 tasks. Node C concludes that it is
affected by the loss of node E, and must recover 8 tasks. These tasks
will once again expand from 3 to 6, and 8 to 14, respectively.

#+CAPTION:    Recovering Tasks from Nodes D & E
#+LABEL:      fig:graph-expansion-network-partition-recovery
#+ATTR_LaTeX: :width 50mm
[[./img/chp3/simultaneous-failure/two-faults3-recovered.pdf]]

The scenario in this section demonstrates that in the presence of
network failure, incurring the loss of connection from the root node
to two nodes, the program executing on the partition containing the
root node expands to 54 futures --- the same as failure-free
evaluation.

** Summary

This chapter has presented the language and scheduling design of a
HdpH-RS. The next chapter presents a Promela model of the scheduling
algorithm from Section [[Fault Tolerant Scheduling Algorithm]]. A key
property is verified using the SPIN model checker. This property
states that a supervised future (Section [[HdpH-RS Terminology]]) is
eventually filled despite all possible combinations of node
failure. Chapter [[Implementing a Fault Tolerant Programming Language
and Reliable Scheduler]] then presents a Haskell implementation of the
HdpH-RS programming primitives and the verified reliable scheduling
design.

* The Validation of Reliable Distributed Scheduling for HdpH-RS

Chapter [[Designing a Fault Tolerant Programming Language for
Distributed Memory Scheduling]] presents the design of HdpH-RS --- the
fault tolerant programming primitives ~supervisedSpawn~ and
~supervisedSpawnAt~ (Section [[HdpH-RS Programming Primitives]]) supported
by a reliable scheduler (Section [[Designing a Fault Tolerant
Scheduler]]). This chapter validates the critical reliable properties of
the scheduler. The SPIN model checker is used to ensure that the
HdpH-RS scheduling algorithms (Section [[Fault Tolerant Scheduling
Algorithm]]) honour the small-step semantics on states (Section
[[Operational Semantics]]), supervising sparks in the absence and presence
of faults.

Model checking has been shown to be an effective tool in validating
the behaviour of fault tolerant systems, such as embedded spacecraft
controllers \cite{DBLP:conf/icre/SchneiderECH98}, reliable
broadcasting algorithms \cite{DBLP:conf/spin/JohnKSVW13}, and fault
tolerant real-time startup protocols for safety critical applications
\cite{DBLP:conf/formats/DutertreS04}. Model checking has previously
been used to eliminate non-progress cycles of process scheduling in
asynchronous distributed systems \cite{DBLP:journals/tse/Holzmann97}.

Fault tolerant distributed algorithms are central to building reliable
distributed systems. Due to the various sources of non-determinism in
faulty systems, it is easy to make mistakes in the correctness
arguments for fault tolerant distributed systems. They are therefore
natural candidates for model checking
\cite{DBLP:conf/spin/JohnKSVW13}. The HdpH-RS scheduler must hold
reliability properties when scaled to complex non-deterministic
distributed environments:

1) *Asynchronous message passing* Causal ordering
   \cite{DBLP:journals/cacm/Lamport78} of asynchronous distributed
   scheduling events is not consistent with wall-clock times. Message
   passing between nodes is asynchronous and non-blocking, instead
   writing to channel buffers. Because of communication delays, the
   information maintained in a node concerning its neighbours'
   workload could be outdated \cite{Xu:1997:LBP:548748}.

2) *Work stealing* Idle nodes attempt to steal work from
   overloaded nodes. To recover tasks in the presence of failure, a
   supervisor must be able to detect node failure and must always know
   the location of its supervised tasks. The asynchronous message
   passing from (1) complicates location tracking. The protocol
   for reliably relocating supervised tasks between nodes in the
   presence of failure is intricate, and model checking the protocol
   increases confidence in the design.

3) *Node failure* Failure is commonly detected with timeouts or
   ping-pong protocols (Section [[Fault Detectors]]). The Promela
   abstraction models node failure, and latency's of node failure
   detection.

The model of the HdpH-RS scheduler shows that by judiciously
abstracting away extraneous complexity of the HdpH-RS implementation,
the state space can be exhaustively searched for validating a key
reliability requirement. The key HdpH-RS reliable scheduling property
is validated with SPIN \cite{DBLP:journals/tse/Holzmann97}
\cite{DBLP:books/daglib/0020982} in Section [[Verifying Scheduling
Properties]], by defining a corresponding safety property in linear
temporal logic. Bugs were fixed in earlier versions of the Promela
model, when violating system states were identified. An example of bug
finding using this iterative implementation of HdpH-RS using model
checking is described in Section [[Identifying Scheduling Bugs]].

The motivation for modeling work stealing in asynchronous environments
is given in Section [[Modeling Asynchronous Environments]]. The scope of
the Promela abstraction of the HdpH-RS scheduler is in Section [[Promela
Model of Fault Tolerant Scheduling]]. The model of the work stealing
scheduler is in Section [[Scheduling Model]]. The use of linear temporal
logic for expression a key reliability property is shown in Section
[[Verifying Scheduling Properties]]. The SPIN model checker to
exhaustively search the model's state space to validate that the
reliability property holds on all reachable states. The SPIN model
checking results are in Section [[Model Checking Results]].


** Modeling Asynchronous Environments

*** Asynchronous Message Passing
    
Most message passing technologies in distributed systems can be
categorised in to three classes \cite{DBLP:books/daglib/0067166}:
unreliable datagrams, remote procedure calls and reliable data
streams. /Unreliable datagrams/ discard corrupt messages, but do
little additional processing. Messages may be lost, duplicated or
delivered out of order. An example is UDP \cite{rfc768}. In /remote
procedure calls/, communication is presented as a procedure invocation
that returns a result. When failure does occur however, the sender is
unable to distinguish whether the destination failed before or after
receiving the request, or whether the network has delayed the
reply. /Reliable data streams/ communicate over channels that provide
flow control and reliable, sequenced message delivery. An example is
TCP \cite{rfc793}.

A TCP connection in the absence of faults provides FIFO ordering
between two nodes. The relationship between messaging events can be
described as /causal ordering/. Causal ordering is always consistent
with the actual wall-clock times that events occur. This can be
written as $send(m_1)\rightarrow send(m_2)$
\cite{DBLP:journals/cacm/Lamport78}, where $\rightarrow$ means
happened-before. The ordering property of TCP guarantees that
$recv(m_1)$ occurs before $recv(m_2)$ on the other end of the
connection. This ordering guarantee is not sufficient in distributed
systems that have /multiple/ TCP connections. The latency of TCP data
transfer is well known \cite{DBLP:conf/infocom/CardwellSA00}, for
example due to memory copying with kernel buffers, heterogeneous
network latency, and TCP data transfer latency
\cite{DBLP:conf/infocom/CardwellSA00}.

HdpH-RS uses a network abstraction layer \cite{network-transport} that
assigns an /endpoint/ to each node. An endpoint is a mailbox that
consumes messages from multiple connections established with other
nodes. Causal ordering of messaging events is no longer consistent
with wall-clock time. This potentially leads to /overtaking/ on two
separate TCP connections.

Take a simple architecture with three nodes A, B and C. If A is
connected with B on dual connection $c_1$ and to C on dual connection
$c_2$, then the endpoint $E_A$ on node A is defined as $E_A = \left\{
{c_1,c_2}\right\}$. Nodes B and C send messages to $E_A$ in the causal
order $c_1.send(m_1) \rightarrow c_2.send(m_2)$. Ordering of events
$c_1.recv(m_1)$ and $c_2.recv(m_2)$ at $E_A$ is unknown. The HdpH-RS
work stealing protocol enforces a certain order of events, and message
responses are determined by task location state (Section [[Fault
Tolerant Scheduling Algorithm]]).

*** Asynchronous Work Stealing

Work stealing for balancing load is introduced in Section [[Introducing
Work Stealing Scheduling]], describing how nodes inherit thief, victim
and supervisory roles dynamically. Location tracker messages sent by a
thief and a victim with respect to a spark migrating between the two
may be received in any order by the spark's supervisor due to
asynchronous message passing (Section [[Asynchronous Message Passing]]).

The HdpH-RS fishing protocol gives the control of spark migration to
the supervisor. The fishing protocol (Section [[Work Stealing Protocol]])
is abstracted in Promela in Section [[Promela Model of Fault Tolerant
Scheduling]]. The protocol handles any message sequence from thieves and
victims to ensure that task location tracking is never
invalidated. The Promela model honours the small-step transition rules
~migrate_supervised_spark~ and ~recover_supervised_spark~ (Section
[[Small Step Operational Semantics]]) for migrating and replicating
supervised sparks shown in Section [[Node State]]. The ~kill_node~ transition rule in the small-step semantics can
be triggered at any time as described in Section [[Node Failure]]. This is
modelled in Promela with a non-deterministic unconditional choice that
workers can take (Section [[Channels & Nodes]]).

** Promela Model of Fault Tolerant Scheduling

*** Introduction to Promela

Promela is a meta-language for building verification models, and the
language features are intended to facilitate the construction of
high-level abstractions of distributed systems. It is not a systems
implementation language. The emphasis in Promela abstraction is
synchronisation and coordination with messages, rather than
computation. It supports for example, the specification of
non-deterministic control structures and it includes primitives for
process creation, and a fairly rich set of primitives for interprocess
communication. Promela is /not/ a programming language
\cite{DBLP:books/daglib/0020982}, and so does not support functions
that returns values and function pointers. This chapter
verifies the fault tolerant scheduler from Section [[Designing a Fault
Tolerant Scheduler]]. Chapter [[Implementing a Fault Tolerant Programming
Language and Reliable Scheduler]] presents the Haskell /implementation/
of the verified scheduler and HdpH-RS programming primitives. The SPIN
analyser is used to verify fractions of process behaviour, that are
considered suspect \cite{promela}.

Temporal logic model checking is a method for automatically deciding
if a finite state program satisfies its specification
\cite{DBLP:journals/toplas/ClarkeGL94}. Since the size of the state
space grows exponentially with the number of processes, model checking
techniques based on explicit state enumeration can only handle
relatively small examples \cite{DBLP:conf/dagstuhl/ClarkeGJLV01}. When
the number of states is large, it may be very difficult to determine
is such a program is correct. Hence, the infinite state space of the
real HdpH-RS scheduler is abstracted to be a small finite model that
SPIN can verify.

The HdpH-RS scheduler has been simplified to its core supervision
behaviours that ensure supervised task survival. The model includes 1
supervisor, 3 workers and 1 supervised spark. The abstract model is
sufficiently detailed to identify real bugs in the implementation
e.g. the identified bug described in Section [[Identifying Scheduling
Bugs]].

#+BEGIN_QUOTE
SPIN is not a simulation or an application environment, it is a formal
verification environment.  Is this really a model that will comply
with the correctness requirements only if the number of channels is
1,000?  If so, I would consider that a design error in itself. /Gerard
J. Holzmann/ \cite{holzmann-quote}
#+END_QUOTE

**** Constructing Promela Models

Promela programs consist of processes, message channels, and
variables. Processes are global objects that represent the concurrent
nodes in HdpH-RS. Message channels for the supervisor and workers are
declared globally. The variables used in the propositional symbols in
LTL formulae (Section [[Verifying Scheduling Properties]]) are declared
globally. Other variables are declared locally within the
process. Whist processes specify behaviour, channels and global
variables define the environment in which the processes run. A simple
Promela model is shown in Listing \ref{lst:promela-construction}.

#+BEGIN_LATEX
\begin{Code}

\begin{lstlisting}[name=annotatedpromela,numbers=left,basicstyle=\scriptsize\ttfamily,captionpos=b,caption=Example Promela Model Construction,label=lst:promela-construction]
typedef Sparkpool {
  int spark_count=0; /* #spark replicas */
  int spark=0;       /* highest sequence number */
}

chan chans[4] = [10] of {mtype,int,int};



inline report_death(me){ /* .. */ }


active proctype Supervisor() { 
  int thief_pid, victim_pid, seq_n;



  if
  :: chans[0].inChan ? ACK(thief_pid,seq_n) ->
       /* .. */
  :: chans[0].inChan ? REQ(victim_pid,thief_pid) ->
       /* .. */
  fi;

  atomic { /* .. */ }
}
\end{lstlisting}
\end{Code}
#+END_LATEX

The Promela abstraction of the HdpH-RS scheduler is detailed in
Section [[HdpH-RS Abstraction]].  The full Promela implementation is in
Appendix [[Promela Model Implementation]]. The Promela terminology defines
/processes/ as isolated units of control, whose state can only be
changed by other processes through message passing. This chapter
deviates by using /nodes/ as a synonym for processes, in order to be
consistent with the HdpH-RS terminology.


*** Key Reliable Scheduling Properties

The SPIN model checker is used to verify key reliable scheduling
properties of the algorithm designs from Section [[Fault Tolerant
Scheduling Algorithm]]. SPIN accepts design specification written in
the verification language Promela, and it accepts correctness claims
specified in the syntax of standard Linear Temporal Logic (LTL)
\cite{DBLP:conf/focs/Pnueli77}. Section [[Model Checking Results]] shows
the LTL formula used by SPIN to verify the Promela abstraction of
HdpH-RS scheduling.

The correctness claims guarantee supervised spark evaluation. This is
indicated by filling its corresponding supervised future (the ~IVar~)
on the supervising node. The first is a counter property and is used
to ensure the Promela abstraction does model potential failure of any
or all of the mortal worker nodes:

1. *Any or all worker nodes may fail* To check that the model
   potentially kills one or more mortal workers, SPIN is used to find
   counter-example executions when one of the workers terminates,
   which it is trivially able to do after searching 5 unique states
   (Section [[Counter Property]]).

Two further properties are used to exhaustively verify the absence of
non-desirable states of the ~IVar~ on the supervisor node:

2. *The IVar is empty until a result is sent* Evaluating a task
   involves transmitting a value to the supervisor, the host of the
   ~IVar~. This property verifies that the ~IVar~ cannot be full until
   one of the nodes has transmitted a value to the supervisor. The
   absence of a counter system state is verified after exhaustively
   searching 3.66 million states (Section [[Desirable Properties]]).

3. *The IVar is eventually always full* The ~IVar~ will eventually be
   filled by either a remaining worker, or the supervisor. This is
   despite the failure of any or all worker nodes. Once it is full, it
   will always be full because there are no operations to remove
   values from \texttt{IVar}s in HdpH-RS. The absence of a counter
   system state is verified after exhaustively searching 8.22 million
   states (Section [[Desirable Properties]]).

*** HdpH-RS Abstraction

The model considers tasks scheduled with ~supervisedSpawn~ in HdpH-RS
--- modeling the tracking and recovery of supervised sparks. The
location of threads scheduled with ~supervisedSpawnAt~ is always known
i.e. the scheduling target. This would not elicit race conditions on
location tracking messages ~REQ~ and ~ACK~, and are therefore not in
the scope of the model. The Promela model is a partial abstraction
that encapsulates behaviours necessary for guaranteeing the evaluation
of supervised sparks. There are six characteristics of the HdpH-RS
scheduler in the Promela model:

1. *One immortal supervisor* that initially puts a spark into its
   local sparkpool. It also creates spark replicas when necessary
   (item \ref{promela-scope-replication}).

2. *Three mortal workers* that attempt to steal work from the supervisor
   and each other. Failure of these nodes is modeled by terminating
   the Promela process for each node.

3. *Computation* of a spark may happen at any time by any node
   that holds a copy of the spark. This simulates the execution of the
   spark, which would invoke an ~rput~ call to fill the ~IVar~ on the
   supervisor. It is modeled by sending a ~RESULT~ message to the
   supervisor. This message type is not in HdpH-RS, rather it mimics
   ~PUSH~ used by ~rput~ to transmit a value to an ~IVar~ in HdpH-RS.

4. \label{promela-scope-failure} *Failure* of a worker node means that
   future messages to it are lost. The ~kill_node~ transition rule is
   modeled with a non-deterministic suicidal choice any of the three
   worker nodes can make. This choice results in a node asynchronously
   broadcasting its death to the remaining healthy nodes and then
   terminating. Failure detection is modeled by healthy nodes
   receiving ~DEADNODE~ messages.

5. *Asynchronicity* of both message passing and failure detection is
   modeled in Promela using buffered channels. Buffered channels model
   the buffered FIFO TCP connections in HdpH-RS.

6. \label{promela-scope-replication} *Replication* is used by the
   supervisor to ensure the safety of a potentially lost spark in the
   presence of node failure. The model includes spark replication from
   algorithm \ref{alg:handleDeadnode} in Section [[Fault Tolerant
   Scheduling Algorithm]], honouring the ~recover_supervised_spark~
   small-step transition rule in Section [[Small Step Operational
   Semantics]]. Replication numbers are used to tag spark replicas in
   order to identify obsolete spark copies. Obsolete replica migration
   could potentially invalidate location records for a supervised
   spark, described in Section [[Duplicate Sparks]]. Therefore, victims
   are asked to discard obsolete sparks, described in Algorithm
   \ref{alg:handleObsolete} of Section [[Fault Tolerant Scheduling
   Algorithm]].

Two scheduling properties in Section [[Linear Temporal Logic &
Propositional Symbols]] state that a property must eventually be
true. The formula $\Box\; (ivar\_empty\;
\mathbin{\mathcal{U}\kern-.1em} \;any\_result\_sent)$ in Section
[[Linear Temporal Logic & Propositional Symbols]] is a /strong
until/ connective and verifies two properties of the model. First,
that the $ivar\_empty$ property must hold until at least
$any\_result\_sent$ is true. Second, that $any\_result\_sent$ is true
in some future state (the /weak until/ connective does not demand this
second property). The formula $\Diamond\; \Box\; ivar\_full$ demands
that the ~IVar~ on the supervisor is eventually always full.

Without any deterministic choice of sending a ~RESULT~ message to the
supervisor, the three thieving worker nodes could cycle through a
sequence of work stealing messages, forever passing around the spark
and each time visiting an identical system state. The SPIN model
checker identified this cyclic trace in an earlier version of the
model. These cycles contradict the temporal requirements of the
/strong until/ and /eventually/ connectives in the two properties
above.

Determinism is introduced to the model by ageing the spark through
transitions of the model. The age of the spark is zero at the initial
system state. Each time it is scheduled to a node, its age is
incremented. Moreover, each time it must be replicated by the
supervisor its age is again incremented. When the age of the spark
reaches 100, all nodes are forced to make a deterministic choice to
send a ~RESULT~ message to the supervisor if they hold a replica or
else the next time they do. This models the HdpH-RS assumption that a
scheduler will eventually execute the spark in a sparkpool.

*** Out-of-Scope Characteristics

Some aspects of the HdpH-RS scheduler design and implementation are
not abstracted in to the Promela model, because they are not part of
the fault tolerance actions to guarantee that an ~IVar~ will be
written to.

1. *Multiple IVars* The model involves only /one/ ~IVar~ and one
   supervised spark, which may manifest into multiple replicas --- one
   active and the rest obsolete. Multiple \texttt{IVar}s are not
   modeled.

2. *Non-supervised sparks* Only /supervised/ sparks created with
   ~supervisedSpawn~ are modeled, while unsupervised sparks are
   not. Non-supervised sparks are created by calls to ~spawn~ (Section
   [[HdpH-RS Programming Primitives]]), and create sparks that are
   intentionally not resilient to faults, and by-pass the fault
   tolerant fishing protocol.

3. *Threads* Threads are created
   with ~spawnAt~ and ~supervisedSpawnAt~. Section [[HdpH-RS Programming
   Primitives]] describes why eagerly placed threads are more straight
   forward to supervise. Once they are transmitted to a target node,
   they do not migrate.

** Scheduling Model
*** Channels & Nodes

Nodes interact with message passing. Using Promela syntax, nodes
receive messages with ~?~, and send messages with ~!~. A simple
example is shown in Figure \ref{fig:promela-msc}. Node A sends a
message ~FOO~ with ~!~ to a channel that node B receives messages
on. Node B reads the message from the channel with ~?~. The channels
in the Promela model of HdpH-RS are asynchronous, so that messages can
be sent to a channel buffer, rather than being blocked waiting on a
synchronised participatory receiver. This reflects the data buffers in
TCP sockets, the transport protocol in HdpH-RS.

#+CAPTION:    Message Passing in Promela
#+LABEL:      fig:promela-msc
#+ATTR_LaTeX: :width 50mm
[[./img/chp4/msc/msc-promela.pdf]]

**** Channels

There are four channels in the model, one for each of the four
nodes. They are globally defined as shown on line \ref{promela:chans}
of Listing \ref{lst:promela-channels}. The channels can store up to
ten messages, which is more than strictly required in this model of
four nodes where the protocol enforces nodes to wait for responses for
each message sent. Each message consists of four fields: a symbolic
name and three integers. The symbolic name is one of the protocol
messages e.g. ~FISH~, and the three integers are reserved for process
IDs and replica numbers. A ~null~ macro is defined as $-1$, and is
used in messages when not all fields are needed. For example, ~REQ~
message uses three fields to identify the victim, the thief, and the
replica count of the targeted spark. The ~ACK~ message however only
used to identify the thief and the replica number, so ~null~ is used
in place of the last field. The supervisor node is instantiated in the
initial system state (line \ref{promela:sup-active-proctype}). It
starts the three workers (lines \ref{promela:run-worker0} to
\ref{promela:run-worker2}), passing values 0, 1 and 2 telling each
worker which channel to consume. The supervisor node consumes messages
from channel ~chans[3]~.

#+BEGIN_LATEX
\begin{Code}
\begin{promelacode}{Identifying Node Channels}{lst:promela-channels}
chan chans[4] = [10] of {mtype, int , int , int } ; @\label{promela:chans}@

active proctype Supervisor() { @\label{promela:sup-active-proctype}@
  run Worker(0); @\label{promela:run-worker0}@
  run Worker(1);
  run Worker(2); @\label{promela:run-worker2}@
  /* omitted */
}

proctype Worker(int me) { /* consume from chans[me] channel */ } @\label{promela:worker-proctype}@
\end{promelacode}
\end{Code}
#+END_LATEX


**** Supervisor Node

The supervisor is modeled as an /active/ ~proctype~, so is
instantiated in the initial system state. The supervisor executes
repetitive control flow that receives work stealing messages from
worker nodes and authorisation messages from the supervisor, shown in
Listing \ref{lst:promela-supervisor-control-flow}. The spark is
created on line \ref{ref:promela-create-spark}, and the workers are
started on line \ref{ref:promela-start-workers}. The underlying
automaton is a message handling loop from ~SUPERVISOR_RECEIVE~ (line
\ref{promela:supervisor-recv}). The exception is when the spark has
aged beyond 100 (line \ref{promela:supervisor-max-age}), in which case
a ~RESULT~ message is sent to itself. The label ~SUPERVISOR_RECEIVE~
is re-visited after the non-deterministic message handling choice
(line \ref{promela:supervisor-loop-recv}), and is only escaped on line
\ref{promela:supervisor-complete} if a ~RESULT~ message has been
received. In this case the ~IVar~ becomes full and the supervisor
terminates.

#+BEGIN_LATEX
\begin{Code}
\begin{promelacode}{Repetitive Control Flow Options for Supervisor}{lst:promela-supervisor-control-flow}
active proctype Supervisor() {
  int thiefID, victimID, deadNodeID, seq, authorizedSeq, deniedSeq;
  
  atomic { 
    supervisor.sparkpool.spark_count = 1; @\label{ref:promela-create-spark}@
    spark.context = ONNODE;
    spark.location.at = 3;
  }
  run Worker(1); run Worker(2); run Worker(3); @\label{ref:promela-start-workers}@

SUPERVISOR_RECEIVE: @\label{promela:supervisor-recv}@
         /* deterministic choice to send RESULT to itself once spark age exceeds 100 */
  if  :: (supervisor.sparkpool.spark_count > 0 && spark.age > maxLife) -> @\label{promela:supervisor-max-age}@
         chans[3] ! RESULT(null,null,null);
      :: else ->
         if   /* non-deterministic choice to send RESULT to itself */
           :: (supervisor.sparkpool.spark_count > 0) ->
              chans[3] ! RESULT(null,null,null);

              /* otherwise receive work stealing messages */
           :: chans[3] ? FISH(thiefID, null,null) -> /*  fish request, Listing @\ref{lst:fish-msg-reaction}@ */ 
           :: chans[3] ? REQ(victimID, thiefID, seq) -> /* schedule request, Listing @\ref{lst:schedreq-msg-reaction}@ */
           :: chans[3] ? AUTH(thiefID, authorizedSeq, null) -> /* request response, Listing @\ref{lst:schedauth-msg-reaction}@*/
           :: chans[3] ? ACK(thiefID, seq, null) -> /* spark arrival ack, Listing @\ref{lst:ack-msg-reaction}@ */
           :: chans[3] ? DENIED(thiefID, deniedSeq,null) -> /* request response, Listing @\ref{lst:scheddenied-msg-reaction}@ */
           :: chans[3] ? DEADNODE(deadNodeID, null, null) -> /* notification, Listing @\ref{lst:deadnode-msg-reaction}@ */
           :: chans[3] ? RESULT(null, null, null) ->
              supervisor.ivar = 1;
              goto EVALUATION_COMPLETE; @\label{promela:supervisor-complete}@
           fi;
  fi;
goto SUPERVISOR_RECEIVE; @\label{promela:supervisor-loop-recv}@

EVALUATION_COMPLETE:
}
\end{promelacode}
\end{Code}
#+END_LATEX

**** Worker Nodes

Each worker executes repetitive control flow that receives work
stealing message from worker nodes and authorisation messages from the
supervisor, shown in Listing
\ref{lst:promela-worker-control-flow}. The underlying automaton is a
message handling loop from ~WORKER_RECEIVE~ (line
\ref{promela:worker-recv}). The exception is when the spark has aged
beyond 100 (line \ref{promela:worker-max-age}), in which case a
~RESULT~ message is sent to the supervisor. Otherwise the control flow
takes one of three non-deterministic choices. First, the node may
die. Second, it may send a ~RESULT~ message to the supervisor if it
holds a replica. Third,
it may receive a work stealing message from a work or scheduling
request response from the supervisor. The ~WORKER_RECEIVE~ label is
re-visited after the non-deterministic message handling choice (line
\ref{promela:worker-loop-recv}), and is only escaped if it has died
(line \ref{promela:worker-died-end}) or the ~IVar~ on the supervisor
is full. In either case the worker terminates.

#+BEGIN_LATEX
\begin{Code}
\begin{promelacode}{Repetitive Control Flow Options for a Worker}{lst:promela-worker-control-flow}
proctype Worker(int me) {
  int thiefID, victimID, deadNodeID, seq, authorisedSeq, deniedSeq;
  
WORKER_RECEIVE: @\label{promela:worker-recv}@
  if   /* deterministic choice to send RESULT to supervisor once spark age exceeds 100 */
    :: (worker[me].sparkpool.spark_count > 0 && spark.age > maxLife) -> @\label{promela:worker-max-age}@
       atomic {
         worker[me].resultSent = true;
         chans[3]  ! RESULT(null,null,null);
         goto END; @\label{promela:worker-died-end}@
       }
            
    :: else ->
       if
         :: skip ->  /* die */ @\label{promela:worker-die}@
            worker[me].dead = true;
            report_death(me); /* Listing @\ref{lst:promela-failure-report}@ */
            goto END; @\label{promela:worker-goto-end}@
   
            /* non-deterministic choice to send RESULT to supervisor */
         :: (worker[me].sparkpool.spark_count > 0) ->
              chans[3] ! RESULT(null,null,null);

            /* conditions for pro-active fishing */            
         :: (worker[me].sparkpool.spark_count == 0
             && (worker[me].waitingFishReplyFrom == -1)
             && spark.age < (maxLife+1)) ->  /* go fishing */
           
            /* otherwise receive work stealing messages */
         :: chans[me] ? FISH(thiefID, null, null) -> /* fish request, Listing @\ref{lst:fish-msg-reaction}@ */
         :: chans[me] ? AUTH(thiefID, authorisedSeq, null) -> /* request response, Listing @\ref{lst:schedauth-msg-reaction}@ */
         :: chans[me] ? SCHEDULE(victimID, seq, null) -> /* recv spark, Listing @\ref{lst:schedule-msg-reaction}@ */
         :: chans[me] ? DENIED(thiefID, deniedSeq, null) -> /* request response, Listing @\ref{lst:scheddenied-msg-reaction}@ */
         :: chans[me] ? NOWORK(victimID, null, null) -> /* fish unsuccessful, Listing @\ref{lst:nowork-msg-reaction}@ */
         :: chans[me] ? OBSOLETE(thiefID, null, null) -> /* task obsolete, Listing @\ref{lst:obsolete-msg-reaction}@ */
         :: chans[me] ? DEADNODE(deadNodeID, null, null) -> /* notification, Listing @\ref{lst:deadnode-msg-reaction}@ */
       fi;
  fi;

  if   /* if the IVar on the supervisor if full then terminate */
    :: (supervisor.ivar == 1) -> goto END;
    :: else -> goto WORKER_RECEIVE; @\label{promela:worker-loop-recv}@
  fi;    
  
END:
\end{promelacode}
\end{Code}
#+END_LATEX

*** Node Failure

Failure is modeled by a non-deterministic choice that nodes can make
at each iteration of the repetition flow in the ~Worker~ definition in
Listing \ref{lst:promela-worker-control-flow}. A node can choose to
die on line \ref{promela:worker-die}. This sets the ~dead~ value to
~true~ for the node, the ~report_death~ macro is invoked, and the
repetition flow is escaped with \texttt{goto~END} on line
\ref{promela:worker-goto-end} which terminates the process.

#+BEGIN_LATEX
\begin{Code}
\begin{promelacode}{Reporting Node Failure}{lst:promela-failure-report}
inline report_death(me){ @\label{promela:report-death-me}@
  chans[0] ! DEADNODE(me, null, null) ; 
  chans[1] ! DEADNODE(me, null, null) ;
  chans[2] ! DEADNODE(me, null, null) ;
  chans[3] ! DEADNODE(me, null, null) ; /* supervisor */
}
\end{promelacode}
\end{Code}
#+END_LATEX

The ~report_death~ definition is shown in Listing
\ref{lst:promela-failure-report}. The HdpH-RS transport layer sends
~DEADNODE~ messages to the scheduler message handler on each node when
a connection with a remote node is lost. The ~report_death~ macro
takes an integer ~me~ on line \ref{promela:report-death-me} which is
used to identify by other nodes to identify the failed node, and sends
a ~DEADNODE~ message to the other three nodes including the
supervisor. Their reaction to this message is shown in Section [[Message
Handling]]. This macro is not executed atomically, modeling the
different failure detection latencies on each HdpH-RS node. SPIN
is therefore able to search through state transitions whereby a node
failure is only partially detected across all nodes.

*** Node State

#+BEGIN_LATEX
\begin{figure}

\begin{minipage}{1\linewidth}
% \hspace{-.5cm}
\begin{minipage}[t]{0.5\linewidth}
\vspace{0pt}

\begin{promelacode}{Sparkpool State}{lst:promela-sparkpool}
typedef Sparkpool { @\label{code:Sparkpool-typedef}@
   int spark_count; /* #sparks in pool */ @\label{code:spark-count}@
   int spark;       /* highest #replica */ @\label{code:tracing-spark}@
};
\end{promelacode}

\begin{promelacode}{Spark State}{lst:promela-spark-state}
mtype = { ONNODE , INTRANSITION }; @\label{code:tracing-mtype}@
typedef Spark {
  int highestReplica=0; @\label{code:tracing-highestReplica}@
  Location location; @\label{code:tracing-location}@
  mtype context=ONNODE; @\label{code:tracing-context}@
  int age=0; @\label{code:tracing-age}@
}

typedef Location @\label{code:Location-typedef}@
{
  int from; @\label{code:tracing-from}@
  int to; @\label{code:tracing-to}@
  int at=3; @\label{code:tracing-at}@
}
\end{promelacode}

\end{minipage}
\begin{minipage}[t]{0.5\linewidth}
\vspace{0pt}

\begin{promelacode}{Supervisor State}{lst:promela-supervisor-state}
typedef SupervisorNode { @\label{code:SupervisorNode-typedef}@
  Sparkpool sparkpool; @\label{code:supervisor-sparkpool}@
  bool waitingSchedAuth=false; @\label{code:supervisor-waitingSchedAuth}@
  bool resultSent=false;
  bit ivar=0; @\label{code:supervisor-ivar}@
};
\end{promelacode}

\begin{promelacode}{Worker State}{lst:promela-worker-state}
typedef WorkerNode { @\label{code:WorkerNode-typedef}@
  Sparkpool sparkpool;
  int waitingFishReplyFrom; @\label{code:worker-waitingFishReplyFrom}@
  bool waitingSchedAuth=false; @\label{code:worker-waitingSchedAuth}@
  bool resultSent=false;
  bool dead=false; @\label{code:worker-dead}@
  int lastTried; @\label{code:worker-lastTried}@
};
\end{promelacode}
\end{minipage}
\end{minipage}

\caption{State Abstraction in Promela Model}
\end{figure}

#+END_LATEX

**** Sparkpool

The supervisor and worker nodes each have a local sparkpool. The state
of a sparkpool is in Listing \ref{lst:promela-sparkpool}. The
sparkpool capacity in the model is 1. The sparkpool is either empty or
it holds a spark. When it holds a spark, its replication number is
used to send messages to the supervisor: to request scheduling
authorisation in a ~REQ~ message, and confirm receipt of the spark
with an ~ACK~ message.

The spark's Location is stored in ~location~ and ~context~ on lines
\ref{code:tracing-location} and \ref{code:tracing-context} of Listing
\ref{lst:promela-spark-state}, which are modified by the supervisor
when ~REQ~ and ~ACK~ messages are received. The location context of
the supervised spark is either ~ONNODE~ or ~INTRANSITION~ (Section
[[Task Locality]]). The most recently allocated replica number is held in
~highestReplica~ on line \ref{code:tracing-highestReplica}, and is
initially set to $0$. The age of the spark on line
\ref{code:tracing-age} is initially set to $0$, and is incremented
when the spark is scheduled to another node or when it is replicated.

The actual expression in the spark is not modeled. The
indistinguishable scenario when an ~IVar~ is written either once or
multiple times is demonstrated with executions through the operational
semantics in Section [[Execution of Transition Rules]]. Any node,
including the supervisor, may transmit a result to the supervisor if
it holds a spark copy. As such, a spark is simply represented as its
replication count (line \ref{code:tracing-spark}).

**** Supervisor State

The local state of the supervisor is shown in Listing
\ref{lst:promela-supervisor-state}. In the initial system state, it
adds the spark to its sparkpool (line
\ref{code:supervisor-sparkpool}), which may be fished away by a worker
node. To minimise the size of the state machine, the supervisor does
not try to steal the spark or subsequent replicas once they are fished
away.  The ~IVar~ is represented by a bit on line
\ref{code:supervisor-ivar}, $0$ for empty and $1$ for full. Lastly, a
~waitingSchedAuth~ (line \ref{code:supervisor-waitingSchedAuth}) is
used to reject incoming ~REQ~ messages, whilst it waits for an ~AUTH~
from itself if it is targeted by a thief.

#+CAPTION:    Location Tracking with \texttt{migrate\_supervised\_spark} Transition Rule
#+LABEL:      fig:promela-location-tracking
#+ATTR_LaTeX: :width 80mm
[[./img/chp4/msc/hdphrs-fishing-protocol-promela.pdf]]

An example of modifying location tracking with the
~migrate_supervised_spark~ rule (Section [[Small Step Operational
Semantics]]) is shown in Figure \ref{fig:promela-location-tracking}. It
is a more detailed version of the fault tolerant fishing protocol from
Figure \ref{fig:migration-modification} of Section [[Task Locality]]. The
Promela message passing syntax in the MSC is an abstraction of ~send~
and ~receive~ Haskell function calls in the HdpH-RS implementation
(Section [[Message Passing API]]). Node B is the victim, and node C is the
thief. Node A hosts supervised future $i \{{ j
\llangle{M}\rrangle_{B}^{\mathscr{S}}}\}_{A}$ (Section [[Small Step
Operational Semantics]]). Once the message sequence is complete, the new
supervised future state is $i \{{ j
\llangle{M}\rrangle_{C}^{\mathscr{S}} }\}_{A}$.

#+CAPTION:    Supervised Spark Recovery with \texttt{recover\_supervised\_spark} Transition Rule
#+LABEL:      fig:promela-location-tracking-recovery
#+ATTR_LaTeX: :width 90mm
[[./img/chp4/msc/hdphrs-fishing-protocol-promela-recovery.pdf]]

An example of recovering a supervised spark with the
~recovery_supervised_spark~ rule is shown in Figure
\ref{fig:promela-location-tracking-recovery}. The supervised spark $j$
of supervised future $i \{{ j \llangle{M}\rrangle_{C}^{\mathscr{S}}
}\}_{A}$ is on node C. The node hosting $i$ receives a ~DEADNODE~
message about node C, and creates a new replica $k$. The existence and
location state of $k$ is added to the supervised future $i \{{ k
\llangle{M}\rrangle_{A}^{\mathscr{S}} }\}_{A}$.

**** Worker State

The local state of a worker is shown in Listing
\ref{lst:promela-worker-state}. When a thieving node proactively sends
a fish to another node, the ~waitingFishReplyFrom~ stores the channel
index identifier for the victim i.e. 0, 1 or 2 if targeting a worker
node, or 3 if targeting the supervisor. The value of
~waitingFishReplyFrom~ is reset to $-1$ when a ~SCHEDULE~ is received,
or ~NOWORK~ message is received allowing the node to resume
fishing. When a victim has sent a ~REQ~ to the supervisor, the
~waitingSchedAuth~ boolean on line \ref{code:worker-waitingSchedAuth}
is used to reject subsequent fishing attempts from other thieves until
it receives a ~AUTH~ or ~NOWORK~. When a worker sends a ~FISH~, it
records the target in ~lastTried~ on line
\ref{code:worker-lastTried}. If a ~NOWORK~ message is received, then
this value is used to ensure that the new target is not the most
recent target. This avoids cyclic states in the model when the same
victim forever responds ~FISH~ requests with a ~NOWORK~ replies to the
thief. Lastly, the ~dead~ boolean (line \ref{code:worker-dead}) is
used to represent node failure in the model. Once this is switched to
~true~, the ~report_death~ macro (Section [[Node Failure]]) is used to
transmit a ~DEADNODE~ message to all other nodes.

*** Spark Location Tracking

When the task tracker records on the supervisor is \texttt{ONNODE},
then it can be sure that the node identified with ~spark.location.at~
(line \ref{code:tracing-at} of Listing
\ref{lst:promela-supervisor-state}) is holding the spark. If the node
fails at this point, then the spark should be recreated as it has
certainly been lost. However, when a spark is in transition between
two nodes i.e \texttt{INTRANSITION}, the supervisor cannot be sure of
the location of the spark (Section [[Task Locality]]); it is on either of
the nodes identified by ~spark.location.from~ or ~spark.location.to~
(lines \ref{code:tracing-from} and \ref{code:tracing-to}). To overcome
this uncertainty, the model faithfully reflects the HdpH-RS
pessimistic duplication strategy in algorithm \ref{alg:handleDeadnode}
when a ~DEADNODE~ is received. This potentially generates replicas
that concurrently exist in the model. This is handled using replica
counts (Section [[Duplicate Sparks]]).

*** Message Handling

This section presents the message handling in the Promela abstraction
that models the fault tolerant scheduling algorithm from Section [[Fault
Tolerant Scheduling Algorithm]]. An example control flow sequence on the
supervisor is:

1. Receive a ~REQ~ from a victim targeted by a thief for supervised $spark_1$.
2. \label{item:check-onnode} Check that the location for $spark_1$ is
   ~ONNODE victim~.
3. \label{item:set-intransition} Modify location status for $spark_1$
   to ~INTRANSITION victim thief~.
4. \label{item:send-auth} Send an ~AUTH~ to the victim. 

Steps \ref{item:check-onnode} and \ref{item:set-intransition} are
indivisibly atomic in the HdpH-RS design, as the message handler on
each node is single threaded. That is, steps \ref{item:check-onnode},
\ref{item:set-intransition} and \ref{item:send-auth} are executed by
the message handler before the next message is received. Location
state is implemented in HdpH-RS as a mutable ~IORef~ variable, and
steps \ref{item:check-onnode} and \ref{item:set-intransition} check
and modify the state of the ~IORef~ atomically using
~atomicModifyIORef~. The message handling in the Promela model therefore
follows the pattern of: receiving a message; atomically modify local
state; and possibly ended with sending a message.

The Promela ~atomic~ primitive can be used for defining fragments of
code to be executed indivisibly. There is also a ~d_step~ primitive
that serves the same purpose, though with limitations. There can be no
~goto~ jumps, non-deterministic choice is executed deterministically,
and code inside ~d_step~ blocks must be non-blocking. The Promela
abstraction of HdpH-RS scheduling nevertheless opts for ~d_step~
blocks in favour of ~atomic~ blocks after receiving messages whenever
possible. A ~d_step~ sequence can be executed much more efficiently
during verification than an atomic sequence. The difference in
performance can be significant, especially in large-scale
verification \cite{promela-d-step}.

**** FISH Messages

Fish messages are sent between worker nodes, from a thief to a
victim. When a victim receives a fish request, it checks to see if it
holds the spark, and if it is not waiting for authorisation from the
supervisor node to schedule it elsewhere. If this is the case, the
state of ~waitingSchedAuth~ for the spark is set to true, and a ~REQ~
is sent to the supervisor node. Otherwise, the thief is sent a
~NOWORK~ message. The reaction to receiving a ~FISH~ is shown in
Listing \ref{lst:fish-msg-reaction}, corresponding to Algorithm
\ref{alg:handleFish} of Section [[Fault Tolerant Scheduling Algorithm]].

#+BEGIN_LATEX
\begin{Code}
\begin{promelacode}{Response to \texttt{FISH} Messages}{lst:fish-msg-reaction}
/* on worker nodes */
chans[me] ? FISH(thiefID, null, null) ->
    if   /* worker has spark and is not waiting for scheduling authorisation */
      :: (worker[me].sparkpool.spark_count > 0 && ! worker[me].waitingSchedAuth) ->
         worker[me].waitingSchedAuth = true;
         chans[3] ! REQ(me, thiefID, worker[me].sparkpool.spark);
      :: else -> chans[thiefID] ! NOWORK(me, null, null) ; /* worker doesn't have the spark */
    fi

/* on the supervisor */
chans[3] ? FISH(thiefID, null,null) ->
    if   /* supervisor has spark and is not waiting for scheduling authorisation from itself */
      :: (supervisor.sparkpool.spark_count > 0 && ! supervisor.waitingSchedAuth) -> 
         supervisor.waitingSchedAuth = true;
         chans[3] ! REQ(3, thiefID, supervisor.sparkpool.spark);
      ::  else -> chans[thiefID] ! NOWORK(3, null,null) ; /* supervisor don't have the spark */
    fi; 
\end{promelacode}
\end{Code}
#+END_LATEX

**** REQ Messages

This message is sent from a victim to the supervising node.  The first
check that a supervisor performs is the comparison between the highest
replication number of a task copy ~spark.highestSequence~ and the
replication number of the task to be authorised for scheduling
~seq~. If they are not equal, then an ~OBSOLETE~ message is sent to
the victim, indicating that the task should be discarded. This checks
that the spark to be stolen is the most recent copy of a future task
in HdpH-RS.

If the replication numbers are equal, there is one more condition to
satisfy for authorisation to be granted. The supervising node only
authorises the migration of the spark if the book keeping status of
the spark is ~ONNODE~ (see Section [[Task Locality]]). If this is the
case, the spark book keeping is set to ~INTRANSITION~, updating the
~spark.location.from~ and
~spark.location.to~ fields to reflect the movement of
the spark. Finally, the ~AUTH~ message is sent to the victim. If the
context of the spark is ~INTRANSITION~, the schedule request
from the victim is denied by responding with ~DENIED~. The reaction to
receiving a ~REQ~ is shown in Listing \ref{lst:schedreq-msg-reaction},
corresponding to Algorithm \ref{alg:handleReq} of Section [[Fault
Tolerant Scheduling Algorithm]].

#+BEGIN_LATEX
\begin{Code}
\begin{promelacode}{Response to \texttt{REQ} Messages}{lst:schedreq-msg-reaction}
chans[3] ? REQ(victimID, thiefID, seq) ->
    if
      :: seq == spark.highestSequence ->
         if
              /* conditions for authorisation */
           :: spark.context == ONNODE && ! worker[thiefID].dead-> @\label{promela:check-not-dead}@
              d_step {
                spark.context  = INTRANSITION;
                spark.location.from = victimID ;
                spark.location.to = thiefID ;
              }
              chans[victimID] ! AUTH(thiefID, seq, null); /* authorise request */

              /* otherwise deny request */
           :: else ->
              chans[victimID] ! DENIED(thiefID, seq, null); /* deny request */
         fi
      :: else ->
         chans[victimID] ! OBSOLETE(thiefID, null, null); /* obsolete sequence number */
    fi
\end{promelacode}
\end{Code}
#+END_LATEX

**** AUTH Messages

This message is sent from the supervising node to a victim that had
requested authorisation to schedule the spark to a thief. There are no
pre-conditions for the response --- the victim sends the spark in a
~SCHEDULE~ message to the thief. The reaction to receiving an ~AUTH~
is shown in Listing \ref{lst:schedauth-msg-reaction}, corresponding to
Algorithm \ref{alg:handleAuth} of Section [[Fault Tolerant Scheduling
Algorithm]].

#+BEGIN_LATEX
\begin{Code}
\begin{promelacode}{Response to \texttt{AUTH} Messages}{lst:schedauth-msg-reaction}
/* on worker nodes */
chans[me] ? AUTH(thiefID, authorisedSeq, null) ->
    d_step {
      worker[me].waitingSchedAuth = false;
      worker[me].sparkpool.spark_count--;
      worker[me].waitingFishReplyFrom = -1;
    }
    chans[thiefID] ! SCHEDULE(me, worker[me].sparkpool.spark, null);

/* on the supervisor */
chans[3] ? AUTH(thiefID, authorizedSeq, null) ->
    d_step {
      supervisor.waitingSchedAuth = false;
      supervisor.sparkpool.spark_count--;
    }
    chans[thiefID] ! SCHEDULE(3, supervisor.sparkpool.spark ,null);
\end{promelacode}
\end{Code}
#+END_LATEX

**** SCHEDULE Messages

A thief sends a victim a ~FISH~ message in the hunt for sparks. The
victim will later reply with a ~SCHEDULE~ message, if authorised by
the supervisor. The thief accepts the spark, and sends an ~ACK~
message to the supervisor of that spark. The reaction to receiving a
~SCHEDULE~ is shown in Listing \ref{lst:schedule-msg-reaction},
corresponding to Algorithm \ref{alg:handleSchedule} of Section [[Fault
Tolerant Scheduling Algorithm]].

#+BEGIN_LATEX
\begin{Code}
\begin{promelacode}{Response to \texttt{SCHEDULE} Messages}{lst:schedule-msg-reaction}
chans[me] ? SCHEDULE(victimID, seq, null) ->
    d_step {
      worker[me].sparkpool.spark_count++;
      worker[me].sparkpool.spark = seq ;
      spark.age++;
    }
    chans[3] ! ACK(me, seq, null) ; /* Send ACK To supervisor */
\end{promelacode}
\end{Code}
#+END_LATEX

**** ACK Messages

This message is sent from a thief to the supervisor. An ~ACK~ is only
acted upon if the replication number ~seq~ for the task equals the
replication number known by the supervisor. If they are not equal, the
~ACK~ is simply ignored. If the replication numbers are equal, the
response is an update of the location state for that spark ---
switching from ~INTRANSITION~ to ~ONNODE~. The reaction to receiving a
~ACK~ is shown in Listing \ref{lst:ack-msg-reaction}, corresponding to
Algorithm \ref{alg:handleAck} of Section [[Fault Tolerant Scheduling
Algorithm]].

#+BEGIN_LATEX
\begin{Code}
\begin{promelacode}{Response to \texttt{ACK} Messages}{lst:ack-msg-reaction}
chans[3] ? ACK(thiefID, seq, null) ->
    if   /* newest replica arrived at thief */
      :: seq == spark.highestSequence ->
         d_step {
           spark.context = ONNODE;
           spark.location.at = thiefID ;
         }
        
         /* ACK not about newest replica */
      :: else -> skip ;
    fi
\end{promelacode}
\end{Code}
#+END_LATEX

**** DENIED Messages

This message is sent from the supervising node to a victim that had
requested authorisation to schedule the spark to a thief. There are no
pre-conditions for the response --- the victim sends a ~NOWORK~
message to the thief. The reaction to receiving an ~DENIED~ is shown
in Listing \ref{lst:scheddenied-msg-reaction}, corresponding to
Algorithm \ref{alg:handleDenied} of Section [[Fault Tolerant Scheduling
Algorithm]].

#+BEGIN_LATEX
\begin{Code}
\begin{promelacode}{Response to \texttt{DENIED} Messages}{lst:scheddenied-msg-reaction}
/* on worker nodes */
chans[me] ? DENIED(thiefID, deniedSeq, null) ->
    worker[me].waitingSchedAuth = false;
    chans[thiefID] ! NOWORK(me, null, null) ;

/* on the supervisor */
chans[3] ? DENIED(thiefID, deniedSeq,null) ->
    supervisor.waitingSchedAuth = false;
    chans[thiefID] ! NOWORK(3, null, null) ;
\end{promelacode}
\end{Code}
#+END_LATEX

**** NOWORK Messages

A thief may receive a ~NOWORK~ in response to a ~FISH~ message that it
had sent to a victim. This message was returned either because the
victim did not hold the spark, or because the victim was waiting for
authorisation to schedule the spark in response to an earlier ~FISH~
message. When a ~NOWORK~ message is received, the thief is free to
target another victim for work. The reaction to receiving a ~NOWORK~
is shown in Listing \ref{lst:nowork-msg-reaction}, corresponding to
Algorithm \ref{alg:handleNowork} of Section [[Fault Tolerant Scheduling
Algorithm]].

#+BEGIN_LATEX
\begin{Code}
\begin{promelacode}{Response to \texttt{NOWORK} Messages}{lst:nowork-msg-reaction}
chans[me] ? NOWORK(victimID, null, null) ->
    worker[me].waitingFishReplyFrom = -1;  /* can fish again */
\end{promelacode}
\end{Code}
#+END_LATEX

**** OBSOLETE Messages

An ~OBSOLETE~ message is sent from a supervisor to a victim. It is a
possible response message to a ~REQ~ message when a scheduling request
is made with respect to an old spark copy. The message is used to
inform a victim to discard the spark, which then returns a ~NOWORK~
message to the thief. The reaction to receiving an ~OBSOLETE~ is shown
in Listing \ref{lst:obsolete-msg-reaction}, corresponding to Algorithm
\ref{alg:handleObsolete} of Section [[Fault Tolerant Scheduling
Algorithm]].

#+BEGIN_LATEX
\begin{Code}
\begin{promelacode}{Response to \texttt{OBSOLETE} Messages}{lst:obsolete-msg-reaction}
chans[me] ? OBSOLETE(thiefID, null, null) ->
    d_step {
      worker[me].waitingSchedAuth = false;
      worker[me].sparkpool.spark_count--;
      worker[me].waitingFishReplyFrom = -1;
    }
    chans[thiefID] ! NOWORK(me, null, null) ;
\end{promelacode}
\end{Code}
#+END_LATEX

**** DEADNODE Messages

The HdpH-RS transport layer propagates ~DEADNODE~ messages to a node
that has lost a connection with another node. This message has a
purpose for both the supervisor of the spark, and worker nodes who may
be waiting for a fishing reply from the failed node. The reaction to
receiving a ~DEADNODE~ is shown in Listing
\ref{lst:deadnode-msg-reaction}, corresponding to Algorithm
\ref{alg:handleDeadnode} of Section [[Fault Tolerant Scheduling
Algorithm]].

***** Worker

If a thief node has previously sent a ~FISH~ message to a victim node,
it will be involved in no scheduling activity until receiving a
reply. If the victim fails, it will never return a reply. So instead,
the ~DEADNODE~ message is used to unblock the thief, allowing to fish
elsewhere.

***** Supervisor

If the supervising node receives indication of a failed worker node,
then it must check to see if this affects the liveness of the
spark. The location status of the spark is checked. If the spark
was on the failed node at the time of the failure, the supervisor
recreates the spark in its local sparkpool from line
\ref{promela:should-replicate}. Furthermore, if the spark was in
transition towards or away from the failed node, again the spark is
recreated on the supervisor locally. An example of this is in Figure
\ref{fig:promela-location-tracking-recovery} in Section [[Node State]]. A
migration of supervised spark $j$ is attempted with the
~migrate_supervised_spark~ rule, from B to C. Node C fails. The
liveness of supervised spark $j$ cannot be determined, so the
~recover_supervised_spark~ is triggered to create a replica $k$ on
node A. This is affected in the Promela model in Listing
\ref{lst:deadnode-msg-reaction}.


#+BEGIN_LATEX
\begin{Code}
\begin{promelacode}{Response to \texttt{DEADNODE} Messages}{lst:deadnode-msg-reaction}
/* On a worker node */
chans[me] ? DEADNODE(deadNodeID, null, null) ->
    d_step {
      if   /* reset to start fishing from other nodes */
        :: worker[me].waitingFishReplyFrom > deadNodeID ->
           worker[me].waitingFishReplyFrom = -1 ;
        :: else -> skip ;
      fi
    }

/* On the supervising node */
chans[3] ? DEADNODE(deadNodeID, null, null) ->
    bool should_replicate;
    d_step {
      should_replicate = false;

      if   /* decide if spark needs replicating */
        :: spark.context == ONNODE \
           && spark.location.at == deadNodeID -> should_replicate = true;
        :: spark.context  == INTRANSITION \
           && (spark.location.from == deadNodeID \
               || spark.location.to == deadNodeID) -> should_replicate = true;
        :: else -> skip;
      fi;

      if   /* replicate spark */
        :: should_replicate -> @\label{promela:should-replicate}@
           spark.age++;
           supervisor.sparkpool.spark_count++;
           spark.highestSequence++;
           supervisor.sparkpool.spark = spark.highestSequence ;
           spark.context = ONNODE;
           spark.location.at = 3 ;
        :: else -> skip;
      fi;
    }
\end{promelacode}
\end{Code}
#+END_LATEX

**** Node Automata

The ~Supervisor~ and ~Worker~ nodes are both translated in to a finite
automaton. SPIN is used in Section [[Model Checking Results]] to search
the intersection of ~Supervisor~, ~Worker~ and ~Scheduler~ with the
LTL property automaton in Section [[Linear Temporal Logic &
Propositional Symbols]] to validate the HdpH-RS scheduler
abstraction. The finite automaton for the ~Supervisor~ node is shown
in Figure \ref{fig:supervisor-automata}. The finite automaton for the
~Worker~ node is shown in Figure \ref{fig:worker-automata}.

#+BEGIN_LATEX
\begin{sidewaysfigure}
\includegraphics{spin_model/supervisor.pdf}
\caption{Supervisor Automata}
\label{fig:supervisor-automata}
\end{sidewaysfigure}

\begin{sidewaysfigure}
\includegraphics{spin_model/worker.pdf}
\caption{Worker Automata}
\label{fig:worker-automata}
\end{sidewaysfigure}
#+END_LATEX

** Verifying Scheduling Properties

SPIN is used to generate an optimised verification program from the
high level specification. If any counterexamples to the Linear
Temporal Logic (LTL) correctness claims are detected, these can be fed
back into the interactive simulator and inspected in detail to
establish and remove their cause. Section [[Linear Temporal Logic &
Propositional Symbols]] presents the LTL grammar and propositional
symbols used in the key resiliency properties in Section [[Model
Checking Results]].

*** Linear Temporal Logic & Propositional Symbols

Temporal logic \cite{Prior-1957} provides a formalism for describing
the occurrence of event in time that is suitable for reasoning about
concurrent programs \cite{DBLP:conf/focs/Pnueli77}. To prove that a
program satisfies some property, a standard method is to use
LTL model checking. When the property is expressed with an LTL
formula, the SPIN model checker transforms the negation of this
formula into a Büchi automaton, building the product of that automaton
with the programs, and checks this product for emptiness
\cite{DBLP:conf/cav/GastinO01}. An LTL formula $\phi$ for SPIN may
contain any propositional symbol $p$, combined with unary or binary,
boolean and/or temporal operators \cite{DBLP:journals/tse/Holzmann97},
using the grammar in Figure \ref{fig:ltl-grammar}. LTL is used to
reason about causal and temporal relations of the HdpH-RS scheduler
properties. The special temporal operators used for verifying the
fault tolerant scheduling design in Section [[Model Checking Results]] are
shown in Table \ref{tab:ltl-operators}.


#+BEGIN_LATEX
\setlength{\grammarparsep}{7pt plus 1pt minus 1pt} % increase separation between rules
\setlength{\grammarindent}{5em} % increase separation between LHS/RHS 

\begin{figure}
\begin{BNF}
\begin{multicols}{2}
\begin{grammar}
<$\phi$> ::= "p"
 \alt "true"
 \alt "false"
 \alt $\left(\phi\right)$
 \alt $\langle\psi\rangle$ <binop> $\langle\phi\rangle$
 \alt $\langle\psi\rangle$ <unop> $\langle\phi\rangle$
\vspace*{\fill}

\columnbreak

<unop> ::= $\Box$ (always)
 \alt $\Diamond$ (eventually)
 \alt "!" (logical negation)

<binop> ::= $\mathbin{\mathcal{U}\kern-.1em}$ (strong until)
 \alt "&&" (logical and)
 \alt "||" (logical or)
 \alt $\rightarrow$ (implication)
 \alt $\leftrightarrow$ (equivalence)
\end{grammar}
\vspace*{\fill}
\end{multicols}
\end{BNF}
\caption{LTL Grammar}
\label{fig:ltl-grammar}
\end{figure}
#+END_LATEX

#+BEGIN_LATEX
\begin{table}\small
\begin{center}
\begin{tabular}{|c|c|}
\hline
Formula & Explanation \\
\hline
\hline
$\Box \phi$                           & $\phi$ must always hold               \\
$\Diamond \Box \phi$                       & $\phi$ must eventually always hold           \\
$\psi \mathbin{\mathcal{U}\kern-.1em} \phi$     & $\psi$ must hold until at least $\phi$ is true \\
\hline
\end{tabular}
\caption{LTL Formulae Used To Verify Fault Tolerant Scheduler}
\label{tab:ltl-operators}
\end{center}
\end{table}
#+END_LATEX

SPIN translates LTL formulae into \texttt{never} claims, automatically
placing accept labels within the claim. The SPIN verifier then checks
to see if this \texttt{never} claim can be violated. To prove that no
execution sequence of the system matches the negated correctness
claim, it suffices to prove the absence of acceptance cycles in the
combined execution of the system and the Büchi automaton representing
the claim \cite{DBLP:journals/tse/Holzmann97}. An LTL formula is a
composition of propositional symbols, which are defined with macros,
.e.g. ~#define p (x > 0)~. The propositional symbols used in the
verification of scheduling Promela model are shown in Listing
\ref{lst:ft-propositional-symbols}.

#+BEGIN_LATEX
\begin{Code}
\begin{promelacode}{Propositional Symbols used in LTL Formulae of Fault Tolerant Properties}{lst:ft-propositional-symbols}
/* IVar on the supervisor node is full */
#define ivar_full  ( supervisor.ivar == 1 )

/* IVar on the supervisor node is empty */
#define ivar_empty ( supervisor.ivar == 0 )

/* No worker nodes have failed */
#define all_workers_alive ( !worker[0].dead && !worker[1].dead && !worker[2].dead )

/* One or more nodes have transmitted a value to supervisor to fill IVar */
#define any_result_sent ( supervisor.resultSent
                          || worker[0].resultSent
                          || worker[1].resultSent
                          || worker[2].resultSent )
\end{promelacode}
\end{Code}
#+END_LATEX

*** Verification Options & Model Checking Platform

**** Verification Options

#+BEGIN_LATEX
\begin{Code}
\begin{shellcode}{Compiling \& Executing SPIN Verifier}{lst:verifier-generation}
$ spin -a -m hdph-rs.pml
$ gcc -DMEMLIM=1024 -O2 -DXUSAFE -DCOLLAPSE -w -o pan pan.c
$ ./pan -m10000  -E -a -f -c1 -N never_2
\end{shellcode}
\end{Code}
#+END_LATEX

Listing \ref{lst:verifier-generation} shows how the SPIN verifier is
generated, compiled and executed.

*Generation* The ~-a~ flag tells SPIN to generate a verifier in a
~pan.c~ source file. The ~-m~ flag tells SPIN to lose messages sent to
full channel buffers. This reflects the impossibility of filling a TCP
receive buffer on a dead HdpH-RS node. The ~-m~ flag prevents healthy
nodes from blocking of full channels in the model.

*Compilation* The memory limit is set to 1024Mb with the ~-DMEMLIM~
flag. Checks for channel assertion violations are disabled with
~-DXUSAFE~, as they are not used. The ~-DCOLLAPSE~ flag enables state
vector compression.

*Verification* The maximum search space is set to 10000 with the ~-m~
flag, and a maximum of 124 is used. The ~-a~ flag tells SPIN to check
for acceptance cycles in the LTL property automata. Weak fairness is
enabled with the ~-f~ flag. The ~-E~ flag suppresses the reporting of
invalid end states. This ensures that every statement always
enabled from a certain point is eventually executed. The verification
ends after finding the first error with the ~-c~ flag. The ~-N~ flag
selects the LTL property to be verified.

** Model Checking Results

This section reports the results of SPIN verification. The results of
model checking the three LTL properties are in Table
\ref{tab:model-checking-results}. Taking the $\Diamond\; \Box\;
ivar\_full$ property as an example, the results can be interpreted as
follows. A reachable depth of 124 is found by SPIN for the model. The
reachable state space is 8.2 million. A total of 22.4 million
transitions were explored in the search. Actual memory usage for
states was 84.7Mb.

#+BEGIN_LATEX
\begin{table}\small
\begin{tabular}{|l|c||c|c|c|c|c|}
\hline
LTL Formula & Errors & Depth & States & Transitions & Memory\\
\hline
\hline
$\Box\; all\_workers\_alive$ & Yes & 11 & 5 & 5 & 0.2Mb \\
\hline
$\Box\; (ivar\_empty\; \mathbin{\mathcal{U}\kern-.1em}\; any\_result\_sent)$ & No & 124 & 3.7m & 7.4m & 83.8Mb \\
\hline
$\Diamond\; \Box\; ivar\_full$ & No & 124 & 8.2m & 22.4m & 84.7Mb \\
\hline
\end{tabular}
\caption{Model Checking Results}
\label{tab:model-checking-results}
\end{table}
#+END_LATEX

*** Counter Property

**** Validating the Possibility of Worker Node(s) Fail

To check that worker nodes are able to fail in the model, a verification
attempt is made on the $\Box\; all\_workers\_alive$ LTL formula. To
check that the model has the potential to kill mortal workers, SPIN
searches for a counter-example system state with any of the
~worker[0].dead~, ~worker[1].dead~ or ~worker[2].dead~ fields set to
~true~. SPIN trivially identifies a counter example after searching 5
system states by executing the choice on line \ref{promela:worker-die}
in Listing \ref{lst:promela-worker-control-flow} to kill a node.

*** Desirable Properties

**** The IVar is Empty Until a Result is Sent

To check that the model is faithful to the fact that an ~IVar~ is
empty at least until its corresponding task has been evaluated, the
$\Box\;(ivar\_empty\;\mathbin{\mathcal{U}\kern-.1em}\;any\_result\_sent)$
formula is verified. SPIN searches for two violating system
states. First, where ~any_result_sent~ is true when ~ivar_empty~ is
false. Second, a cycle of identical system states is identified while
~any_result_sent~ remains false. This is due to the nature of the
/strong until/ connective stipulating that ~any_result_sent~ must
eventually be true in some future state. This is preferred to the
/weak until/ connective, which would not verify that a result is ever
sent to the supervisor. SPIN cannot find a violating system state
after exhaustively searching 3.7 million reachable states up to a
depth of 124.


**** The IVar is Eventually Always Full

The key property for fault tolerance is that the ~IVar~ on the
supervisor must eventually always be full. This would indicate that
either a worker node has sent a ~RESULT~ message, or the supervisor
has written to the ~IVar~ locally. The $\Diamond\; \Box\; ivar\_full$
formula is verified. SPIN searches for a system state cycle when
~ivar_full~ remains false i.e. ~supervisor.ivar==0~. Each time the
spark is passed to a node, its age is incremented. Each time the
supervisor creates a replica, the spark's age is incremented. When
this age reaches 100, the model enforces the deterministic choice of
sending a ~RESULT~ message to the supervisor on any nodes that hold a
replica, representing ~rput~ calls in HdpH-RS. The supervisor sets
~supervisor.ivar~ to $1$ when it receives a ~RESULT~ message.

This LTL property checks for the fatal scenario when the supervisor
does not replicate the spark in the model, when it must in order to
ensure the existence of at least one replica. This would happen if the
HdpH-RS fault tolerant fishing protocol algorithm (Section [[Fault
Tolerant Scheduling Algorithm]]) did not catch corner cases whereby
spark location tracking becomes invalidated. The consequence would be
the supervisor's incorrect decision not to replication the spark when
a ~DEADNODE~ message is received. Should the location tracking be
invalidated and the dead node held the only replica, no nodes would
ever be able to send a ~RESULT~ message to the supervisor. SPIN
therefore searches for states where the ~spark_count~ value is 0 for
all nodes, and the supervisor does not create a replica in any future
state. SPIN cannot find a violating system state after exhaustively
searching 8.2 million reachable states up to a depth of 124.

** Identifying Scheduling Bugs

SPIN identified bugs in the fault tolerant scheduling algorithm
(Section [[Fault Tolerant Scheduling Algorithm]]), which were abstracted
into earlier iterations of the Promela model. This section describes
one of many examples of how SPIN identified transitions to violating
states with respect to the $\Diamond\; \Box\; ivar\_full$ property.
Each bug was removed with modifications to the scheduling algorithm
(Section [[Fault Tolerant Scheduling Algorithm]]), and corresponding fixes
in the Haskell scheduler implementation (Chapter [[Implementing a Fault
Tolerant Programming Language and Reliable Scheduler]]). With respect to
the bug described here, the fix to the Promela model and the Haskell
implementation is shown in Appendix [[Feeding Promela Bug Fix to
Implementation]].

**** The Bug

The LTL formula \texttt{<>~[]~ivar\_full} claims that the empty ~IVar~
is eventually always full. Attempting to verify this property
uncovered a corner case that would lead to deadlock and an ~IVar~
would never been filled with a value. There were insufficient
pre-conditions for sending an ~AUTH~ message from a supervisor, in an
earlier version of the model. The counter example that SPIN generated
is shown in Figure \ref{fig:identified-bug-with-promela}. The
supervisor node A will not receive another ~DEADNODE~ message about
node C after sending the ~AUTH~ message, so the supervised spark would
not be recovered and the ~IVar~ will never by filled.

#+CAPTION:    Identified Bug Of Scheduling Algorithm Using Promela Model
#+LABEL:      fig:identified-bug-with-promela
#+ATTR_LaTeX: :width 90mm
[[./img/chp4/msc/identified-bug-with-promela.pdf]]

**** The Fix

The fix was to add an additional guard on the supervisor, before
sending an ~AUTH~ to the victim (line \ref{promela:check-not-dead} of
Listing \ref{lst:schedreq-msg-reaction}). Each ~REQ~ message includes
a parameter stating who the thief is (Section [[Work Stealing
Protocol]]). When a ~DEADNODE~ message is received, the HdpH-RS
scheduler removes the failed node from its local virtual machine
(VM). The fix ensures that the thief specified in a ~REQ~ message is
in the supervisor's VM.

* Implementing a Fault Tolerant Programming Language and Reliable Scheduler

This chapter presents the implementation of the HdpH-RS fault tolerant
primitives and verified reliable scheduler. The implementation of the
HdpH-RS system architecture is described in Section [[HdpH-RS
Architecture]]. The ~supervisedSpawn~ and ~supervisedSpawnAt~
implementations are described in Section [[HdpH-RS Primitives]]. Task
recovery implementation is described in Section [[Recovering Supervised
Sparks and Threads]]. The state each node maintains is described in
Section [[HdpH-RS Node State]]. The HdpH-RS transport layer and
distributed virtual machine implementations are described in Section
[[Fault Detecting Communications Layer]]. This includes a discussion on
detecting failures at scale. The failure detection in
HdpH-RS periodically checks for lost connections rather than assuming
failure with timeouts, which may produce false positives when
latency's are variable and high.

A comparison with four fault tolerant programming model
implementations is made in Section [[Comparison with Other Fault
Tolerant Language Implementations]]. They are fault tolerant MPI,
Erlang, Hadoop MapReduce, and the fault tolerance designs for Glasgow
distributed Haskell.

** HdpH-RS Architecture

The HdpH-RS architecture is closely based on the HdpH architecture
\cite{Maier_Trinder_IFL2011}. The architecture supports semi-explicit
parallelism with work stealing, message passing and the remote writing
to \texttt{IVar}s. The HdpH-RS architecture shown in Figure
\ref{fig:hdphrs-architecture} extends the HdpH architecture
presented in Section [[HdpH]] in 5 ways.
1. *IVar Representation* The representation of \texttt{IVar}s has been
   extended from HdpH to support the fault tolerant concept of
   /supervised futures/ (Section [[HdpH-RS Programming Primitives]]). A
   supervised empty future (~IVar~) in HdpH-RS stores the location of its
   corresponding supervised spark or thread, replication counters and
   a scheduling policy /within/ the ~IVar~ (Section [[Implementing
   Futures]]).
2. *Reliable Scheduling* The work stealing scheduler from HdpH is
   extended for fault tolerance, an implementation of the design from
   Section [[Designing a Fault Tolerant Scheduler]].
3. *Failure Detection* The message passing module in HdpH-RS uses a
   fault detecting TCP-based transport layer (Section [[Detecting Node Failure]]).
4. *Guard post* Each node has an additional piece of state, a
   /guard post/, with a capacity to hold one spark. It is used to
   temporarily suspend a spark that is awaiting authorisation to migrate
   to a thief. The purpose of guard posts has previously been
   described in Section [[Task Locality]]. The implementation is shown in
   Section [[Guard Posts]].
5. *Registry* The registry on each HdpH-RS node stores globalised
   \texttt{IVar}s. It is used to identify at-risk computations upon
   failure detection, by inspecting internal state of empty
   \texttt{IVar}s for corresponding supervised spark or supervised
   thread locations.

#+CAPTION:    HdpH-RS System Architecture
#+LABEL:      fig:hdphrs-architecture
#+ATTR_LaTeX: :width 120mm
[[./img/chp5/hdphrs-architecture/hdphrs-architecture.pdf]]

#+CAPTION:    HdpH-RS Module View (extensions from HdpH modules in *bold*)
#+LABEL:      fig:hdphrs-module-view
#+ATTR_LaTeX: :width 120mm 
[[./img/chp5/module-view/hdphrs-module-view.pdf]]

The HdpH code is a collection of hierarchical Haskell modules,
separating the functionality for threadpools, sparkpools,
communication and the exposed programming API. The module hierarchy is
given in Figure \ref{fig:hdphrs-module-view} showing its association
with the transport layer, and illustrates how a user application uses
the HdpH-RS API.

In extending HdpH, the ~FTStrategies~ module is added for the fault
tolerant strategies (Section [[Fault Tolerant Programming with HdpH-RS]]),
and 14 modules are modified. This amounts to an additional 1271 lines
of Haskell code in HdpH-RS, an increase of 52%. The increase is mostly
attributed to the new spawn family of primitives, fault detection and
recovery, and task supervision code in the ~Scheduler~, ~Sparkpool~,
~IVar~ and ~Comm~ modules.

*** Implementing Futures

A case study of programming with futures is given in Appendix
[[Programming with Futures]], and the use of \texttt{IVar}s as futures in
HdpH-RS is described in Section [[HdpH-RS Programming Primitives]]. The
implementation of futures in HdpH-RS is shown in Listing
\ref{lst:ivar-type}. The state of a future is stored in ~IVarContent~
on line \ref{code:ivar-content}. An ~IVar~ is implemented as a mutable
~IVarContent~, in an IORef on line \ref{code:ivar-ioref}. The
~IVarContent~ data structure supports both supervised and unsupervised
futures. An empty future created with ~spawn~ or ~spawnAt~ is
implemented as \texttt{Empty~[]~Nothing}. A supervised empty future,
created with ~supervisedSpawn~ or ~supervisedSpawnAt~, is implemented
as \texttt{Empty~[]~(Just~SupervisedFutureState)}. The task tracking,
replica count and back-up copy of the task expression are stored in
~SupervisedFutureState~, and is described shortly. It is necessary for
recovering lost supervised sparks and threads.
 
#+BEGIN_LATEX
\begin{Code}
\begin{haskellcode}{\texttt{IVar} Representation in HdpH-RS}{lst:ivar-type}
type IVar a = IORef (IVarContent a) @\label{code:ivar-ioref}@
data IVarContent a = @\label{code:ivar-content}@
      Full a @\label{code:full-a}@
    | Empty
  { blockedThreads :: [a -> Thread]
  , taskLocalState :: Maybe SupervisedFutureState } @\label{code:taskLocalState}@
\end{haskellcode}
\end{Code}
#+END_LATEX

In both supervised and unsupervised futures, the definition of a full
future is the same.  A \texttt{Full~a} value on line \ref{code:full-a}
represents a full ~IVar~ $i\gv{M}{n}$, where $i$ on node $n$ holds the
value $M$. When an ~rput~ writes to an ~IVar~, its state changes from
\texttt{Empty~\_~\_} to \texttt{Full~a}. All blocked threads are woken
up and informed of the value in the ~IVar~. All information about its
previously empty state can now be garbage collected. Once the ~IVar~
is full, the HdpH-RS scheduler has satisfied the resiliency property
that the ~IVar~ will be eventually filled despite the presence of
remote node failure (Section [[Model Checking Results]]).

There are 2 pieces of supervision state. The first is within an
~IVar~, and stored locally in a ~SupervisedFuturesState~
structure. The atomic states in the operational semantics in Section
[[Operational Semantics]] show that of all task states, only
supervised sparks need additional state recording a supervised spark's
location and its replica number. Threads that correspond to
supervised threaded futures do not travel with supervision state, as
they cannot migrate after being pushed to the target node. Supervised
futures and supervised spark state is summarised in Table
\ref{tab:supervision-state}. The supervision state within a supervised
spark is later presented as ~SupervisedSpark~ in Listing
\ref{lst:remote-task-state}.

#+BEGIN_LATEX
\begin{table}\small
\begin{center}
\begin{tabular}{c|c|c|c|}
\cline{2-4}
 & \multicolumn{2}{c|}{Supervision State} & \multirow{2}{*}{Future State} \\
\cline{2-3}
  & On supervisor & Within Spark/Thread &  \\
\cline{2-4}
\hline
\multicolumn{1}{|c|}{\texttt{spawn}} & \cross & \cross & $i\gv{}{n}$ \\
\multicolumn{1}{|c|}{\texttt{spawnAt}} & \cross & \cross & $i\gv{}{n}$ \\
\multicolumn{1}{|c|}{\texttt{supervisedSpawn}} & \texttt{SupervisedFutureState} & \texttt{SupervisedSpark} & $i\gv{j \supspk{M}{n'}}{n}$ \\
\multicolumn{1}{|c|}{\texttt{supervisedSpawnAt}} & \texttt{SupervisedFutureState} & \cross & $i\gv{j \thr{M}{n'}}{n}$ \\
\hline
\end{tabular}
\end{center}
\caption{Supervision State}
\label{tab:supervision-state}
\end{table}
#+END_LATEX

**** Supervised Empty Future State

The ~SupervisedFutureState~ data structure in Figure
\ref{lst:local-future-state} is used to implement supervised futures
and is stored in the registry, one entry per supervised future. A copy
of the corresponding task expression is stored within the empty future
as ~task~ on line \ref{code:local-task}. When a remote node failure
puts the liveness of the corresponding task in jeopardy, ~task~ is
extracted from the empty ~IVar~ and rescheduled as a replica. The
replication number ~replica~ on line \ref{code:local-newestSReplica}
in the registry is incremented. The $j$ value in a supervised future
$i\gv{j \supspk{M}{n'}}{n}$ is implemented as a replica number, and
ensures that obsolete spark copies are no longer permitted to migrate
in accordance with ~migrate_supervised_spark~ rule in Section [[Small
Step Operational Semantics]]. The ~scheduling~ field on line
\ref{code:local-scheduling} captures whether the future task was
created as a spark with ~supervisedSpawn~ or as a thread with
~supervisedSpawnAt~, using the definition ~Scheduling~ on line
\ref{code:how-scheduled}. This is used in the recovery of the future
task, determining whether it goes into a local threadpool or the local
sparkpool, in accordance with Algorithm \ref{alg:handleDeadnode} in
Section [[Fault Tolerant Scheduling Algorithm]]. To determine the safety
of a task in the presence of failure, its location is stored within
the empty ~IVar~ on line \ref{code:local-location}. The
~CurrentLocation~ definition is on line
\ref{code:current-location-type}. A supervised spark created with
~supervisedSpawn~ is either \texttt{OnNode~thief} or
\texttt{InTransition~victim~thief}. A thread created with
~supervisedSpawnAt~ is immediately updated to \texttt{OnNode~target}.

#+BEGIN_LATEX
\tikzstyle{every picture}+=[remember picture] \tikzstyle{na} =
[baseline=-.5ex]

\begin{figure}

\begin{haskellcodebare}{name=Test}
-- supervised sparked future is a globalised IVar: (Empty [] (Just SupervisedFutureState))
type GIVar a = GRef (IVar a)
data GRef a = @\grefCode@ { slot :: !Integer, @\ivarAtCode@:: !NodeId }
\end{haskellcodebare}

\Large
\begin{equation*}
\ivarSlot \{{ \taskReplica
\taskScheduling{\taskM}\rrangle_{\taskLocation}^{\mathscr{S}}}\}_{\ivarLocation}
\end{equation*}
\normalsize

\begin{haskellcodebare}{name=Test}
-- | Supervision state in IVar
data SupervisedFutureState =
  SupervisedFutureState
   { -- | highest replica number. 
    replica :: Int @\label{code:local-newestSReplica}@ @\taskReplicaCode@
    -- | Used when rescheduling
    -- recovered tasks.
   , scheduling :: Scheduling @\label{code:local-scheduling}@ @\taskSchedulingCode@
    -- | The copy of the task expression. 
   , task :: Closure (Par ()) @\label{code:local-task}@ @\taskMCode@
    -- | location of most recent task copy.
   , location :: CurrentLocation } @\label{code:local-location}@ @\taskLocationCode@

-- | spark location.
data CurrentLocation = @\label{code:current-location-type}@
  OnNode NodeId | InTransition { movingFrom :: NodeId , movingTo :: NodeId }

-- | The task was originally created as a spark
--  or as an eagerly scheduled thread.
data Scheduling = Sparked | Pushed @\label{code:how-scheduled}@ \end{haskellcodebare}

% Now it's time to draw some edges between the global nodes. Note that
% we have to apply the 'overlay' style.
\begin{tikzpicture}[overlay]
        \draw[->] (taskLocationCodeP) to [out=0,in=-90,looseness=2]
        (taskLocationSymb);
        \draw[->] (taskReplicaCodeP) to
        [out=0,in=-90,looseness=2] (taskReplicaSymb);
        \draw[->]
        (taskSchedulingCodeP) to [out=0,in=-90,looseness=2]
        (taskSchedulingSymb);
        \draw[->] (taskMCodeP) to
        [out=0,in=-90,looseness=2] (taskMSymb);
        \draw[->]
        (ivarAtCodeP) to [out=-90,in=90,looseness=1]
        (ivarLocationSymb);
        \draw[->] (grefCodeP) to
        [out=-90,in=90,looseness=1] (ivarSlotSymb);
\end{tikzpicture}

\caption{Implementation of State for Supervised Sparked Futures}
\label{lst:local-future-state}
\end{figure}
#+END_LATEX

#+BEGIN_LATEX
\begin{Code}
\begin{haskellcode}{Modifying Empty IVar Location}{lst:modifying-ivar-location}
-- | wrapper for 'GRef a' from Figure @\ref{lst:local-future-state}@, uniquely identifying an IVar.
data TaskRef = TaskRef { slotT :: !Integer, atT :: !NodeId } @\label{code:task-ref}@

-- | query location of task in response to REQ message.
--   return 'Just CurrentLocation' if IVar is empty, else 'Nothing'.
locationOfTask :: TaskRef -> IO (Maybe CurrentLocation) @\label{code:locationOfTask}@

-- | set task to be InTransition before sending an AUTH message to a victim.
taskInTransition :: TaskRef -> NodeId -> NodeId -> IO () @\label{code:taskInTransition}@

-- | set task to be OnNode when ACK received by thief.
taskOnNode       :: TaskRef -> NodeId -> IO () @\label{code:taskOnNode}@

-- | scans through local registry, identifying all at-risk IVars.
--   that is: all IVars whose corresponding spark or thread may be
--   been lost with the failure of the specified node.
vulnerableEmptyFutures :: NodeId -> IO [IVar a] @\label{code:vulnerableEmptyFutures}@
\end{haskellcode}
\end{Code}
#+END_LATEX

The definition of task references and 4 location tracking functions on
\texttt{IVar}s are shown in Listing
\ref{lst:modifying-ivar-location}. Task references of type ~TaskRef~
on line \ref{code:task-ref} are used by the scheduler to inspect
location information of a spark to respond to ~REQ~ messages, and
modify the location tracking state in response to ~AUTH~ and ~ACK~
messages (Section [[Work Stealing Protocol]]). The ~locationOfTask~ on
line \ref{code:locationOfTask} is used to lookup whether the
corresponding supervised spark is ~OnNode~ or ~InTransition~, to
determine a response to ~REQ~ (Algorithm \ref{alg:handleReq} in
Section [[Fault Tolerant Scheduling Algorithm]]). If the corresponding
task for a supervised future is a thread, its location will always be
~OnNode~, as threads cannot migrate. If the location state for a
supervised spark is ~OnNode~, then ~taskInTransition~ on line
\ref{code:taskInTransition} is used to modify the location state,
before an ~AUTH~ message is sent to the victim. If the location state
for a supervised spark is ~InTransition~, a ~DENIED~ message is sent
to the victim, and the location state in the ~IVar~ is not
modified. When an ~ACK~ is received from a thief, the location state
for the remote spark is modified within the ~IVar~ with the
~taskOnNode~ function on line \ref{code:taskOnNode}. Lastly, the
~vulnerableEmptyFutures~ on line \ref{code:vulnerableEmptyFutures} is
used when a node receives a ~DEADNODE~ message propagated from the
fault detecting transport layer.

#+BEGIN_LATEX
\begin{Code}
\begin{haskellcode}{Par Computation That Modifies Local Registry on Node A}{lst:registry-modification-example-mixed}
-- | Par computation that generates registry entries in Table @\ref{tab:registry-snapshot}@
foo :: Int -> Par Integer
foo x = do
    ivar1 <- supervisedSpawn   $(mkClosure [| f x |])
    ivar2 <- supervisedSpawnAt $(mkClosure [| g x |]) nodeD
    ivar3 <- spawn             $(mkClosure [| h x |])
    ivar4 <- supervisedSpawn   $(mkClosure [| k x |])
    ivar5 <- supervisedSpawn   $(mkClosure [| m x |])
    ivar6 <- spawnAt           $(mkClosure [| p x |]) nodeF
    ivar7 <- supervisedSpawn   $(mkClosure [| q x |])
    ivar8 <- supervisedSpawn   $(mkClosure [| r x |])
    ivar9 <- spawnAt           $(mkClosure [| s x |]) nodeB
    x <- get ivar1
    y <- get ivar2
    {- omitted -}
\end{haskellcode}
\end{Code}
#+END_LATEX


#+BEGIN_LATEX
\begin{table}
\begin{center}
{\scriptsize
\begin{tabular}{|l||c|c||c|c|c|c||c|}
\hline 
\multirow{2}{*}{Function Call} & \multicolumn{2}{c||}{TaskRef} & \multicolumn{4}{c||}{SupervisedFutureState} & \multirow{2}{*}{Vulnerable} \\ \cline{2-7} & at & slot & task & scheduling & replica & location & \\
\hline
\hline
$supervisedSpawn\;(f\,x)$ & A & 1 & $f\,x$ & \texttt{Sparked}~ & 2 & \texttt{OnNode B} & No
\\
\hline
$supervisedSpawnAt\; (g\,x)\, D$ & A & 2 & $g\,x$ & \texttt{Pushed} & 1 & \texttt{OnNode D} & Yes \\
\hline
$spawn\; (h\,x)$ & A & 3 & \multicolumn{4}{c||}{\texttt{Nothing}} & ? \\
\hline
$supervisedSpawn\; (k\,x)$ & A & 4 & $k\,x$ & \texttt{Sparked} & 3 & \texttt{InTransition D C} & Yes \\
\hline
$supervisedSpawn\; (m\,x)$ & A & 5 & $m\,x$ & \texttt{Sparked} & 1 & \texttt{InTransition G F} & No
\\
\hline
$spawnAt\; (p\,x)\, F$ & A & 6 & \multicolumn{4}{c||}{\texttt{Nothing}} & ? \\
\hline
$supervisedSpawn\; (q\,x)$ & A & 7 & \multicolumn{4}{c||}{\texttt{Full 394}} & No \\
\hline
$supervisedSpawn\; (r\,x)$ & A & 8 & $r\,x$ & \texttt{Sparked} & 1 & \texttt{OnNode A} & No \\
\hline
$spawnAt\; (s\,x)\, B$ & A & 9 & \multicolumn{4}{c||}{\texttt{Full 68}} & No \\
\hline
\end{tabular}
}
\caption{Using Registry on Node A To Recover Tasks When Failure of Node D Detected}
\label{tab:registry-snapshot}
\end{center}
\end{table}
#+END_LATEX

An example of a local registry snapshot state is in Table
\ref{tab:registry-snapshot}. There have been 9 function calls on node
A, as shown in Listing
\ref{lst:registry-modification-example-mixed}. Of these, six are calls
to the fault tolerant primitives. The 3rd is a non-fault tolerant
~spawn~ call, and the 6th and 9th are non-fault tolerant ~spawnAt~
calls. When the failure of node D is detected, the registry is used to
identify vulnerable remote tasks. In this case, tasks for futures
~A:2~ and ~A:4~ need recovering. Supervised futures ~A:1~, ~A:5~ and
~A:8~ are not affected as their corresponding task is not at node D,
or in transition to or from node D. Futures ~A:7~ and ~A:9~ are not
affected because they are already full with values 394 and 68
respectively.

The safety of tasks for futures ~A:3~ and ~A:6~ cannot be determined,
due to the use of non-fault tolerant primitives. The HdpH-RS RTS may
deadlock if two conditions are met. First, that either of the tasks
corresponding to futures ~A:3~ or ~A:6~ are lost with the loss of node
D. Second, that the values of futures ~A:3~ or ~A:6~ are needed by the
program i.e. are waited for with a blocking ~get~ operation. The MPI
philosophy would be to terminate the entire program if the safety of
all registered futures cannot be guaranteed. The HdpH-RS RTS does not
take such brutal action in the case of this uncertainty, which raises
the possibility of deadlock. One advantage of using the fault tolerant
algorithmic skeletons in Section [[Fault Tolerant Parallel Skeletons]] is
that accidental use of ~spawn~ or ~spawnAt~ is not possible when fault
tolerant execution is the intention.

**** Supervised Spark State

The definition of a task is shown in Listing
\ref{lst:remote-tasks}. Only sparks created with ~supervisedSpawn~ are
defined as \texttt{Left~(Closure~(SupervisedSpark))}.  Tasks created
with ~spawn~, ~spawnAt~ or ~supervisedSpawnAt~ are all defined as
\texttt{Closure~(Par~())}.

#+BEGIN_LATEX
\begin{Code}
\begin{haskellcode}{Definition of a Task}{lst:remote-tasks}
type Task = Either (Closure SupervisedSpark) (Closure (Par ()))
\end{haskellcode}
\end{Code}
#+END_LATEX

The definition of ~SupervisedSpark~ is in Listing
\ref{lst:remote-task-state}. Closures of this structure migrate
between nodes, with the actual task expression within it on line
\ref{code:remote-clo}. It also has a reference to the ~IVar~ that will
be filled with the value of evaluating the task as ~remoteRef~ on line
\ref{code:remote-remoteRef}. Finally, the replication number for this
task is ~thisReplica~ on line \ref{code:remote-thisReplica}. This
value is included in ~REQ~ and ~ACK~ message sent to a spark's
supervisor, to ensure location tracking is not invalidated.

#+BEGIN_LATEX
\begin{Code}
\begin{haskellcode}{Remote Representation of Supervised Future Task}{lst:remote-task-state}
-- | Remote representation of supervised future task
data SupervisedSpark =
  SupervisedSpark { clo         :: Closure (Par ()) @\label{code:remote-clo}@
                  , remoteRef   :: TaskRef @\label{code:remote-remoteRef}@
                  , thisReplica :: Int } @\label{code:remote-thisReplica}@
\end{haskellcode}
\end{Code}
#+END_LATEX

*** Guard Posts

Each node has a guard post. A guard post serve two purposes: first, to
suspend a spark until authorisation of its migration is granted;
second, to discard obsolete sparks. For each spark that enters a guard
post, an authorisation request is made for its migration with ~REQ~
(Section [[Work Stealing Protocol]]). One check that is carried out by the
supervisor is that the spark is tagged with the highest replication
number of its corresponding future (Section [[Duplicate Sparks]]). This
authorisation check is shown in Algorithms \ref{alg:handleAuth},
\ref{alg:handleDenied} and \ref{alg:handleObsolete} in Section [[Fault
Tolerant Scheduling Algorithm]]. If it is authorised, the spark is
scheduled to the thief. If not, it is pushed back into sparkpool local
to the guard post. It is simply discarded if identified as an
/obsolete/ replica.

The guard post data structure is in Listing
\ref{lst:hdphrs-guard-post}. A ~GuardPost~ on line
\ref{code:guardPost} may be occupied with a guarded spark, or is empty
i.e. a ~Nothing~ value. It records the thief in ~destinedFor~ (line
\ref{code:destinedFor}). An ~AUTH~ message includes the destination of
the spark. This is checked against ~destinedFor~ as a sanity check. If
they do not match, an error is thrown.

#+BEGIN_LATEX
\begin{Code}
\begin{haskellcode}{GuardPost Implementation}{lst:hdphrs-guard-post}
type GuardPost = Maybe OccupiedGuardPost @\label{code:guardPost}@
data OccupiedGuardPost =
  OccupiedGuardPost { guardedSpark :: Closure SupervisedTask
                    , destinedFor  :: NodeId } @\label{code:destinedFor}@
\end{haskellcode}
\end{Code}
#+END_LATEX

** HdpH-RS Primitives

Using ~spawn~ and ~spawnAt~ invokes the fault oblivious HdpH
scheduler. The fault tolerant primitives ~supervisedSpawn~ and
~supervisedSpawnAt~ invoke the fault tolerant scheduler (Section
[[Designing a Fault Tolerant Scheduler]]). They match the
API of ~spawn~ and ~spawnAt~, allowing programmers to trivially opt in
(or out) of fault tolerant scheduling. The original HdpH primitives
~spark~ and ~pushTo~ \cite{Maier_Trinder_IFL2011} are demoted to
internal scheduling functions in the implementation of the spawn
family of primitives.

#+BEGIN_LATEX
\begin{Code}
\begin{haskellcode}{Creation of Futures and Supervised Futures}{lst:mkSpawnClo}
-- | Creation of non-fault tolerant futures and tasks.
--   Used by 'spawn' and 'spawnAt'.
mkSpawnedClo :: Closure (Par (Closure a)) -- task from user application @\label{code:mkSpawnedClo}@
             -> Par (Closure (Par ()),    -- task to be scheduled
                     IVar (Closure a),    -- IVar to be filled by evaluating closure
                     GIVar (Closure a))   -- handle to identify IVar & scheduled task
mkSpawnedClo clo = do
  v <- new @\label{code:mkSpawnedClo-new}@
  gv <- glob v @\label{code:mkSpawnedClo-glob}@
  let clo' = $(mkClosure [| spawn_abs (clo, gv) |]) @\label{code:mkSpawnedClo-mkClosure}@
  return (clo',v,gv) @\label{code:mkSpawnedClo-return}@

-- | Executed by the node that converts task to a thread.
spawn_abs :: (Closure (Par (Closure a)), GIVar (Closure a)) -> Par () @\label{code:spawn-abs}@
spawn_abs (clo, gv) = unClosure clo >>= rput gv

-- | Creation of fault tolerant supervised futures and tasks.
--   Used by 'supervisedSpawn' and 'supervisedSpawnAt'.
mkSupervisedSpawnedClo
             :: Closure (Par (Closure a)) -- task from user application
             -> Scheduling      -- Sparked or Pushed
             -> CurrentLocation -- (OnNode here) if sparked, (OnNode target) if pushed
             -> Par (Closure (Par ()),  -- task to be scheduled
                     IVar (Closure a),  -- IVar to be filled by evaluating closure
                     GIVar (Closure a)) -- handle to identify IVar & scheduled task
mkSupervisedSpawnedClo clo howScheduled currentLocation = do
  v <- new
  gv <- glob v  
  let clo' = $(mkClosure [| spawn_abs (clo, gv) |])
  v' <- newSupervisedIVar clo'          -- create supervised IVar
  io $ superviseIVar v' (slotOf gv) -- insert supervised IVar in place of placeholder v
  return (clo',v',gv)
 where
    newSupervisedIVar :: Closure (Par ()) -> IO (IVar a) @\label{code:mkSupervisedSpawnedClo-newSupervisedIVar}@
    newSupervisedIVar clo' =
      let localSt = SupervisedFutureState
            { task = clo'
            , scheduling = howScheduled
            , location = currentLocation
            , newestReplica = 0 }
      io $ newIORef $ Empty { blockedThreads = [] , taskLocalState = Just localSt }
\end{haskellcode}
\end{Code}
#+END_LATEX

With the introduction of the spawn family of primitives, programmers
do not create and write to \texttt{IVar}s directly. Listing
\ref{lst:mkSpawnClo} show the implementation of future creation. The
creation of supervised futures involves 2 steps. For non-supervised
futures, there is only 1. Non-supervised future creation is straight
forward, and is done using ~mkSpawnClo~ on line
\ref{code:mkSpawnedClo}. A new ~IVar~ is created on line
\ref{code:mkSpawnedClo-new}, and globalised on line
\ref{code:mkSpawnedClo-glob}. A task is created on line
\ref{code:mkSpawnedClo-mkClosure}, using ~spawn_abs~ function on line
\ref{code:spawn-abs}. It applies the value of evaluating the
user-specified task expression to an ~rput~ call on the globalised
~IVar~. This task and the empty ~IVar~ are returned to the function
caller, either ~spawn~, ~spawnAt~ or ~supervisedSpawnAt~. The ~IVar~
is then returned for the user to perform a blocking ~get~ operation.

The ~mkSupervisedSpawnClo~ on line \ref{code:mkSpawnedClo} performs
two steps. First, it creates and globalises an ~IVar~ as before. The
~glob~ operation reserves a placeholder in the registry, to insert the
supervised ~IVar~ when it is constructed. The corresponding task is
then constructed. It takes a user expression, and writes the value to
globalised ~IVar~ with ~rput~. Lastly, the supervised ~IVar~ is
constructed as a ~SupervisedFutureState~ on line
\ref{code:mkSupervisedSpawnedClo-newSupervisedIVar}. It contains the
task, the scheduling choice (either ~Sparked~ or ~Threaded~), a
replica value of 0, and a location tracking state of ~InTransition~ if
the task was sparked with ~supervisedSpawn~, or ~OnNode~ if it was
eagerly placed with ~supervisedSpawnAt~. This empty supervised future
is inserted in to the registry at the reserved index location.

#+BEGIN_LATEX
\begin{Code}
\begin{haskellcode}{Implementation of the Spawn Family}{lst:spawn-family-implementation}
spawn :: Closure (Par (Closure a)) -> Par (IVar (Closure a)) @\label{code:spawn-impl}@
spawn clo = do
  (clo',v,_) <- mkSpawnedClo clo
  spark clo' -- sparks (Right clo') @\label{code:spark-spawn-impl}@
  return v

supervisedSpawn :: Closure (Par (Closure a)) -> Par (IVar (Closure a)) @\label{code:supervisedSpawn-impl}@
supervisedSpawn clo = do
  here <- myNode
  (clo',v,gv) <- mkSupervisedSpawnedClo clo Sparked (OnNode here)
  let supervisedTask = mkSupervisedSpark clo' gv
  sparkSupervised supervisedTask -- sparks (Left supervisedTask) @\label{code:spark-supervised-impl}@
  return v
 where 
   mkSupervisedSpark :: Closure (Par ()) -> GIVar a -> Closure SupervisedSpark @\label{code:mksupspark-impl}@
   mkSupervisedSpark closure (GIVar gv) = toClosure
     SupervisedSpark { clo = closure , remoteRef = taskHandle gv , thisReplica = 0 }

spawnAt :: Closure (Par (Closure a)) -> NodeId -> Par (IVar (Closure a)) @\label{code:spawnAt-impl}@
spawnAt clo target = do 
  (clo',v,_) <- mkSpawnedClo clo
  pushTo clo' target -- pushes (Right clo') to target @\label{code:pushto-spawnat-impl}@
  return v

supervisedSpawnAt :: Closure (Par (Closure a)) -> NodeId -> Par (IVar (Closure a)) @\label{code:supervisedSpawnAt-impl}@
supervisedSpawnAt clo target = do
  (clo',v,_) <- mkSupervisedSpawnedClo clo Pushed (OnNode target)
  pushTo clo' target -- pushes (Right clo') to target @\label{code:pushto-supervisedspawnat-impl}@
  return v
\end{haskellcode}
\end{Code}
#+END_LATEX

The implementation of the spawn primitives are shown in Listing
\ref{lst:spawn-family-implementation}. The implementations of ~spawn~
(line \ref{code:spawn-impl}) uses ~spark~ internally on line
\ref{code:spark-spawn-impl} to add the task to the local
sparkpool. The implementation of ~spawnAt~ (line
\ref{code:spawnAt-impl}) and ~supervisedSpawnAt~ (line
\ref{code:supervisedSpawnAt-impl}) uses ~pushTo~ internally, to push
the task to the target node. In these 3 cases, the
scheduled expression is \texttt{Right~(Closure~(Par~()))} with either
~spark~ or ~pushTo~ on lines \ref{code:spark-spawn-impl},
\ref{code:pushto-spawnat-impl} and
\ref{code:pushto-supervisedspawnat-impl}.

The implementation of ~supervisedSpawn~ (line
\ref{code:supervisedSpawn-impl}) uses ~mkSupervisedSpark~ on line
\ref{code:mksupspark-impl} to add supervision state in to a
~SupervisedSpark~ structure. The scheduled task is
\texttt{Left~(Closure~SupervisedSpark)} with ~sparkSupervised~ on
line \ref{code:spark-supervised-impl}.

** Recovering Supervised Sparks and Threads

When a supervisor receives a ~DEADNODE~ message indicating a node
failure (Section [[Detecting Node Failure]]), it may replicate tasks if
their liveness is at risk. This is decided by Algorithm
\ref{alg:handleDeadnode} in Section [[Fault Tolerant Scheduling
Algorithm]], and implemented as ~vulnerableEmptyFutures~ in Listing
\ref{lst:modifying-ivar-location}.  It uses ~replicateSpark~ and
~replicateThread~ in Listing \ref{lst:task-replication-types}, the
implementations for which are in Appendix [[Replicating Sparks and
Threads]]. Both return a ~Maybe~ type, due to a potential race
condition whereby another local scheduler or node writes a value to
the ~IVar~ during the local recovery operation. If the ~IVar~ becomes
full, then a ~Nothing~ value is returned indicating a full ~IVar~ and
no recovery action needs taking.

#+BEGIN_LATEX
\begin{Code}
\begin{haskellcode}{Replicating Sparks \& Threads in Presence of Failure}{lst:task-replication-types}
replicateSpark  :: IVar a -> IO (Maybe (SupervisedSpark m))
replicateThread :: IVar a -> IO (Maybe (Closure (Par ()))
\end{haskellcode}
\end{Code}
#+END_LATEX

1. The ~replicateSpark~ and ~replicateThread~ functions both takes an
   ~IVar~ as an argument. The ~DEADNODE~ handler (Appendix [[Handling
   Dead Node Notifications]]) has determined the safety of the
   corresponding task to be at risk as a consequence of node
   failure.
2. It increments the replication number in the ~IVar~.
3. A new replica is returned, and scheduled according the
   ~recover_spark~ and ~recover_thread~ transition rules in the
   operational semantics (Section [[Small Step Operational Semantics]]).
   1. If a supervised spark is being recovered, a
      ~SupervisedSpark~ is returned and added to the local
      sparkpool.
   2. If a thread is being recovered, a \texttt{Closure~(Par~())} is
      returned, unpacked with ~unClosure~, and added to a local
      threadpool.

** HdpH-RS Node State

Node state in encapsulated in a monad stack in HdpH-RS, shown in Table
\ref{tab:hdphrs-monad-stack}. This monad stack is inherited from
HdpH. This Section describes where the state within each monad has
been extended for fault tolerance. The ~CommM~, ~SparkM~ and ~ThreadM~
monads are all synonyms for the reader monad transformer ~ReaderT~
\cite{DBLP:conf/afp/Jones95}, encapsulating the mutable state within
each associated module. Whilst the Reader monad transformer provides a
read-only environment to the given monad, mutability in the HdpH-RS
monads is achieved with \texttt{IORef}s in this read-only
environment. This section describes the mutable state in each monad,
paying close attention to the implementation of fault tolerance in the
designs in Chapter [[Designing a Fault Tolerant Programming Language for
Distributed Memory Scheduling]].

#+BEGIN_LATEX
\begin{table}
\begin{center}
\begin{tabular}{|c|}
\hline
\textbf{ThreadM} \\
\textbf{SparkM} \\
\textbf{CommM} \\
\textbf{IO} \\
\hline
\end{tabular}
\caption{HdpH-RS Runtime System Monad Stack}
\label{tab:hdphrs-monad-stack}
\end{center}
\end{table}
#+END_LATEX

*** Communication State
  
The ~CommM~ monad sits on top of the ~IO~ monad. It encapsulates the
distributed virtual machine state, in Listing
\ref{lst:hdphrs-commM-monad}. The queue of messages received by an endpoint
is stored in ~s_msgQ~ on line \ref{code:s_msgQ}, implemented as a
channel of lazy bytestrings. The state of the distributed virtual
machine allows remote nodes to be removed when their failure is
identified. The nodes in the distributed virtual machine is stored in
a mutable field ~s_nodes_info~ on line \ref{code:s_nodes_info}. It is
modified in two circumstances.

1. When the HdpH-RS virtual machine is bootstrapped, node discovery
   is achieved with UDP multicast between all hosts in a
   cluster. This list of nodes is written to the ~IORef~ in
   ~s_nodes_info~.
2. When a connection is lost with a node, the ~s_nodes_info~ ~IORef~ is
   atomically modified. The lost node is removed from the list
   ~s_allNodes~ (line \ref{code:s_allNodes}) and ~s_nodes~ decremented
   (line \ref{code:s_nodes}). The ~s_allNodes~ is used in the
   implementation of the ~allNodes~ function in the HdpH-RS API.


#+BEGIN_LATEX
\begin{Code}
\begin{haskellcode}{HdpH-RS CommM Monad State}{lst:hdphrs-commM-monad}
-- | CommM is a reader monad on top of the IO monad; mutable parts of the state
--   (namely the message queue) are implemented via mutable references.
type CommM = ReaderT State IO

data State =
  State { s_nodes_info :: IORef VMNodes, -- active nodes in VM @\label{code:s_nodes_info}@
          s_myNode     :: NodeId,        -- currently executing node @\label{code:s_myNode}@
          s_msgQ       :: MessageQ, }    -- queue holding received messages @\label{code:s_msgQ}@

data VMNodes =
  VMNodes { s_allNodes :: [NodeId], -- alive nodes in distributed virtual machine @\label{code:s_allNodes}@
            s_nodes    :: Int }     -- number of alive nodes @\label{code:s_nodes}@
\end{haskellcode}
\end{Code}
#+END_LATEX

*** Sparkpool State

#+CAPTION:    Sparkpool Deque, Implemented as \texttt{DequeIO~Task} in \texttt{s\_pool} (Listing \ref{lst:hdphrs-sparkM-monad})
#+LABEL:      fig:sparkpool-deque
#+ATTR_LaTeX: :width 80mm
[[./img/chp5/deques/sparkpool-deque.pdf]]

In many divide-and-conquer patterns, tasks generated early on are
bigger than later tasks. The early tasks are often sub-divided many
times before work is executed. To reduce communication costs, it is
desirable to send large tasks to remote nodes, so that remote nodes
can generate work themselves, rather than fishing frequently from other
nodes. The access to the sparkpool queue encourages large tasks to be
stolen and small tasks to stay local to the generating node, as shown
in Figure \ref{fig:sparkpool-deque}. New sparks generated with ~spawn~
or ~supervisedSpawn~ are pushed to the back of the queue. The node
hosting the sparkpool steals work from its own sparkpool from the back
of the queue. A busy node that receives a ~FISH~ message from a thief
pops a spark from the front of the sparkpool and replies with a
~SCHEDULE~ message containing that spark.

#+BEGIN_LATEX
\begin{Code}
\begin{haskellcode}{HdpH-RS SparkM Monad State}{lst:hdphrs-sparkM-monad}
-- |'SparkM ' is a reader monad sitting on top of the 'CommM' monad
type SparkM = ReaderT State CommM

-- spark pool state (mutable bits held in IORefs and the like)
data State =
  State { s_pool       :: DequeIO Task,          -- actual spark pool @\label{code:s_pool}@
          s_guard_post :: IORef GuardPost,  -- spark suspended for authorisation @\label{code:s_guard_post}@
          s_sparkOrig  :: IORef (Maybe NodeId),  -- origin of most recent spark recvd @\label{code:s_sparkOrig}@
          s_fishing    :: IORef Bool,            -- True iff FISH outstanding @\label{code:s_fishing}@
          s_noWork     :: ActionServer }         -- for clearing "FISH outstndg" flag @\label{code:s_noWork}@
\end{haskellcode}
\end{Code}
#+END_LATEX

The ~SparkM~ monad sits on top of the ~CommM~ monad. It encapsulates
the sparkpool state, in Listing \ref{lst:hdphrs-sparkM-monad}. The
sparkpool itself is ~s_pool~ on line \ref{code:s_pool}. It is a
double-ended queue that contains tasks of type \texttt{Task}, holding
both supervised and unsupervised sparks. The guard post ~s_guard_post~
on line \ref{code:s_guard_post} is either empty or holds one
supervised spark that is held for an authorised migration. The fishing
protocol is optimised to fish from the node it has most recently
received a ~SCHEDULE~ message, in the hope it has more on offer. The
~s_sparkOrig~ field holds this information on line
\ref{code:s_sparkOrig}. The ~s_fishing~ boolean on line
\ref{code:s_fishing} is used to indicate whether a node is in the
process of a work stealing i.e. waiting for a reply to a ~FISH~
message. The ~s_noWork~ field corresponds to the
~waitingFishReplyFrom~ field in typedef ~WorkerNode~ in the Promela
model (Section [[Node State]]). It is reset when a ~NOWORK~ message is
received, allowing a node to fish for sparks once again.

*** Threadpool State

The ~ThreadM~ monad sits on top of the ~SparkM~ monad. It encapsulates
the threadpool state, in Listing \ref{lst:hdphrs-threadM-monad}. In
the benchmark executions in Chapter [[Fault Tolerant Programming &
Reliable Scheduling Evaluation]], the number of threadpools matches the
number of utilised cores per node. The state on line
\ref{code:threadm-state} is a list of threadpools. Each threadpool is
a doubled-ended queue. Access to the threadpool is controlled in the
same way as the sparkpool. Each scheduler accesses its own threadpool
from the back.  Schedulers are able to steal from the front of other
threadpools on the same shared-memory node, when their own threadpool
is empty.


#+BEGIN_LATEX
\begin{Code}
\begin{haskellcode}{HdpH-RS ThreadM Monad State}{lst:hdphrs-threadM-monad}
-- |'ThreadM' is a reader monad sitting on top of the 'SparkM' monad
type ThreadM = ReaderT State SparkM

-- |thread pool state (mutable bits held in DequeIO)
type State = [(Int, DequeIO Thread)]  -- list of actual thread pools, @\label{code:threadm-state}@
\end{haskellcode}
\end{Code}
#+END_LATEX


** Fault Detecting Communications Layer

*** Distributed Virtual Machine

The mechanism for peer discovery is different for each distributed
environment a user is working with. Using a cluster with MPI, a
program is /told/ about its peers. Using other transports like TCP, a
program must /discover/ its peers. In the Cloud Computing setting, a
program may /start/ other peers by instantiating new virtual
machines. The HdpH-RS approach is peer /discovery/ with UDP.

Programs that use MPI have access to API calls to obtain a list of
peers. The ~MPI_Comm_size(..)~ call looks up all MPI ranks, and
~MPI_Comm_rank(..)~ returns the rank of the calling process. The first
HdpH release \cite{Maier_Trinder_IFL2011} used an MPI backend, and
used these MPI calls to obtain the list of peers. Section [[Fault
Tolerant MPI]] describes the fault tolerance limitations of MPI --- any
fault will typically bring down the entire communicator, making this
an unsuitable backend for HdpH-RS. Socket based transports are more
suitable. HdpH-RS uses a TCP-based transport layer.

One drawback of moving HdpH-RS away from MPI is that an additional
node discovery step must be taken. The HdpH-RS Comm module uses UDP
multicast for node discovery. Once the distributed virtual machine is
constructed, it is stored as mutable state on each node (Section
[[HdpH-RS Node State]]) and the scheduler is started. If the failure of remote nodes
are detected, they are removed from the peers list on each node. The
distributed virtual machine software stack is shown in Figure
\ref{fig:hdphrs-vm-layers}.


#+CAPTION:    HdpH-RS Distributed Virtual Machine Software Stack
#+LABEL:      fig:hdphrs-vm-layers
#+ATTR_LaTeX: :width 130mm
[[./img/chp5/transport-layers/transport-layers-hdphrs.pdf]]


*** Message Passing API

The ~network-transport-tcp~ library \cite{network-transport-tcp} is
used in the HdpH-RS Comm module for sending and receiving messages. It
is also used for detecting lost connections to propagate ~DEADNODE~
messages to the ~Scheduler~ and ~Sparkpool~ modules.

The Comm module additionally provides a simple virtual machine API in
Listing \ref{lst:hdphrs-comm-api}.  It exposes ~myNode~ on line
\ref{code:myNode}, which is similar to ~MPI_Comm_rank(..)~, and
~allNodes~ on line \ref{code:allNodes} which is analogous to MPI's
~MPI_Comm_size(..)~.


#+BEGIN_LATEX
\begin{Code}
\begin{haskellcode}{HdpH-RS Comm module API}{lst:hdphrs-comm-api}
-- The currently executing node.
myNode   :: CommM NodeId @\label{code:myNode}@

-- List of all nodes in the virtual machine.
allNodes :: CommM [NodeId] @\label{code:allNodes}@

-- True iff the currently executing node is the main node.
isMain   :: CommM Bool

-- |Send a HdpH-RS payload message.
send     :: NodeId -> Message -> CommM (Either (NT.TransportError NT.SendErrorCode) ()) @\label{code:hdphrs-send}@

-- | Receive a HdpH-RS payload message.
receive  :: CommM Message @\label{code:hdphrs-receive}@
\end{haskellcode}
\end{Code}
#+END_LATEX

The HdpH-RS ~send~ and ~receive~ functions on lines
\ref{code:hdphrs-send} and \ref{code:hdphrs-receive} of Listing
\ref{lst:hdphrs-comm-api} are lightweight wrappers over the
corresponding functions in the ~network-transport~ API which is shown
in Listing \ref{lst:nt-api}. In this API, the ~receive~ function
returns an ~Event~ (line \ref{code:Event}). An event may be a normal
payload message with the ~Received~ constructor on line
\ref{code:Received}. These are normal HdpH-RS messages from Section
[[RTS Messages]]. The ~EventConnectionLost~ constructor on line
\ref{code:EventConnectionLost} is translated to a ~DEADNODE~ message
in HdpH-RS. The \texttt{EventErrorCode}s are documented in Appendix
[[Network Transport Event Error Codes]].

#+BEGIN_LATEX
\begin{Code}
\begin{haskellcode}{Network.Transport API}{lst:nt-api}
-- | send a message on a connection.
send :: Connection -> [ByteString] -> IO (Either (TransportError SendErrorCode) ())

-- | endpoints have a single shared receive queue.
receive :: EndPoint -> IO Event

-- | Event on an endpoint.
data Event = @\label{code:Event}@ 
        Received !ConnectionId [ByteString] @\label{code:Received}@ 
      | @\textcolor{light-gray}{ConnectionClosed !ConnectionId}@
      | @\textcolor{light-gray}{ConnectionOpened !ConnectionId Reliability EndPointAddress}@
      | @\textcolor{light-gray}{ReceivedMulticast MulticastAddress [ByteString]}@
      | @\textcolor{light-gray}{EndPointClosed}@
      | ErrorEvent (TransportError EventErrorCode)

-- | Error codes used when reporting errors to endpoints (through receive)
data EventErrorCode = @\textcolor{light-gray}{EventEndPointFailed}@
                    | @\textcolor{light-gray}{EventTransportFailed}@
                    | EventConnectionLost EndPointAddress @\label{code:EventConnectionLost}@
\end{haskellcode}
\end{Code}
#+END_LATEX

*** RTS Messages

The ~receive~ function in the HdpH-RS Comm module returns HdpH-RS
messages in Listing \ref{lst:hdphrs-scheduling-messages}. These
messages were first introduced and described in Section [[Designing a
Fault Tolerant Scheduler]].

The ~PUSH~ message on line \ref{code:PushMsg} is used to eagerly
schedule tasks with ~spawnAt~ and ~supervisedSpawnAt~, and also for
~rput~ calls to transmit the value into an ~IVar~ future. The
following messages are used internally by the ~Scheduler~ and
~Sparkpool~ modules: ~FISH~, ~SCHEDULE~, ~NOWORK~, ~REQ~, ~AUTH~,
~DENIED~, ~OBSOLETE~ and ~ACK~. Finally, the ~DEADNODE~ message is
generated by the ~Comm~ module when node failure is detected.

#+BEGIN_LATEX
\begin{Code}
\begin{haskellcode}{Scheduling Messages in HdpH-RS}{lst:hdphrs-scheduling-messages}
data Msg = PUSH        -- eagerly pushing work @\label{code:PushMsg}@
             Task      -- task
         | FISH        -- looking for work
             !NodeId   -- thief sending the FISH
         | SCHEDULE    -- reply to FISH sender (when there is work)
              Task     -- spark
             !NodeId   -- victim sending the SCHEDULE
         | NOWORK      -- reply to FISH sender (when there is no work)
         | REQ         -- request for a spark
             TaskRef   -- The globalised spark pointer
             !Int      -- replica number of task
             !NodeId   -- the victim it would send the SCHEDULE
             !NodeId   -- the thief that would receive SCHEDULE
         | AUTH
             !NodeId   -- thief to send SCHEDULE to
         | DENIED
             !NodeId   -- thief to return NOWORK to
         | ACK         -- notify supervisor that spark has been received
             TaskRef   -- The globalised spark pointer
             !Int      -- replica number of task
             !NodeId   -- thief that is sending ACK to supervisor
         | DEADNODE    -- a node has died
             !NodeId   -- which node has died
         | OBSOLETE    -- obsolete task copy (old replica number)
             !NodeId   -- thief waiting for guarded spark, to receive NOWORK
         | HEARTBEAT   -- keep-alive heartbeat message
\end{haskellcode}
\end{Code}
#+END_LATEX


*** Detecting Node Failure
**** HdpH-RS Error Events

When the network abstraction layer detects TCP connection loss, it
propagates this as a ~EventConnectionLost~ message in a ~ErrorEvent~
constructor to HdpH-RS. The implementation of the ~receive~ function
used by the scheduler is shown in Appendix [[Propagating Failures from
Transport Layer]]. It inspects each event received from the network
abstraction layer, and does the following:

1. If a bytestring is received in a ~Received~ message, then it is
   unpacked and returned to the HdpH-RS scheduler as a normal HdpH-RS
   message.
2. If an ~ErrorEvent~ is received, then the lost connection is
   inspected for 2 cases:
   1) *The connection with the root node is lost* In this case, the
      local node can play no further part in the current program
      execution. It terminates itself as a node instance on line
      \ref{code:nt-shutdown}. Connectivity with the root node may be
      lost either because the root node has failed, or through network
      partitioning (Section [[Simultaneous Failure]]).
   2) *The connection with a non-root node is lost* In this case, the
      remote node is removed from the local virtual machine. This is
      done by sending a ~DEADNODE~ message to the scheduler on line
      \ref{code:nt-deadnode}. the scheduler will remove the remote
      node from the distributed VM, and decrements the ~nodes~
      parameter, which is the sum of all nodes in the distributed VM.

**** Propagating Error Messages

Failure detection in HdpH-RS depends on the ~network-transport-tcp~
implementation \cite{network-transport-tcp}, and the failure detection
on TCP connections. There is a three-way handshake to control the
state of a TCP connection \cite{rfc793}. The TCP handshake happens at
the Operating System level, beneath the level of the
~network-transport-tcp~ Haskell library. A sequence flag ~SYN~ is used
to initialise a connection. A TCP connection termination sequence is
shown in Figure \ref{fig:tcp-closing-connection}. A finish flag ~FIN~
is used to cleanly terminate a connection and an acknowledge flag
~ACK~ is used to acknowledge received data
\cite{DBLP:conf/infocom/WangZS02}.

#+CAPTION:    TCP Handshake for Closing Connections
#+LABEL:      fig:tcp-closing-connection
#+ATTR_LaTeX: :width 60mm
[[./img/chp5/tcp-handshake/tcp-handshake.pdf]]

TCP is an idle protocol, so if neither side sends any data once a
connection has been established, then no packets are sent over the
connection \cite{tcp-half-open}. The act of receiving data is
completely passive in TCP, and an application that only reads from a
socket cannot detect a dropped connection. This scenario is called a
/half-open connection/ \cite{DBLP:conf/sp/SchubaKKSSZ97}. When a node
sends data to the other side it will receive an ~ACK~ if the
connection is still active, or an error if it is not. Broken
connections can therefore be detected with transmission attempts.

Half open TCP connections can be caused by Operating System processes
or nodes crashing. When a process is terminated abnormally, there
will not be the usual ~FIN~ message to terminate a connection. Whilst
an Operating System may send a ~FIN~ on behalf of the process, this is
Operating System dependent. If the failure is a hard node crash, there
will certainly be no notification sent to the other side that the
connection has been lost.

In order to discover lost TCP connections, two options are available
\cite{tcp-half-open}. The first is to add explicit keep-alive messages
to an application protocol. The second is to assume the worst, and
implement timers for /receiving/ messages on connections. If no
packets are received within the timeout period, a connection may be
regarded as lost.

As architectures scale to thousands of nodes, error propagation
through work stealing message transmissions cannot be relied upon. The
HdpH-RS keep-alive messages is a guarantee of periodic traffic between
nodes independent of work stealing messages, enabling nodes to
discover failure by discovering lost connections.

**** Keep-Alive Messages in HdpH-RS

Whilst the transmission of work stealing messages will probably
trigger timely TCP failures for smaller architectures, there is a high
failure detection latency in larger networks. This has an important
implication for performance in HdpH-RS. Take an example where node A
creates a supervised spark $spark_1$ and ~IVar~ $i_1$ with the
execution of ~supervisedSpawn~. Node B fishes $spark_1$, and later
suffers a power outage. Node A may not receive a TCP ~FIN~ message
from B due to the sudden failure. Node A does not send any work
stealing messages to B, but is waiting for the value of evaluating
$spark_1$ to be written to $i_1$. To ensure a more reliable failure
detection of B, node A needs some other message transmission mechanism
than just work stealing.

The keep-alive implementation in HdpH-RS is simple. A ~keepAliveFreq~
parameter has been added to the RTS configuration parameters, of the
scheduler. This flag is documented in Section [[Reliability and Fault
Injection Flags]], along with examples of using it. It is an Integer
value that allows the user to state $N$, the time in seconds between
each keep-alive. If $N > 0$ then a keep-alive server is enabled on
each node. This server is forked into a thread when the scheduler is
initialised. When the frequency delay expires, a node broadcasts a
keep-alive to every node it is currently connected to. As TCP failures
are detected on send attempts, the keep-alive is silently ignored on
the receiving end. The heartbeats in HdpH-RS is shown in Listing
\ref{lst:hdphrs-keepalive}. After $N$ seconds, a node broadcasts a
~HEARTBEAT~ to all other nodes (line \ref{code:send-heartbeat}). When
a node receives a ~HEARTBEAT~ message, it is silently ignored (line
\ref{code:recv-heartbeat}).

For small architectures, heartbeats are unlikely to be the trigger
that detects failure. On large architectures, work stealing messages
between any two nodes are less likely to be transmitted within the
keep-alive frequency, so the keep-alive messages are an important
mechanism for failure detection.

#+BEGIN_LATEX
\begin{Code}
\begin{haskellcode}{Sending \& Receiving Periodic ~HEARTBEAT~ Messages}{lst:hdphrs-keepalive}
-- | broadcasting periodic heartbeats.
keepAliveServer :: Int -> [NodeId] -> IO () @\label{code:send-heartbeat}@
keepAliveServer delaySeconds nodes = forever go
  where
    go = do
       threadDelay (delaySeconds * 1000000)
       mapM_ sendHeartBeat nodes
    sendHeartBeat node = void $ send node (encode HEARTBEAT)

-- | receive heartbeats & ignore them.
handleHEARTBEAT :: Msg RTS -> RTS () @\label{code:recv-heartbeat}@
handleHEARTBEAT HEARTBEAT = return ()
\end{haskellcode}
\end{Code}
#+END_LATEX

The main drawback to this failure detection strategy is the dependency
on connection oriented protocols like TCP. There are two main
weaknesses. First, the failure detection strategy of using
connection-oriented transmission attempts would not work for
connectionless protocols like UDP \cite{rfc768}. Second, the design
assumes a fully connected network. Every node has a persistent
connection with every other node. The scalability limitations of TCP
connections are well known \cite{DBLP:journals/sigops/HaRX08}.

- *File descriptors* Each node has a maximum number of file descriptors to
  facilitate concurrent connections. It is 1024 by default on
  Linux. It is specified in ~/proc/sys/fs/file-max~, though can easily
  be changed.
- *Socket buffers* Each TCP connection contain a socket receive and send buffer. A
  receive buffer may be in the $87kb$ region (in file
  ~/proc/sys/net/ipv4/tcp_rmem~) and write buffers in the $16kb$
  region (in file ~/proc/sys/net/ipv4/tcp_wmem~). An Operating System
  allocates memory for socket buffers, which limits the number of
  theoretically possible TCP connections.

The author is in discussion \cite{haskell-distributed-irc} with the
~network-transport~ authors on a generalised failure detection for
distributed Haskells. A failure detection mechanism has not yet been
added to the library. The proposed generalised strategy is to use
passive heartbeat timeouts. That is, every node broadcasts a timeout,
and expects a heartbeat message from all other nodes. If a heartbeat
is not received from a remote node within a given timeout threshold,
it is assumed to have failed. It is the opposite to HdpH-RS, which
detects failure actively on the heartbeat broadcaster, not passively
on the receiver.

The generalised fault detection design for distributed Haskells does
have some drawbacks. First, network congestion may delay message
delivery that results in false-positives in failure detection
\cite{DBLP:conf/infocom/ZhuangGSK05}. Second, the latency of failure
detection is higher. It is at least the size of the timeout window +
the delay between heartbeats (Section [[Fault Detectors]]).

** Comparison with Other Fault Tolerant Language Implementations

The implementation of fault tolerance in HdpH-RS shares many
techniques with other fault tolerant programming languages and
distributed schedulers. Replication (Section [[Fault Recovery]]) is the
key recovery mechanism in HdpH-RS. It is a tactic used in the
supervision behaviours of Erlang
\cite{DBLP:journals/cacm/Armstrong10}, and is ingrained in the Google
MapReduce model \cite{DBLP:journals/cacm/DeanG08}, such as in Hadoop
\cite{DBLP:books/daglib/0029284}.

*** Erlang

The supervision approach in Erlang is a strong influence on the
implementation of HdpH-RS. Below are four distinctions between the
performance of Erlang and HdpH in the absence of faults, and the
recovery in Erlang and HdpH-RS in the presence of faults.

*Handling failures* The failure recovery in HdpH-RS is akin to the
Erlang OTP supervision behaviours. The user does not need to handle
failures programmatically. However, when the ~monitor~ and ~link~
Erlang primitives are used directly, it is the responsibility of the
programmer to recover from faults.

*Recovering stateful computations* Erlang has better support for
recovering non-idempotent computations, using the restart strategies
and frequencies of the supervision behaviour in Erlang OTP. HdpH-RS
does not support the recovery of non-idempotent tasks, and is designed
for evaluating pure expressions that are of course idempotent.

*Load balancing* Support for load balancing is more powerful in
HdpH and HdpH-RS than in Erlang. A work stealing
mechanism in Erlang could be thought of as HdpH-RS without sparkpools
and only threadpools --- once an Erlang process is spawned, it cannot
migrate to another node. This makes Erlang less suitable than HdpH-RS
for some classes of computation, where parallelism is highly
irregular. Parallel symbolic computing is one example.

*Programming errors* Erlang is an untyped language, allowing many
programming errors to go unnoticed at compile time
\cite{DBLP:conf/sfp/TrinderPL00}. Such typing error can introduce
faults at runtime. A type system has been proposed
\cite{DBLP:conf/icfp/MarlowW97}, though is not often used. Only a
subset of the language is type-checkable, the major omission being the
lack of process types \cite{DBLP:conf/hopl/Armstrong07}. In contrast,
the Haskell host language for HdpH-RS is strongly typed, eliminating a
large class of software bugs at compile time.

*** Hadoop

Hadoop is a popular MapReduce implementation. MapReduce is a
programming model and an associated implementation for processing and
generating large data sets \cite{DBLP:journals/cacm/DeanG08}. If
Hadoop is used using the MapReduce interface directly, a programmer
only defines a /map/ and a /reduce/ function, and these are automatically
parallelisable.

To relate the programming model to HdpH-RS, the MapReduce model is in
fact an algorithmic skeleton. It has been implemented in HdpH-RS as
2 of 10 parallel skeletons. First an implicit ~parMapReduceRangeThresh~
skeleton, and second an explicit ~pushMapReduceRangeThresh~ skeleton.  They
are used in the implementation of the Mandelbrot benchmark in Section
[[Benchmarks]]. The failure recovery in Hadoop is task replication, the
same as Erlang and HdpH-RS. The failure recovery in HdpH-RS can be
compared to Hadoop in three ways:

*Failure detection latency* Failure detection latency in Hadoop is 10
minutes by default in order to tolerate non-responsiveness and network
congestion. A slave sends a heartbeat to the master node every 3
seconds. If the master does not receive a heartbeat from a given slave
within a 10 minute window, failure is assumed
\cite{hadoop-failure-latency}. In contrast, the failure detection
latency in HdpH-RS is a maximum of 5 seconds by default, and can be
modified by the user.

*Supervision Bottleneck* A master node supervises the health of all
slave nodes in Hadoop. In contrast, the HdpH-RS architecture supports
hierarchically nested supervisors. This is achieved with either
recursive ~supervisedSpawn~ or ~supervisedSpawnAt~ calls, or by using
the MapReduce or divide-and-conquer skeletons. This means that
supervision and task replication responsibilities are distributed
across the entire architecture in HdpH-RS, and not centrally
coordinated like Hadoop.

*Unnecessary task replication* The output of map tasks are stored to
disk locally in Hadoop. In the presence of failure, completed map
tasks are re-scheduled, due to the loss of their results. This is in
contrast to HdpH-RS, where the resulting values of evaluating task
expressions is transmitted with ~rput~ as soon as they are calculated.
 
*** GdH Fault Tolerance Design

The design of a new RTS level of fault tolerance for Glasgow
distributed Haskell (GdH) \cite{Trinder00runtimesystem} has been
proposed, but not implemented. The design augments the GdH RTS with
error detection and error recovery.

The failure detection is similar to the HdpH-RS detection. It is
designed to rely on the eager node failure detection from its PVM
\cite{Beguelin:1991:UGP:898915} network layer. As in HdpH-RS,
intermittent node failure is managed by ignoring future message from
the faulty node. When the transport layer broadcasts the node failure,
future messages sent by a previously faulty node are discarded --- by
the scheduler in the case of GdH, and by ~network-transport-tcp~
\cite{network-transport-tcp} library in the case of HdpH-RS.

Simultaneous failure recovery in the GdH design is similar to the
HdpH-RS implementation. In both, one node is distinguished as the
root node, which starts and terminates the program. If a network is
split in to two or more parts, the partition containing the root node
will restart the pure computations from the lost partitions.

In order to replicate pure computations in the presence of failure,
both the GdH design and HdpH-RS store task copies on the creating
node. The store of these tasks is defined as a /backup heap/ in
GdH. The back-up task copies in HdpH-RS are stored within their
corresponding empty future.

The GdH fault tolerance design include some techniques borrowed from
Erlang, that have not been adopted in HdpH-RS. One example is
continued service. A computation cannot be restarted more than a
fixed number of times before raising an exception in the GdH design,
similar to the child process restart frequency in Erlang OTP. A
message is not re-transmitted more than a fixed number of times before
raising an exception, preventing livelock. Finally, nodes can be added
during the execution of programs, to replace failed nodes. None of
these 3 features have been implemented in HdpH-RS.

*** Fault Tolerant MPI Implementations

Most scientific applications are written in C with MPI
\cite{DBLP:journals/software/KarpB88}. The original MPI standards
specify very limited features related to reliability and fault
tolerance \cite{DBLP:journals/ijhpca/GroppL04}.  Based on early MPI
standards, an entire application is shutdown when one of the executing
processors experiences a failure. As discussed in Section [[Fault Tolerant
MPI]], MPI implementations that include fault tolerant take one of two
approaches. Either they hide faults from the user, or propagate them
for the user to recover from faults programmatically. Some fault
tolerant MPI implementations mask faults, but thereafter support a
reduced set of MPI operations. This masking of failures is the same
approach as HdpH-RS, with the difference that no programming
primitives are disabled by faults.

* Fault Tolerant Programming & Reliable Scheduling Evaluation
 
This chapter demonstrates how to program with reliable scheduling and
gives a performance evaluation of HdpH-RS. Section [[Programming With
HdpH-RS Fault Tolerance Primitives]] demonstrates how to write fault
tolerance programs with HdpH-RS. The parallel skeletons from HdpH have
been ported to HdpH-RS (Section [[Fault Tolerant Parallel Skeletons]]),
adding fault tolerance by implementing them using the
~supervisedSpawn~ and ~supervisedSpawnAt~ primitives. Section
[[Launching Distributed Programs]] describes how to launch HdpH-RS
programs on clustered architectures, and how to configure the HdpH-RS
RTS for fault tolerance and performance tuning. Section [[Benchmarks]]
describes the five benchmarks used to evaluate HdpH-RS. Section
[[Hardware Platforms]] describes the two architectures on which HdpH-RS
has been measured. One is a 32 node Beowulf cluster providing 224
cores. The other is HECToR, a high performance UK national compute
resource, and up to 1400 cores are used to measure the scalability of
HdpH-RS.

The supervision overheads of the fault tolerant work stealing protocol
(Section [[Designing a Fault Tolerant Scheduler]]) are
compared against the HdpH fault oblivious scheduling in Section
[[Performance With No Failure]], by measuring the fault tolerant and
non-fault tolerant version of three skeletons --- parallel-map,
divide-and-conquer and map-reduce. The benchmark executions
demonstrate the scalability of fault tolerance in HdpH-RS. For
example, the Summatory Liouville benchmark demonstrates HdpH-RS
speedup of 145 on 224 cores on Beowulf, and a speedup of 751 on 1400
cores on HECToR. The recovery overheads of detecting faults and
replicating tasks is measured in Section [[Performance With Recovery]] by
injecting two types of failure during execution using the HdpH-RS
scheduler. The first type simulates simultaneous node loss (Section
[[Simultaneous Multiple Failures]]), which can occur when networks are
partitioned. The second type uses a Chaos Monkey implementation in
HdpH-RS (Section [[Chaos Monkey]]), to simulate random failure. Unit tests
are used ensure that programs using the fault tolerant skeletons
terminate with the correct result in the presence of Chaos Monkey
failure injection. The runtime performance of HdpH-RS indicate that
lazy on-demand work stealing is more suitable when failure is the
common case, not the exception.

** Fault Tolerant Programming with HdpH-RS

This section shows how to program with HdpH-RS primitives and
skeletons. The use case is a parallel implementation of Euler's
totient function $\phi$ \cite{sum-euler}. It is an arithmetic function
that counts the totatives of $n$, i.e. the positive integers less than
or equal to $n$ that are relatively prime to $n$. Listing
\ref{lst:sumeuler-sequential} shows the implementation of $\phi$ and
the sequential sum of totients. It is parallelised in Section
[[Programming With HdpH-RS Fault Tolerance Primitives]] using the HdpH-RS
primitives directly, and in Section [[Programming With Fault Tolerant
Skeletons]] using the ~parMapSlicedFT~ skeleton.

#+BEGIN_LATEX
\begin{Code}
\begin{haskellcode}{Sequential Implementation of Sum Euler}{lst:sumeuler-sequential}
-- | Euler's totient function (for positive integers)
totient :: Int -> Integer
totient n = toInteger $ length $ filter (\ k -> gcd n k == 1) [1..n]

-- | sequential sum of totients
sum_totient :: [Int] -> Integer
sum_totient = sum . map totient
\end{haskellcode}
\end{Code}
#+END_LATEX

*** Programming With HdpH-RS Fault Tolerance Primitives

Using the HdpH-RS primitives directly for computing Sum Euler between
$0$ and $10000$ with a slice size of 100 is shown in Listing
\ref{lst:programming-with-primitives}. The ~dist_sum_totient_sliced~
function creates a sliced list of lists on line \ref{code:sliced_list} using
the lower and upper bounds and chunk size. For each element in the list, a
supervised spark and corresponding ~IVar~ (the supervised future) is created with
~supervisedSpawn~ on line \ref{code:supSpawn-sumeuler}. The results
are summed on line \ref{code:sum-euler}. The ~main~ function on line
\ref{code:sum-euler-main} prints the result $30397486$.

#+BEGIN_LATEX
\begin{Code}
\begin{haskellcode}{Programming with HdpH-RS Primitives Directly}{lst:programming-with-primitives}
dist_sum_totient_sliced :: Int -> Int -> Int -> Par Integer
dist_sum_totient_sliced lower upper chunksize = do
  sum <$> (mapM get_and_unClosure =<< (mapM spawn_sum_euler $ sliced_list)) @\label{code:sum-euler}@
     where
      sliced_list = slice slices [upper, upper - 1 @..@ lower] :: [[Int]] @\label{code:sliced_list}@

      get_and_unClosure :: IVar (Closure a) -> Par a                                                                                                               
      get_and_unClosure = return . unClosure <=< get

spawn_sum_euler :: [Int] -> Par (IVar (Closure Integer))
spawn_sum_euler xs = supervisedSpawn $(mkClosure [| spawn_sum_euler_abs (xs) |]) @\label{code:supSpawn-sumeuler}@

spawn_sum_euler_abs :: ([Int]) -> Par (Closure Integer)
spawn_sum_euler_abs (xs) = force (sum_totient xs) >>= return . toClosure

main = do @\label{code:sum-euler-main}@
  result <- runParIO conf (dist_sum_totient_sliced0 10000 100)
  print result -- "30397486"
\end{haskellcode}
\end{Code}
#+END_LATEX

*** Fault Tolerant Parallel Skeletons

Algorithmic skeletons abstract commonly-used patterns of parallel
computation, communication, and interaction
\cite{murray-cole-thesis}. The implementation of skeletons manage
logic, arithmetic and control flow operations, communication and data
exchange, task creation, and synchronisation. Skeletons provide a
top-down design approach, and are often compositional
\cite{DBLP:journals/spe/Gonzalez-VelezL10}.

The ~supervisedSpawn~ and ~supervisedSpawnAt~ primitives from Section
[[HdpH-RS Programming Primitives]] guarantee the evaluation of a single
task --- a supervised spark or thread corresponding to a supervised
future. Higher-level abstractions built on top of these primitives
guarantee the completion of a set of tasks of this type. These
abstractions hide lower level details by creating supervised sparks or
threads, and supervised futures (\texttt{IVar}s) dynamically.

Eight parallel skeletons from HdpH have been extended for fault
tolerance. HdpH-RS introduces four skeleton patterns to the HdpH
skeletons library:
~parMapForkM~, ~forkDivideAndConquer~, ~parMapReduceRangeThresh~ and
~pushMapReduceRangeThresh~. The first two abstract shared-memory parallel
patterns, and the latter two are extended for fault tolerance as
~parMapReduceRangeThreshFT~ and ~pushMapReduceRangeThreshFT~. The HdpH-RS
skeletons API is in Appendix [[HdpH-RS Skeleton API]]. Lazy work stealing
skeletons are replaced with supervised lazy work stealing versions by
using ~supervisedSpawn~. Fault tolerant skeletons that use eager task
placement are implemented with ~supervisedSpawnAt~.  As an example,
Listing \ref{lst:parMap-ft} shows the implementation of the fault
tolerant parallel-map skeleton ~parMapFT~. It uses a ~parClosureList~
strategy on line \ref{code:parClosureList} which uses the fault
tolerant HdpH-RS ~supervisedSpawn~ primitive within ~sparkClosure~ on
line \ref{code:sparkClosure}. There are variants of parallel-map
skeletons, for slicing and chunking input lists.

#+BEGIN_LATEX
\begin{Code}
\begin{haskellcode}{Fault Tolerant Parallel Map Using \texttt{supervisedSpawn} on Line \ref{code:sparkClosure}}{lst:parMap-ft}
parMapFT :: (ToClosure a) @\label{code:parMap}@
         => Closure (Strategy (Closure b))
         -> Closure (a -> b)
         -> [a]
         -> Par [b]
parMapFT clo_strat clo_f xs =
  do clo_ys <- map f clo_xs `using` parClosureList clo_strat
     return $ map unClosure clo_ys
       where f = apC clo_f
             clo_xs = map toClosure xs

parClosureList :: Closure (Strategy (Closure a)) -> Strategy [Closure a] @\label{code:parClosureList}@
parClosureList clo_strat xs = mapM (sparkClosure clo_strat) xs >>= mapM get

sparkClosure :: Closure (Strategy (Closure a)) -> ProtoStrategy (Closure a)
sparkClosure clo_strat clo =
  supervisedSpawn $(mkClosure [| sparkClosure_abs (clo, clo_strat) |]) @\label{code:sparkClosure}@

sparkClosure_abs :: (Closure a,Closure (Strategy (Closure a))) -> Par (Closure a)
sparkClosure_abs (clo, clo_strat) = (clo `using` unClosure clo_strat) >>= return
\end{haskellcode}
\end{Code}
#+END_LATEX

Divide-and-conquer is a more elaborate recursive parallel pattern.  A
fault tolerant ~parDivideAndConquerFT~ skeleton is shown in Listing
\ref{lst:parDivideAndConquer}. It is a skeleton that allows a
problem to be decomposed into sub-problems until they are sufficiently
small, and then reassembled with a combining function. Its use is
demonstrated with Queens in Section [[Runtime & Speed Up]]. The
implementation of Queens is in Appendix [[Queens]].

#+BEGIN_LATEX
\begin{Code}
\begin{haskellcode}{Divide \& Conquer HdpH-RS Skeleton}{lst:parDivideAndConquer}
parDivideAndConquerFT
    :: Closure (Closure a -> Bool)                     -- isTrivial
    -> Closure (Closure a -> [Closure a])              -- decomposeProblem
    -> Closure (Closure a -> [Closure b] -> Closure b) -- combineSolutions 
    -> Closure (Closure a -> Par (Closure b))          -- trivialAlgorithms
    -> Closure a                                       -- problem
    -> Par (Closure b)
\end{haskellcode}
\end{Code}
#+END_LATEX

MapReduce is another recursive pattern that decomposes large tasks in
to smaller tasks that can be evaluated in parallel. A fault tolerant
~parMapReduceRangeThreshFT~ skeleton is shown in Listing
\ref{lst:parMapReduceRangeThresh}. It is adapted from the monad-par
library \cite{monad-par-code}, extended for distributed-memory
scheduling with HdpH-RS. It takes a ~Integer~ threshold value,
and an inclusive ~Integer~ range over which to compute. A map
function is used to compute one result from an ~Integer~ input, and a reduce
function to combine the result of two map functions. Lastly, it takes an
initial value for the computation. Its use is demonstrated with
Mandelbrot in Section [[Runtime & Speed Up]]. The implementation of Mandelbrot is in
Appendix [[Mandelbrot]].

#+BEGIN_LATEX
\begin{Code}
\begin{haskellcode}{MapReduce HdpH-RS Skeleton}{lst:parMapReduceRangeThresh}
parMapReduceRangeThreshFT
  :: Closure Int                                         -- threshold
  -> Closure InclusiveRange                              -- range to calculate
  -> Closure (Closure Int -> Par (Closure a))            -- compute one result
  -> Closure (Closure a -> Closure a -> Par (Closure a)) -- reduce two results
  -> Closure a                                           -- initial value
  -> Par (Closure a)

data InclusiveRange = InclusiveRange Int Int
\end{haskellcode}
\end{Code}
#+END_LATEX

The APIs for the lazy scheduling skeletons (that use ~spawn~ and
~supervisedSpawn~ under the hood) are identical for fault tolerant and
non-fault tolerant execution. The type signatures for the eager
scheduling skeletons are different, with the omission of a list of
\texttt{NodeId}s in each fault tolerant case. The non-fault tolerant
skeletons assume no failure, and are provided a list of
\texttt{NodeId}s indicating the nodes that will be sent tasks
e.g. with ~pushDnCFT~. The HdpH-RS skeletons assume failure, and it
would not make sense to provide a list of \texttt{NodeId}s to a fault
tolerant skeleton, as this list may be invalid if some of the nodes in
the list fail during execution. Instead the skeleton collects the list
of nodes dynamically from the distributed VM, using the ~allNodes~
function from the ~Comm~ module (Section [[HdpH-RS Architecture]])
at the task decomposition phase. Failed nodes are removed from the
distributed VM state when their failure is detected, changing the
list of \texttt{NodeId}s returned from ~allNodes~.

*** Programming With Fault Tolerant Skeletons

Using the HdpH-RS parallel skeleton library to compute the same Sum
Euler computation is shown in Listing
\ref{lst:programming-with-skeletons}. It uses the ~parMapSlicedFT~
skeleton on line \ref{code:parMap-sumeuler}, where the slicing of the
input list and the application of ~spawn_sum_euler~ on every element
was previously explicit using the ~supervisedSpawn~ primitive in
Listing \ref{lst:programming-with-primitives}, this is now handled by
the skeleton. In consequence the skeleton code is smaller, 5 lines
rather than 11.

#+BEGIN_LATEX
\begin{Code}
\begin{haskellcode}{Programming with HdpH-RS Skeletons API}{lst:programming-with-skeletons}
slice_farm_sum_totient :: Int -> Int -> Int -> Par Integer
slice_farm_sum_totient lower upper slices =
  sum <$> parMapSlicedFT slices $(mkClosure [| totient |]) list  @\label{code:parMap-sumeuler}@
    where
      list = [upper, upper - 1 @..@ lower] :: [Int]

main = do
  result <- runParIO conf (slice_farm_sum_totient 0 10000 100)
  print result -- "30397486"
\end{haskellcode}
\end{Code}
#+END_LATEX


** Launching Distributed Programs

The MPI launcher ~mpiexec~ is used to deploy HdpH-RS program instances to each
node. It is useful for starting and terminating multiple process
instances on clusters. That is all MPI is used for in
HdpH-RS. It is not used as a communications layer (Section [[Fault
Detecting Communications Layer]]). Once ~mpiexec~ has deployed program
instances to each node, the HdpH-RS transport layer is used for
communication between nodes --- UDP for peer discovery, then TCP for
node to node communication.

#+BEGIN_LATEX
{\footnotesize
\begin{lstlisting}[captionpos=b,basicstyle=\ttfamily\footnotesize, caption=Executing HdpH-RS on Clusters. Described in Tables \ref{tab:mpiexec-args}\, \ref{tab:hdphrs-rts-opts} and \ref{tab:hdphrs-reliability-args}.,label=lst:mpiexec-command]
mpiexec --disable-auto-cleanup -machinefile <hosts> -N n <executable> \
   <HdpH-RS RTS opts> <reliability opts> <program params> +RTS -Nn
\end{lstlisting}
}
#+END_LATEX

An HdpH-RS program can be launched on a distributed environment with
the command in Listing \ref{lst:mpiexec-command}. The MPICHv2
\cite{DBLP:conf/sc/BouteillerCHKLM03} process launcher is used in the
experimentation in Section [[Performance With Recovery]]. MPICHv2
terminates all processes by default when any process terminates before
calling ~MPI_Finalize~. Thankfully, MPICHv2 features an important flag
for fault tolerance, ~--disable-auto-cleanup~, which disables this
termination behaviour. The author engaged with MPICH2 community
\cite{disable-auto-cleanup} for instruction on using this non-standard
feature in MPICHv2 version 1.4.1 \cite{mpich2-1.4.1-release}.

**** Launching Instances of Multithread Haskell Executables

#+BEGIN_LATEX
\begin{table}\small
\begin{center}
\begin{tabular}{|l|p{8cm}|}
\hline
\texttt{mpiexec} flags & Description \\
\hline
\hline
\texttt{--disable-auto-cleanup} & Avoids the default MPI behaviour of terminating
                    all processes when one process exits e.g. because of failure. \\
\texttt{-machinefile} & A file name is specified that lists the machine names
                    used for creating node instances. \\
\texttt{-N} & Specifies how many node instances to create. \\
\texttt{<program>} & The user application using the HdpH-RS API, compiled with GHC. \\
\texttt{<program params>} & Arguments to the user application. These are inputs
                    to the user application. \\
\texttt{+RTS -N} & The number of compute capabilities afforded to the user program.
                    This very often is $n$ or $n-1$ (in the case of the HdpH-RS
                    benchmark results), where $n$ is the number of cores on
                    each node processor. \\
\hline
\end{tabular}
\caption{Arguments and Flags Used By \texttt{mpiexec}}
\label{tab:mpiexec-args}
\end{center}
\end{table}
#+END_LATEX

When a Haskell program that uses the HdpH-RS DSL is compiled, an
executable file is created. The arguments used by ~mpiexec~ for
deploying Haskell executable instances are described in Table
\ref{tab:mpiexec-args}. The program must be compiled with a
~-threaded~ flag, allowing a user to pass GHC specific RTS options at
runtime using ~+RTS~. This is needed in HdpH-RS to specify the number
of SMP processing elements to allocate to the executable per
node. This value specifies the number of HdpH-RS schedulers for
measurements like HdpH-RS runtimes. Throughout this evaluation
chapter, the term /node/ is used to specify a HdpH-RS node, not a
physical host. There is one HdpH-RS node deployed on each host on the
Beowulf cluster (Section [[Beowulf Cluster]]), and four HdpH-RS nodes
deployed on each host on HECToR (Section [[HECToR Cluster]]), one per NUMA
region \cite{DBLP:conf/sigmetrics/ZhouB91}.

**** HdpH-RS Runtime Flags

#+BEGIN_LATEX
\begin{table}\small
\begin{center}
\begin{tabular}{|l|p{10cm}|c|}
\hline
RTS flags & Description & Default \\
\hline
\hline
\texttt{-numProcs} & The number of nodes in the hosts file to be used in the execution of the
               user programs. The ~mpiexec~ launcher uses it to deploy remote program instances
               on each machine, and the HdpH-RS transport layer needs it (~numProcs~) to know
               how many instances to be advertised via UDP. & 1\\
\texttt{-scheds} & Threadpools on each node. The HdpH-RS scheduler forks a dedicated scheduler
               for each threadpool. The GHC runtime additionally needs to be asked for these
               many cores on each processor, in the \texttt{+RTS -N<s>} flag. & 1 \\
\texttt{-maxFish} & Low sparkpool watermark for fishing. The RTS will send \texttt{FISH} 
               messages unless the sparkpool is greater than the \texttt{maxFish} threshold. & 1 \\
\texttt{-minSched} &  Low sparkpool watermark for scheduling. The RTS will
               ignore ~FISH~ messages if the size of the sparkpool is less than ~minSched~. & 2 \\
\texttt{-minFishDly} & After a failed \texttt{FISH} (i.e. receiving a \texttt{NOWORK}
                message), this signals the minimum delay in
                microseconds before sending another \texttt{FISH} message. & 0.01s \\
\texttt{-maxFishDly} & After a failed ~FISH~, this signals the maximum delay
                in microseconds before sending another \texttt{FISH}
                message. The scheduler chooses some random time
                between \texttt{minFishDly} and \texttt{maxFishDly}. & 0.2s \\
\hline
\end{tabular}
\caption{HdpH-RS Runtime System Flags}
\label{tab:hdphrs-rts-opts}
\end{center}
\end{table}
#+END_LATEX

The HdpH-RS RTS options are used by each node instance. The RTS
options enable the user to configure the scheduler for optimal load
balancing for their target architecture. The HdpH-RS specific RTS
flags are shown in Table \ref{tab:hdphrs-rts-opts}. There is no
explicit flag to turn on reliable scheduling. Instead, it is
automatically invoked when ~supervisedSpawn~, ~supervisedSpawnAt~ or
fault tolerant skeletons are used. A node continues to fish for sparks
until its sparkpool holds at minimum of ~maxFish~ sparks. If a node
holds less than ~minSched~ sparks, it will reply to a ~FISH~ message
with a ~NOWORK~ reply. If it holds at least ~minSched~ sparks, it uses
the fault tolerant fishing protocol (Section [[Fault Tolerant Scheduling
Algorithm]]) to initiate a spark migration to the thief.

If a fishing attempt fails, i.e a ~NOWORK~ message is received, a node
will send a ~FISH~ to a new victim. The time delay from receiving the
~NOWORK~ to sending a new ~FISH~ is a randomly chosen value between
~minFishDly~ and ~maxFishDly~. The ~maxFishDly~ default value in HdpH
is 1 second. Fishing hops are disabled in HdpH-RS to avoid deadlocks
(Section [[Fishing Hops]]). The ~maxFishDly~ in HdpH-RS is reduced to 0.2
seconds to compensate for the removal of hopping.

**** Reliability and Fault Injection Flags

#+BEGIN_LATEX
\begin{table}\small
\begin{center}
\begin{tabular}{|l|p{9cm}|}
\hline
Reliability flags & Description \\
\hline
\hline
\texttt{-keepAliveFreq} & Sets a frequency for broadcasting \texttt{HEARTBEAT}
                      messages on every node. \\
\texttt{-killAt} & The length of a comma separated integer list indicates how many
                     nodes should die, and the values in the list indicate when each of those nodes should die. \\
\texttt{-chaosMonkey} & Informs root node to randomly select and poison
                    non-root nodes to die at a specific time, in
                    seconds. \\
\hline
\end{tabular}
\end{center}
\caption{HdpH-RS Reliability Flags}
\label{tab:hdphrs-reliability-args}
\end{table}
#+END_LATEX

The HdpH-RS reliability flags are shown in Table
\ref{tab:hdphrs-reliability-args}. There are two fault injection
facilities in HdpH-RS to simulate failure. They are used for measuring
recovery costs in Section [[Performance With Recovery]]. They simulate single
node failure, simultaneous failure e.g. network partitions, and
chaotic random failure.

The ~killAt~ flag allows a user to specify how many failures will
occur, and after how many seconds after program execution begins. This can
be used for simulating a sequence of node failures e.g. at 5, 10 and
23 seconds. It can also be used to
simulate simultaneous failure, such as a network partition
occurrence. For example, a user can specify that three nodes will fail
at 35 seconds with \texttt{-killAt=35,35,35}.

The second mechanism is invoked with a ~chaosMonkey~ flag, which
enables random failure injection, inspired by Netflix's Chaos Monkey
\cite{chaos-monkey}. The ~chaosMonkey~ mechanism is used as a unit
test to ensure that HdpH-RS returns a result in chaotic and unreliable
environments, and that the result is correct.  There are two phases to
this mechanism. First, a random number of nodes are selected to fail
during the program execution. Second, a node that has been selected to
fail will be poisoned at the start of execution. The poison will take
effect within the first 60 seconds of execution. The inputs to the
benchmarks in Section [[Simultaneous Multiple Failures]] require
failure-free execution of little more than 60 seconds. The maximum of
60 second failures is used to assess recovery costs when failure
occurs near the beginning and near the end of expected runtime.

The implementations of these failure mechanisms exploit the UDP node
discovery in HdpH-RS (Section [[Distributed Virtual Machine]]). The root
node poisons a selection of nodes in accordance with the user
specified timing set with ~killAt~, or randomly if ~chaosMonkey~ is
used. This is achieved by sending poisonous peer discovery UDP
messages to other nodes, described in Section [[Chaos Monkey]]


** Measurements Platform
*** Benchmarks

The runtime performance of HdpH-RS is measured using the five benchmarks
in Table \ref{tab:hdphrs-benchmarks}. They are implemented using fault
tolerant and non-fault tolerant skeletons, shown with both lazy and eager
scheduling. For example, Summatory Liouville is implemented with
~parMapSliced~, ~pushMapSliced~, ~parMapSlicedFT~ and
~pushMapSlicedFT~ (Appendix [[HdpH-RS Skeleton API]]).


#+BEGIN_LATEX
\begin{table}
\begin{center}
{\footnotesize
\begin{tabular}{|c|c|c|c|c|}
\hline
Benchmark & Skeleton & Code Origin & Regularity & \multirow{2}{*}{\pbox{4cm}{\relax\ifvmode\centering\fi Sequential \\ code size (lines)}} \\
 & & & & \\
\hline
\hline
Sum Euler           & chunked parallel maps                       & HdpH \cite{hdph-code}                            & Some        & 2                            \\
Summatory Liouville & sliced parallel map                         & GUM \cite{DBLP:conf/europar/HammondZCPT07} & Little      & 30                           \\
Fibonacci           & divide-and-conquer                                & HdpH \cite{hdph-code}                            & Little      & 2                            \\
N-Queens            & divide-and-conquer                                & monad-par \cite{monad-par-code}                  & Little      & 11                           \\
Mandelbrot          & MapReduce                                         & monad-par \cite{monad-par-code}                  & Very little & 12                           \\
\hline
\end{tabular}
}
\caption{HdpH-RS Benchmarks}
\label{tab:hdphrs-benchmarks}
\end{center}
\end{table}
#+END_LATEX

Sum Euler is a symbolic benchmark that sums Euler's totient function
$\phi$ over long lists of integers.  Sum Euler is an irregular data
parallel problem where the irregularity stems from computing $\phi$ on
smaller or larger numbers.

The Liouville function $\lambda(n)$ is the completely multiplicative
function defined by ${\lambda(p) = -1}$ for each prime $p$. Summatory
Liouville $L(n)$ denotes the sum of the values of the Liouville
function $\lambda(n)$ up to $n$, where $L(n)
:= \sum_{k=1}^{n} \lambda(k)$.

The Fibonacci function is a well known divide-and-conquer
algorithm. It is defined as $f_n = f_{n-1} + f_{n-2}$, where $F_0=0,
F_1 = 1$. A threshold $t$ is a condition for sequential evaluation. If
$n > t$, functions $f(n-1)$ and $f(n-2)$ are evaluated in
parallel. Otherwise $f(n)$ is evaluated sequentially.

The Mandelbrot set is the set of points on the complex plane that
remains bounded to the set when an iterative function is applied to
that. It consists of all points defined by the complex elements $c$
for which the recursive formula $z_{n+1} = z^{2}_{n} + c$ does not
approach infinity when $z_0 = 0$ and $n$ approach infinity. A depth
parameter is used to control the cost of sequential Mandelbrot
computations. A higher depth gives more detail and subtlety in the
final image representation of the Mandelbrot set \cite{mandelbrot}.

The $n$-queens problem computes how many ways $n$ queens can be put on an
$n \times n$ chessboard so that no 2 queens attack each other
\cite{1994-queens}. The implementation uses divide-and-conqueror
parallelism with an explicit threshold. An exhaustive search algorithm
is used.

*** Measurement Methodologies

Nodes on both the Beowulf (Section [[Beowulf Cluster]]) and HECToR
(Section [[HECToR Cluster]]) have 8 cores, 7 of which are used by each
HdpH-RS node instance to limit variability
\cite{DBLP:conf/haskell/HarrisMJ05} \cite{Maier_Trinder_IFL2011}. For
every data point, the mean of 5 runs are reported along with standard
error. Standard error is the mean, calculated by dividing the
population standard deviation by the square root of the sample size,
as $SE_{\bar{x}} = \frac{s}{\sqrt{n}}$. The speedup in this chapter is
measured by comparing the runtime of executing a skeleton on $N$ cores
with the execution of the same skeleton on $1$ core. So if the mean
runtime of ~parMapSliced~ on 1 core of 1 node is 500 seconds, and on
10 nodes i.e. 70 cores the mean runtime is 50 seconds then the speedup
on 70 cores is 10.


*** Hardware Platforms

The HdpH-RS benchmarks are measured on two platforms. The first is a
Beowulf cluster and is used to measure supervision overheads, and
recovery latency in the presence of simultaneous and random
failure. The second is HECToR, a national UK high-performance
computing service. The failure rates for HECToR are described in
Section [[Failure Rates]], which shows that there were 166 single node
failures between January 2012 to January 2013. The distributed
programming models supported by HECToR all depend on MPI for
node-to-node communication. Peer discovery with UDP is not supported,
so the HdpH-RS fault detecting transport layer (Section [[Fault
Detecting Communications Layer]]) cannot be used. The MPI-based HdpH
transport layer has been retrofitted in to HdpH-RS for the purposes of
assessing the scalability of the supervised work stealing in HdpH-RS
on HECToR in the absence of faults.

**** Beowulf Cluster

HdpH-RS is measured on a Heriot-Watt 32 node Beowulf cluster. Each
Beowulf node comprises two Intel Xeon E5504 quad-core CPUs at 2GHz,
sharing 12Gb of RAM. Nodes are connected via Gigabit Ethernet and run
Linux CentOS 5.7 $x86\_64$. Benchmarks are run on up to 32 HdpH-RS
nodes, scaling up to 256 cores.

**** HECToR Cluster

To measure scalability HdpH-RS is measured on a UK national compute
resource. The HECToR compute hardware is contained in 30 cabinets and
comprises a total of 704 compute blades. Each blade contains four
compute nodes running Compute Node Linux, each with two 16 core AMD
Opteron 2.3GHz Interlagos processors. This amounts to a total of
90,112 cores. Each 16-core socket is coupled with a Cray Gemini
routing and communications chip. Each 16-core processor shares 16Gb of
memory, giving a system total of 90.1Tb. The theoretical performance
of HECToR is over 800 Teraflops. There are four HdpH-RS node instances
launched on each host. The 32 cores on each host is separated in to
four NUMA \cite{DBLP:conf/sigmetrics/ZhouB91} regions. Each HdpH-RS
node instance (i.e. GHC executable) is pinned to a specific 8 core
NUMA region. Benchmarks are run on up to 200 HdpH-RS nodes. Each node
uses 7 cores to reduce variability, so HdpH-RS benchmarks are scaled
up to 1400 cores on HECToR.

** Performance With No Failure

*** HdpH Scheduler Performance

The HdpH-RS shared-memory scheduling of sparks and threads is
inherited from HdpH without modification. This section uses the Queens
benchmark to measure shared-memory parallel performance. A ~forkMap~
skeleton has been added to HdpH by the author, and is used to compare
shared-memory parallelism performance of HdpH against monad-par
\cite{DBLP:conf/haskell/MarlowNJ11}, on which the shared-memory HdpH
scheduler is based. It also compares the overhead of sparks and
supervised sparks using the ~parMap~ and ~parMapFT~ skeletons.

#+BEGIN_LATEX
\begin{Code}
\begin{haskellcode}{Queens Implementation using \texttt{monad-par}}{lst:queens-monadpar-impl}
monadpar_queens :: Int -> Int -> MonadPar.Par [[Int]]
monadpar_queens nq threshold = step 0 []
  where
    step :: Int -> [Int] -> MonadPar.Par [[Int]]
    step !n b
       | n >= threshold = return (iterate (gen nq) [b] !! (nq - n))
       | otherwise = do
          rs <- MonadPar.C.parMapM (step (n+1)) (gen nq [b])
          return (concat rs)

    safe :: Int -> Int -> [Int] -> Bool
    safe _ _ []    = True
    safe x d (q:l) = x /= q && x /= q + d && x /= q-d && safe x (d + 1) l

    gen :: [[Int]] -> [[Int]]
    gen bs = [ q:b | b <- bs, q <- [1..nq], safe q 1 b ]
\end{haskellcode}
\end{Code}
#+END_LATEX

The monad-par Queens implementation
\cite{DBLP:conf/haskell/MarlowNJ11} (Listing
\ref{lst:queens-monadpar-impl}) uses a divide-and-conquer pattern with
a parallel map to decompose tasks into subtasks.  This implementation
has been ported faithfully to HdpH using ~forkMap~, ~parMap~ and
~parMapFT~. The HdpH-RS test-suite also includes more abstract
divide-and-conquer skeleton implementations (Section [[Chaos Monkey]])
that are not evaluated here.

The shared-memory performance is measured with a $16 \times 16$ Queens
board, with thresholds from 1 to 6, increasing the number of generated
tasks from 16 to 1002778. The monad-par executions bypasses the HdpH
scheduler completely, and instead uses its own scheduler. All runs are
given 7 processing elements from GHC with the \texttt{+RTS~-N7}
runtime option.

#+CAPTION:    Runtimes for $16 \times 16$ Queens Board Comparing HdpH and monad-par on 7 cores
#+LABEL:      fig:queens-smp-runtimes
#+ATTR_LATEX: :width 80mm
[[./results/queens/monadpar-fork-spawn/queens-1node-runtime.pdf]]

Figure \ref{fig:queens-smp-runtimes} shows the mean of five runs for
all implementations with a standard error bar at each threshold. All
implementations have a similar runtime performance. The mean runtime
with a threshold of 1 yields the slowest runtimes in all
cases. Optimal runtimes are observed when the threshold is between 2
and 4. All runtimes increase in tandem with a threshold increase from
4 to 6. Variability of the 5 runtimes for all runs with HdpH-RS's
~spawn~ and ~supervisedSpawn~ is higher than ~fork~ in HdpH or
monad-par's ~spawn~. The standard error is smaller as the threshold
increases from 3 to 6 for all cases. That is, variability diminishes
as the number of generated tasks increases, reducing granularity.

The ~fork~ implementation using HdpH achieves lower runtimes than
monad-par for all threshold values except 1. This is somewhat
surprising, though the difference is not great. It suggests that there
is no performance penalty in opting for the HdpH-RS scheduler versus
the shared-memory-only monad-par scheduler, if HdpH-RS threads are
used.

The HdpH-RS runtimes using ~spawn~ or ~supervisedSpawn~ are slower
than using ~fork~. This runtime gap widens as the threshold increases
to 5 and 6, generating 163962 and 1002778 sparks respectively. The
increase may be attributed to two aspects of the architecture. First,
there may be contention on the node-global sparkpool data structure,
which is implemented as a double ended pure data structure in an
~IORef~ and is atomically locked as sparks are added and
removed. Second, the evaluation of values during modifications to the
~IVar~ registry may not be sufficiently strict. The registry is not
used by the ~fork~ primitive.

*** Runtime & Speed Up

**** Sum Euler
     
The speedup performance of Sum Euler is measured on the Beowulf
cluster up to 224 using $[1,2,4,8.. 32]$ nodes with $X = 250k$ and a
threshold of $1k$. The runtimes are shown in Figure
\ref{fig:sum-euler-runtime-bwlf} and the speedup in Figure
\ref{fig:sum-euler-speedup-bwlf}. Lazy scheduling is assessed with
~parMapSliced~ and ~parMapSlicedFT~. Eager scheduling is assessed
using ~pushMapSliced~ and ~pushMapSlicedFT~.

#+BEGIN_LATEX
\begin{figure}

\begin{center}
\begin{subfigure}[t]{0.48\textwidth}
\includegraphics[width=\textwidth]{results/sumeuler/sumeuler-bwlf-runtime.pdf}
\caption{Runtime}
\label{fig:sum-euler-runtime-bwlf}
\end{subfigure}
\begin{subfigure}[t]{0.48\textwidth}
\includegraphics[width=\textwidth]{results/sumeuler/sumeuler-bwlf-speedup.pdf}
\caption{Speedup}
\label{fig:sum-euler-speedup-bwlf}
\end{subfigure}
\end{center}
\vspace{-1\baselineskip}

\caption{Sum Euler on Beowulf}
\label{fig:sumeuler-bwlf}
\end{figure}
#+END_LATEX


The speedup results for Sum Euler are in Figure
\ref{fig:sum-euler-speedup-bwlf}. The speedup results show that the
two eager skeletons scale better than the two lazy scheduling
skeletons for Sum Euler. At 224 cores using 32 nodes the
~pushMapSliced~ and ~pushMapSlicedFT~ mean runtimes are 39.5s and
40.1s respectively, speedup's of 113 and 114. The ~parMapSliced~ and
~parMapSlicedFT~ mean runtimes are 53.1s and 67.9s respectively,
speedup's of 84.6 and 67.5. The eager skeletons achieve quicker
runtimes because task regularity is fairly consistent due to input
slicing. The slight speedup degradation after 56 cores when using lazy
work stealing is likely due to the latency of lazy work stealing.

**** Summatory Liouville

The speedup performance of Summatory Liouville is measured on the
Beowulf cluster up to 224 cores using $[1,2,4,8..32]$ nodes with $n =
200m$ and a threshold of $500k$. It is also measured on the HECToR
cluster up to 1400 cores using $[20,40..200]$ nodes with $n = 500m$
and a threshold of $250k$. On HECToR, the $n$ value is larger and the
threshold smaller than Beowulf as more tasks need to be generated to
fully utilise the architecture. That is, 2000 tasks for the 1400 cores
on HECToR and 400 tasks for the 244 cores on Beowulf. The Beowulf
runtimes are shown in Figure
\ref{fig:summatory-liouville-runtime-bwlf} and the speedup in Figure
\ref{fig:summatory-liouville-speedup-bwlf}. The HECToR runtimes are
shown in Figure \ref{fig:summatory-liouville-runtime-hector} and the
speedup in Figure \ref{fig:summatory-liouville-speedup-hector}. Once
again, lazy scheduling is assessed with ~parMapSliced~ and
~parMapSlicedFT~. Eager scheduling is assessed with Sum Euler
implementations using ~pushMapSliced~ and ~pushMapSlicedFT~.

#+BEGIN_LATEX
\begin{figure}

\begin{center}
\begin{subfigure}[t]{0.48\textwidth}
\includegraphics[width=\textwidth]{results/summatory-liouville/summatory-liouville-bwlf-runtime.pdf}
\caption{Runtime}
\label{fig:summatory-liouville-runtime-bwlf}
\end{subfigure}
\begin{subfigure}[t]{0.48\textwidth}
\includegraphics[width=\textwidth]{results/summatory-liouville/summatory-liouville-bwlf-speedup.pdf}
\caption{Speedup}
\label{fig:summatory-liouville-speedup-bwlf}
\end{subfigure}
\end{center}
\vspace{-1\baselineskip}

\caption{Summatory Liouville on Beowulf}
\label{fig:sumeuler-bwlf}
\end{figure}
#+END_LATEX

#+BEGIN_LATEX
\begin{figure}

\begin{center}
\begin{subfigure}[t]{0.48\textwidth}
\includegraphics[width=\textwidth]{results/summatory-liouville/summatory-liouville-hector-runtime.pdf}
\caption{Runtime}
\label{fig:summatory-liouville-runtime-hector}
\end{subfigure}
\begin{subfigure}[t]{0.48\textwidth}
\includegraphics[width=\textwidth]{results/summatory-liouville/summatory-liouville-hector-speedup.pdf}
\caption{Speedup}
\label{fig:summatory-liouville-speedup-hector}
\end{subfigure}
\end{center}
\vspace{-1\baselineskip}

\caption{Summatory Liouville on HECToR}
\label{fig:sumeuler-bwlf}
\end{figure}
#+END_LATEX

The same trend emerges as observed measuring Sum Euler, when comparing
lazy and eager skeletons. That is, due the task regularity achieved
using input slicing, eager scheduling outperforms lazy scheduling at
larger scales. The latency of work stealing impacts on speedup for the
two lazy skeletons after 56 cores on Beowulf, and after 280 cores on
HECToR. All four skeletons continue to speedup as cores are increased
on both Beowulf and HECToR. The speedup of the eager skeletons at 224
cores on Beowulf is 134.7 and 145.5 and at 1400 cores on HECToR is
751.4 and 756.7. The speedup of the lazy skeletons at 224 cores on
Beowulf is 81.4 and 93.7 and at 1400 cores on HECToR is 332.9 and
340.3.

The supervision overhead costs for Summatory Liouville are marginal at
all scales on both Beowulf and HECToR. The overheads are negligible on
the HECToR HPC architecture (Figure
\ref{fig:summatory-liouville-speedup-hector}) demonstrating that the
supervision in HdpH-RS scales to large parallel architectures for
regular task parallelism.

**** Mandelbrot

The speedup performance of Mandelbrot is measured on the Beowulf
cluster using $[1,2,4,8..32]$ nodes with $X = 4096$, and $Y=4096$, a
threshold of $4$ controlling the number of tasks, which is 1023 in
this case. The depth is $4000$. It is also measured on the HECToR
cluster using $[20,40..200]$ nodes with $X = 8192$, and $Y=8192$, a
threshold of $4$ controlling the number of tasks, which is 1023 in
this case. The depth is $8000$. The Beowulf runtimes are shown in
Figure \ref{fig:mandel-runtime-bwlf} and the speedup in Figure
\ref{fig:mandel-speedup-bwlf}. The HECToR runtimes are shown in Figure
\ref{fig:mandel-runtime-hector} and the speedup in Figure
\ref{fig:mandel-speedup-hector}.

#+BEGIN_LATEX
\begin{figure}

\begin{center}
\begin{subfigure}[t]{0.48\textwidth}
\includegraphics[width=\textwidth]{results/mandel/mandel-bwlf-runtime.pdf}
\caption{Runtime}
\label{fig:mandel-runtime-bwlf}
\end{subfigure}
\begin{subfigure}[t]{0.48\textwidth}
\includegraphics[width=\textwidth]{results/mandel/mandel-bwlf-speedup.pdf}
\caption{Speedup}
\label{fig:mandel-speedup-bwlf}
\end{subfigure}
\end{center}
\vspace{-1\baselineskip}

\caption{Mandelbrot on Beowulf}
\label{fig:mandel-bwlf}
\end{figure}
#+END_LATEX

#+BEGIN_LATEX
\begin{figure}[t]

\begin{center}
\begin{subfigure}[t]{0.48\textwidth}
\includegraphics[width=\textwidth]{results/mandel/mandel-hector-runtime.pdf}
\caption{Runtime}
\label{fig:mandel-runtime-hector}
\end{subfigure}
\begin{subfigure}[t]{0.48\textwidth}
\includegraphics[width=\textwidth]{results/mandel/mandel-hector-speedup.pdf}
\caption{Speedup}
\label{fig:mandel-speedup-hector}
\end{subfigure}
\end{center}
\vspace{-1\baselineskip}

\caption{Mandelbrot on HECToR}
\label{fig:mandel-hector}
\end{figure}
#+END_LATEX

The Beowulf runtimes in Figure \ref{fig:mandel-speedup-bwlf} shows
that all four skeletons speedup with only slight degradation up to 244
cores. There is no clear distinction between the lazy and eager
skeletons, and the supervision costs of the fault tolerant skeletons
are neglible. The speedup of all four skeletons peaks at either 196 or
224 cores --- between 57.4 and 60.2.

A contrast between lazy and eager scheduling is apparent in the HECToR
speedup measurements in Figure
\ref{fig:mandel-speedup-hector}. Runtimes are improved for the eager
skeletons continue up to 560 cores, with speedup's of 87.2 and 88.5. A
modest speedup from 560 to 1400 cores is observed. The lazy skeletons
no longer exhibit significant speedup after 140 cores, hitting
speedup's of 49.7 and 50.5. Thereafter, speedup relative to using 1
core is constant up to 1400 cores, with ~parMapReduceRangeThresh~ peaking
at 57.2 using 700 cores.

The supervision overhead costs for Mandelbrot are marginal at all
scales on both Beowulf and HECToR. The overheads are negligible on the
HECToR HPC architecture (Figure \ref{fig:mandel-speedup-hector})
demonstrating that the supervision in HdpH-RS scales to large parallel
architectures for divide-and-conquer style parallelism with
hierarchically nested supervision.

**** Fibonacci
     
The speedup performance of Fibonacci is measured on the Beowulf
cluster up to 224 cores using $[1,2,4,8..32]$ nodes with $X = 53$ and
a threshold of $27$. Lazy task placement is measured with ~parDnC~, a
divide-and-conquer skeleton with lazy scheduling. Fault tolerant lazy
task placement is measured with ~parDnCFT~. Explicit skeletons
~pushDnC~ and ~pushDnCFT~ were tried with Fibonacci $53$ with the
threshold $27$. Whilst the mean runtimes for ~parDnC~ and ~parDnCFT~
using 7 cores is 301.4 and 363.6 seconds, the ~pushDnC~ and
~pushDnCFT~ implementations run for 1598.2 and 1603.3 seconds
respectively. Scaling to more nodes only increased runtimes with
explicit scheduling e.g. a mean runtime of 2466.0 seconds for
~pushDnC~ using 14 cores. Only results with lazy scheduling are
plotted. The runtimes are shown in Figure \ref{fig:fib-runtime-bwlf}
and the speedup in Figure \ref{fig:fib-speedup-bwlf}.

#+BEGIN_LATEX
\begin{figure}

\begin{center}
\begin{subfigure}[t]{0.48\textwidth}
\includegraphics[width=\textwidth]{results/fib/fib-bwlf-runtime.pdf}
\caption{Runtime}
\label{fig:fib-runtime-bwlf}
\end{subfigure}
\begin{subfigure}[t]{0.48\textwidth}
\includegraphics[width=\textwidth]{results/fib/fib-bwlf-speedup.pdf}
\caption{Speedup}
\label{fig:fib-speedup-bwlf}
\end{subfigure}
\end{center}
\vspace{-1\baselineskip}

\caption{Fibonacci on Beowulf}
\label{fig:fib-bwlf}
\end{figure}
#+END_LATEX

Fibonacci was implemented using the lazy divide-and-conquer
skeletons. The runtimes for ~parDnCFT~ are very similar to ~parDnC~ at
all scales, demonstrating that hierarchically nested supervision costs
are negligible. The speedup of both the fault tolerant and non-fault
tolerant skeletons peak at 196 core on Beowulf, at 54.4 and 52.1
respectively.

** Performance With Recovery
*** Simultaneous Multiple Failures

The HdpH-RS scheduler is designed to survive simultaneous
failure. These can occur for many reasons e.g. due to network or power
supply hardware failures to partitions of a cluster. Node failure
recovery is distributed, the failure of a node will eventually be
detected by all healthy nodes. Each is responsible for taking recovery
action. That is, a node must replicate tasks corresponding to
supervised futures that it hosts, in accordance with Algorithm
\ref{alg:handleDeadnode} from Section [[Fault Tolerant Scheduling
Algorithm]].

This section uses the ~killAt~ poisonous RTS flag to instruct the root
node to poison 5 nodes, i.e. 35 cores simultaneously at a set
time. The experiment inputs in this section are set so that
failure-free runtime is a little more than 60 seconds on 20 nodes. The
5 nodes are scheduled to die at [10,20..60] seconds in to the
execution. The runtimes are compared to failure-free runs using the
non-fault tolerant counterpart to assess recovery times.

Two benchmarks are used to measure recovery overheads, Summatory
Liouville for task parallelism and Mandelbrot for divide-and-conquer
parallelism. Summatory Liouville is a benchmark that is implemented
with the parallel-map family. All tasks are generated by the root
node, and tasks are not recursively decomposed. The Mandelbrot
benchmark is implemented with the map-reduce-thresh family, a
divide-and-conquer pattern that results in the supervision of futures
across multiple nodes.

**** Summatory Liouville

The inputs for Summatory Liouville are $n=140m$, and a threshold of
$2m$. This generates 70 tasks. The mean of 5 runs with ~parMapSliced~
is 66.8 seconds. The mean of 5 runs with ~pushMapSliced~ is 42.8
seconds.

#+CAPTION:    Simultaneous Failure of 5 Nodes Executing Summatory Liouville on Beowulf
#+LABEL:      fig:simultaneous-failure-summatory-liouville
#+ATTR_LaTeX: :width 80mm
[[./results/summatory-liouville/summatory-liouville-simultaneous-failure-runtime.pdf]]

The recovery from the death of 5 nodes at 6 different timings for
Summatory Liouville is shown in Figure
\ref{fig:simultaneous-failure-summatory-liouville}. The recovery
overheads of lazy and eager scheduling follow different trends. All
supervised futures are created on the root node in both cases of
~parMapSlicedFT~ and ~pushMapSlicedFT~.

When eager scheduling is used, the recovery overheads are more
substantial early on i.e. at 10, 20 and 30 seconds. These overheads
compared with fault-free execution with ~pushMapSlicedFT~ are 158%,
172% and 140% respectively. As more tasks are evaluated and
therefore more supervised futures on the root node filled, the
recovery costs reduce at 60 seconds. At 40 seconds, the overhead is
70%. There are no measurements taken at 50 and 60 seconds as the mean
failure-free runtime with ~pushMapSliced~ is 43 seconds, so the 5
node poison at 50 and 60 seconds would have no effect.

The recovery overheads for lazy scheduling with ~parMapSlicedFT~
follow an altogether different pattern. With a high standard error
over the 5 executions with failure at 10 seconds, the mean runtime is
/shorter/ than failure-free execution with ~parMapSliced~ by
14%. Unlike eager task placement, this is likely due to a much
smaller number of the 70 generated sparks being fished by the 5 nodes
that had failed after only 10 seconds. Moreover, it is likely that the
balance between computation and communication for the job size with
$n=140m$ favours smaller cluster than 20 nodes, i.e. the 15 nodes that
are left. As the delay until failure is increased to 20, 30, 40 and 50
seconds, the recovery overheads are 2%, 13%, 18% and 22%. As most
supervised futures on the root node become full at the 60 second
failure executions, fewer sparks need replicating. Thus, the runtimes
with failure at 60 seconds are close to the failure-free runtimes,
resulting in a small speedup of 3%.


**** Mandelbrot

The inputs for Mandelbrot are $X=4096$, $Y=4096$, $threshold=4$ and
$depth=4000$. This generates 1023 tasks. The mean of 5 runs with
~parMapReduceRangeThresh~ is 66 seconds. The mean of 5 runs with
~pushMapReduceRangeThresh~ is 92 seconds.

#+CAPTION:    Simultaneous Failure of 5 Nodes Executing Mandelbrot on Beowulf
#+LABEL:      fig:simultaneous-failure-mandelbrot
#+ATTR_LaTeX: :width 80mm
[[./results/mandel/mandel-simultaneous-failure-runtime.pdf]]

The recovery from the death of 5 nodes at the 6 different timings for
Mandelbrot is shown in Figure
\ref{fig:simultaneous-failure-mandelbrot}, and reveals two distinct
trends. The recovery overheads for the lazy ~parMapReduceRangeThreshFT~
skeleton are low, even as the number of generated supervised futures
increases. The mean runtime when 5 nodes are killed at 60 seconds is
marginally shorter by 3.0 seconds than failure-free execution with 20
nodes, a speedup of 5%.

The recovery overheads for the eager ~pushMapReduceRangeThreshFT~ skeleton
increases as more threads are replicated needlessly (Section
[[Increasing Recovery Overheads with Eager Scheduling]]). Early failures
of 10 seconds shortens runtime by 10%. The eventual total of 1023
threads will likely not have been created by this point, and balance
between communication and computation for the inputs of $X=4096$ and
$Y=4096$ may favour the smaller 15 node cluster size. However, when
the 5 node failure occurs at 20, 30, 40, 50 and 60 seconds, the
recovery costs increase. For these times, the recovery overheads are
44%, 18%, 59%, 80% and 110%.


*** Chaos Monkey

A unit testing suite is built-in to eight benchmarks (Table
\ref{tab:chaos-monkey-results}) to check that results computed by
HdpH-RS in the presence of random failure are correct. Sum Euler is
used to measure recovery costs of ~parMapChunkedFT~ and
~pushMapChunkedFT~, Summatory Liouville to asses ~parMapSlicedFT~ and
~pushMapSlicedFT~, Queens to measure ~parDnCFT~ and ~pushDnCFT~ and
Mandelbrot to assess ~parMapReduceRangeThreshFT~ and
~pushMapReduceRangeThreshFT~. The unit tests are invoked when the
~chaosMonkey~ flag is used. The chaos monkey failure injection
exploits the UDP peer discovery in HdpH-RS, and is shown in Listing
\ref{lst:chaosmonkey-impl}. The ~chaosMonkeyPoison~ function on line
\ref{code:chaosMonkeyPoison} is executed by the root node. It randomly
decides how many of the non-root nodes will be poisoned (line
\ref{code:poisonCount}), and the timing of failure in each case (line
\ref{code:poisonTimes}). These messages are sent to a randomly sorted
node sequence.

The executing code on non-root nodes is ~run~ on line
\ref{code:runComm}. A non-root node waits for a ~Booted~ message on
line \ref{code:waitForRootNodeBootMsg}. If a \texttt{Booted~Nothing}
message is received then it is ignored. If a
\texttt{Booted~(Just~x)} message is received, then a suicidal thread
is forked on line \ref{code:suicideThread}. It will kill the node
at $x$ seconds in to the job execution.

#+BEGIN_LATEX
\begin{Code}
\begin{haskellcode}{Implementation of Chaos Monkey}{lst:chaosmonkey-impl}
-- | Executed on root node when -chaosMonkey flag is used.
--      Give a (< 60 second) poison pill to random number of nodes:
--        (Booted Nothing) means remote node is not poisoned.
--        (Booted (Just x)) means remote node dies after x seconds.
--
--      If -chaosMonkey flag is not used, this function is not used.
--      Instead, every non-root node is sent (Booted Nothing) message.
chaosMonkeyPoison :: [NodeId] -> IO [(NodeId,Msg)] @\label{code:chaosMonkeyPoison}@
chaosMonkeyPoison nodes = do
  x <- randomRIO (0,length nodes) -- number of nodes to poison @\label{code:poisonCount}@
  deathTimes <- take x . randomRs (0,60) <$> newStdGen -- nodes die within 60 seconds @\label{code:poisonTimes}@
  randNodes <- shuffle nodes
  let pills =    map (Booted . Just) deathTimes  -- poison pills
              ++ repeat (Booted Nothing)        -- placebo pills
  return (zip randNodes pills)

-- | Executed on non-root nodes:
--      node waits for 'Booted _' message in 'Control.Parallel.HdpH.Internal.Comm'
run = do @\label{code:runComm}@
    {- omitted UDP discovery code -}
    (Booted x) <- waitForRootNodeBootMsg -- receive bootstrap from root @\label{code:waitForRootNodeBootMsg}@
    -- fork a delayed suicide if (Boot (Just x)) received
    when (isJust x) $ void $ forkIO $ do
        threadDelay (1000000 * fromJust x) >> raiseSignal killProcess @\label{code:suicideThread}@
    {- omitted transport layer initialisation -}
\end{haskellcode}
\end{Code}
#+END_LATEX

The ~HUnit~ test framework \cite{hunit-package} is used in the HdpH-RS
function ~chaosMonkeyUnitTest~ shown in Listing
\ref{lst:chaosmonkey-unittest-api} and verifies that the fault
tolerant skeletons return the correct result with chaos monkey
enabled. For example, the ~chaosMonkeyUnitTest~ takes a HdpH-RS RTS
configuration, a ~String~ label for the test
e.g. "sumeuler-0-50k-chaos-monkey", then an expected value $759924264$
for a parallel Sum Euler $0$ to $50000$ computation in the ~Par~
monad. The implementation of ~chaosMonkeyUnitTest~ is in Appendix
[[Using Chaos Monkey in Unit Testing]].

#+BEGIN_LATEX
\begin{Code}
\begin{haskellcode}{API for Chaos Monkey Unit Testing}{lst:chaosmonkey-unittest-api}
chaosMonkeyUnitTest
      :: (Eq a)
      => RTSConf -- user defined RTS configuration
      -> String  -- label identifier for unit test
      -> a       -- expected value
      -> Par a   -- Par computation to execute with failure
      -> IO ()
\end{haskellcode}
\end{Code}
#+END_LATEX

An execution of Sum Euler with the fault tolerant ~pushMapChunkedFT~
skeleton is shown in Listing
\ref{lst:chaos-monkey-execution-pushMap}. It asks for 5 node instances
from ~mpiexec~ with the \texttt{-N~5} flag, and HdpH-RS is told
discover 5 nodes via UDP with ~-numProcs=5~. MPI is forced to ignore a
node failure with the ~--disable-auto-cleanup~ flag. The ~chaosMonkey~
flag turns on random node failure. The root node non-deterministically
decides that 3 nodes shall fail. The first will fail at 4 seconds, the
second at 23 seconds and the third at 36 seconds. When failure is
detected, the at-risk threads on the failed node are
re-scheduled. There are no sparks to re-schedule because
~pushMapChunkedFT~ is entirely explicit, using
~supervisedSpawnAt~. Intuitively as time progresses, fewer threads
will need re-scheduling, as more \texttt{IVar}s are filled by
executing threads. Hence, 26 threads are re-scheduled after the 4
second failure, 20 threads after the 23 second failure, and 17 threads
after the 36 second failure. The last line in
\ref{lst:chaos-monkey-execution-pushMap} indicates that the execution
terminated and the result was matched to the correct result 759924264.

#+BEGIN_LATEX
{\footnotesize
\begin{lstlisting}[basicstyle=\ttfamily\footnotesize, caption=Execution of Sum Euler with \texttt{pushMapFT} \& Chaos Monkey Fault Injection,label=lst:chaos-monkey-execution-pushMap]
$ mpiexec -N 5 --disable-auto-cleanup sumeuler -numProcs=5 -chaosMonkey v13
Chaos monkey deaths at (seconds): [4,23,36]
kamikaze 137.195.143.115:31250:0
replicating threads: 26
kamikaze 137.195.143.115:31631:0
replicating threads: 20
kamikaze 137.195.143.115:29211:0
replicating threads: 17
sumeuler-pushMapChunkedFT result: 759924264
Cases: 1  Tried: 1  Errors: 0  Failures: 0
\end{lstlisting}
}
#+END_LATEX

An execution of Sum Euler with the fault tolerant ~parMapChunkedFT~
skeleton is shown in Listing
\ref{lst:chaos-monkey-execution-parMap}. Again 5 nodes are used, with
chaos monkey poisoning 4 of them, to die at 6, 7, 35 and 55
seconds. Fewer replications are re-scheduled i.e. $3\times$ 2 replicas
and $1\times$ 1 replica, due to lazy work stealing of sparks in
contrast to preemptive eager round-robin task placement with
~pushMapChunkedFT~. The last line in
\ref{lst:chaos-monkey-execution-parMap} indicates that the execution
terminated and the result was matched to the correct result 759924264.


#+BEGIN_LATEX
{\footnotesize
\begin{lstlisting}[basicstyle=\ttfamily\footnotesize, caption=Execution of Sum Euler with \texttt{parMapFT} \& Chaos Monkey Fault Injection,label=lst:chaos-monkey-execution-parMap]
$ mpiexec -N 5 --disable-auto-cleanup sumeuler -numProcs=5 -chaosMonkey v10
Chaos monkey deaths at (seconds): [6,7,35,55]
kamikaze 137.195.143.125:18275:0
replicating sparks: 2
kamikaze 137.195.143.125:15723:0
replicating sparks: 1
kamikaze 137.195.143.125:31678:0
replicating sparks: 2
kamikaze 137.195.143.125:31216:0
replicating sparks: 2
sumeuler-parMapChunkedFT result: 759924264
Cases: 1  Tried: 1  Errors: 0  Failures: 0
\end{lstlisting}
}
#+END_LATEX

**** Chaos Monkey Results

The results of chaos monkey unit testing is shown in Table
\ref{tab:chaos-monkey-results}. All experiments were launched on 10
nodes of the Beowulf cluster. The /Benchmarks/ column shows the input,
threshold, number of generated tasks, and expected values for each
benchmark. The /Skeleton/ column states which fault tolerant parallel
skeleton is used to parallelise the implementation. The /Failed Nodes/
column shows a list of integers. The length of this list indicates the
number of nodes that were poisoned with chaos monkey. The integer
values indicate the time in seconds at which a node will fail. For
example the list \texttt{[24,45]} states that two nodes will fail, the
first at 24 seconds and the second at 45 seconds. The /Recovery/
column states how many tasks were recovered across all remaining nodes
to recover from failure throughout execution. For the implicit
skeletons e.g. ~parMapFT~ only sparks are generated and thus
recovered. For the implicit skeletons e.g. ~pushMapFT~ only threads
are generated and recovered. The /Runtime/ column shows the runtime
for each execution. Lastly, the /Unit Test/ column states whether the
result from HdpH-RS in the presence of failure matched the
pre-computed result using HdpH with no failure. The test-suite passes
100% of the unit tests.

#+BEGIN_LATEX
\begin{table}\footnotesize
\caption{Fault Tolerance Unit Testing: Chaos Monkey Runtimes}
\label{tab:chaos-monkey-results}
\setlength{\tabcolsep}{.16667em}
\begin{tabular}{|>{\footnotesize}l||>{\scriptsize}p{3.9cm}|>{\scriptsize}c|>{\scriptsize}c|>{\scriptsize}c|>{\scriptsize}c|>{\scriptsize}c|}
\hline
\multirow{2}{*}{Benchmark} & \multicolumn{1}{c|}{\multirow{2}{*}{\scriptsize{Skeleton}}} & \footnotesize{Failed Nodes}  & \multicolumn{2}{c|}{Recovery} & Runtime & \multirow{2}{*}{Unit Test} \\
\cline{4-5}
& & \scriptsize{(seconds)} & Sparks & Threads  & (seconds) & \\
\hline
\hline
 & \texttt{parMapChunked} & - & & & 126.1 & pass \\
\cline{2-7}
\multirow{10}{*}{\pbox{30cm}{Sum Euler \\ \emph{lower=0} \\ \emph{upper=100000} \\ \emph{chunk=100} \\ \emph{tasks=1001} \\ \emph{X=3039650754}}} & \multirow{5}{*}{\texttt{parMapChunkedFT}} & [6,30,39,49,50] & 10 & & 181.1 & pass \\
 & & [5,11,18,27,28,33,44,60] & 16 & & 410.2 & pass \\
 & & [31,36,49] & 6 & & 139.7 & pass \\
 & & [37,48,59] & 6 & & 139.5 & pass \\
 & & [1,17,24,27,43,44,47,48,48] & 17 & & 768.2 & pass \\
\cline{2-7}
 & \texttt{pushMapChunked} & - & & & 131.6 & pass \\
\cline{2-7}
 & \multirow{5}{*}{\texttt{pushMapChunkedFT}} & [4,34,36,37,48,49,58] & & 661 & 753.7 & pass \\
 & & [2,6,11,15,17,24,32,37,41] & & 915 & 1179.7 & pass \\
 & & [2,37,39,45,49] & & 481 & 564.0 & pass \\
 & & [4,7,23,32,34,46,54,60] & & 760 & 978.1 & pass \\
 & & [35,38,41,43,46,51] & & 548 & 634.3 & pass \\
\hline
 & \texttt{parMapSliced} & - & & & 56.6 & pass \\
\cline{2-7}
\multirow{10}{*}{\pbox{30cm}{Summatory \\ Liouville \\ $\lambda=50000000$ \\ \emph{chunk=100000} \\ \emph{tasks=500} \\ \emph{X=-7608}}} & \multirow{5}{*}{\texttt{parMapSlicedFT}} & [32,37,44,46,48,50,52,57] & 16 & &  85.1 & pass \\
 & & [18,27,41] & 6 & & 61.6 & pass \\
 & & [19,30,39,41,54,59,59] & 14 & & 76.2 & pass \\
 & & [8,11] & 4 & & 62.8 & pass \\
 & & [8,9,24,28,32,34,40,57] & 16 & & 132.7 & pass \\
\cline{2-7}
 & \texttt{pushMapSliced} & - & & & 58.3 & pass \\
\cline{2-7}
 & \multirow{5}{*}{\texttt{pushMapSlicedFT}} & [3,8,8,12,22,26,26,29,55] & & 268 & 287.1 & pass \\
 & & [1] & & 53 & 63.3 & pass \\
 & & [10,59] & & 41 & 68.5 & pass \\
 & & [13,15,18,51] & & 106 & 125.0 & pass \\
 & & [13,24,42,51] & & 80 & 105.9 & pass \\
\hline
 & \texttt{parDnC} & - & & & 28.1 & pass \\
\cline{2-7}
\multirow{10}{*}{\pbox{30cm}{Queens \\ $14\times14\, board$ \\ \emph{threshold=5} \\ \emph{tasks=65234} \\ \emph{X=365596}}} & \multirow{5}{*}{\texttt{parDnCFT}} & [3,8,9,10,17,45,49,51,57] & 8 & & 52.1 & pass \\
 & & [1,30,32,33,48,50] & 5 & & 49.4 & pass \\
 & & [8,15] & 2 & & 53.3 & pass \\
 & & [20,40,56] & 2 & & 49.9 & pass \\
 & & [] & 0 & & 52.8 & pass \\
\cline{2-7}
 & \texttt{pushDnC} & - & & & 15.4 & pass \\
\cline{2-7}
 & \multirow{5}{*}{\texttt{pushDnCFT}} & [14,33] & & 5095 & 57.1 & pass \\
 & & [3,15,15,23,24,28,32,48] & & 40696 & 649.5 & pass \\
 & & [5,8,26,41,42,42,59] & & 36305 & 354.9 & pass \\
 & & [0,5,8,10,14,28,31,51,54] & & 32629 & 276.9 & pass \\
 & & [31,31,58,60] & & 113 & 47.8 & pass \\
\hline
 & \texttt{parMapReduceRangeThresh} & - & & & 23.2 & pass \\
\cline{2-7}
\multirow{10}{*}{\pbox{30cm}{Mandelbrot \\ \emph{x=4048} \\ \emph{y=4048} \\ \emph{depth=256} \\ \emph{threshold=4} \\ \emph{tasks=1023} \\ \emph{X=449545051}}} & \multirow{5}{*}{\texttt{parMapReduceRangeThreshFT}} & [28,30,36,44,49,54,56,56] & 0 & & 29.1 & pass \\
 & & [] & 0 & & 27.8 & pass \\
 & & [7,24,25,25,44,53,54,59] & 6 & & 32.6 & pass \\
 & & [17,30] & 0 & & 55.4 & pass \\
 & & [0,14] & 2 & & 33.7 & pass \\
\cline{2-7}
 & \texttt{pushMapReduceRangeThresh} & - & & & 366.3 & pass \\
\cline{2-7}
 & \multirow{5}{*}{\texttt{pushMapReduceRangeThreshFT}} & [9,24,34,34,52,59] & & 419 & 205.3 & pass \\
 & & [7,8,11,22,32,35,44,46] & & 686 & 395.9 & pass \\
 & & [27,49] & & 2 & 371.8 & pass \\
 & & [] & & 0 & 380.4 & pass \\
 & & [9,33,50,50,52,53] & & 84 & 216.1 & pass \\
\hline
\end{tabular}
\end{table}
#+END_LATEX

The key observation from these results is that lazy scheduling reduces
recovery costs in the presence of failure. The ~parDnCFT~ skeleton
(divide-and-conquer using ~supervisedSpawn~) creates only sparks,
initially on the root node, which get fished away with work
stealing. The ~pushDnCFT~ skeleton uses ~supervisedSpawnAt~ to
randomly select nodes to eagerly scheduled tasks as threads. The
laziness of ~supervisedSpawn~ minimises the migration of sparks,
taking place only when nodes become idle. It also minimises the number
of sparks that need to be recovered. For example, the Queens unit test
generates $65234$ tasks. The ~pushDnCFT~ results in the re-scheduling
of $40696$ threads in the case of between 1 and 3 node failures at
$[3,15,15]$ before the execution would have likely terminated at 15
seconds. Subsequent node failures at $[23,24,28,32,48]$ seconds may
have resulted in multiple replicas of tasks being eagerly scheduled as
earlier scheduled replicas may have also been lost. The runtime to
termination is 650 seconds. The Queens unit test with ~parDnCFT~
results in the re-scheduling of only $8$ sparks in another case of 5
likely node failures at $[3,8,9,10,17]$ before the execution would
have likely terminated at 28 seconds. Subsequent node failures at
$[45,49,51]$ seconds may have resulted in multiple replicas of tasks
being lazily scheduled as earlier scheduled replicas may have also
been lost. The runtime to termination is 52 seconds. An explanation
for the much lower impact of failure when on-demand work stealing is
used is given in Section [[Increasing Recovery Overheads with Eager
Scheduling]].

On-demand work stealing also manages the trade off between
communication and computation by reducing unnecessary task scheduling
to remote nodes if task granularity is very small. The Mandelbrot
benchmark is used to demonstrate this. The depth is set at $256$, a
relatively low number that generates small task sizes (the speed-up is
later measured in Section [[Runtime & Speed Up]] using a depth of
$4000$). The small Mandelbrot task sizes in the chaos monkey
experiments mean that very few sparks are replicated as most are
executed by the root node. In 3 runs, $0$ sparks are replicated
including an execution with 8 node failures. In contrast, 686 threads
are replicated with a failure accumulation of 8 nodes, when eager
scheduling pushes tasks to remote nodes regardless of task size.

*** Increasing Recovery Overheads with Eager Scheduling

#+CAPTION:    Isolated Supervision Trees with Lazy Scheduling
#+LABEL:      fig:isolated-supervision-trees
#+ATTR_LaTeX: :width 110mm
[[./img/chp6/lazy-eager-recovery/lazy-recovery.pdf]]

The lazy task scheduling with ~parMapChunkedFT~ for Sum Euler,
~parMapSlicedFT~ for Summatory Liouville, ~parDnCFT~ for Queens, and
~parMapReduceFT~ for Mandelbrot result in lower recovery costs than
their eager skeleton counterparts. For example, the Chaos Monkey data
for Sum Euler in Table \ref{tab:chaos-monkey-results} show that the
number of replicated sparks with ~parMapChunkedFT~ is between 6 and
17, with a runtime minimum of 140s and maximum of 768s. The number
of replicated threads with ~pushMapChunkedFT~ is between 481 and 915,
with a runtime minimum of 564s and a maximum of 1180s.

In the eager divide-and-conquer cases of Queens and Mandelbrot, most
threads are unnecessarily re-scheduled. This is illustrated in Figures
\ref{fig:isolated-supervision-trees} and
\ref{fig:split-supervision-trees}. A simple example of lazy scheduling
is shown in Figure \ref{fig:isolated-supervision-trees}. The root node
A initially decomposes the top function call in to 2 separate
sparks. One is fished away to node B. The recursive decomposition of
this task saturates node B with work, and it no longer fishes for
other sparks.  If node B fails, node A lazily re-schedules a replica
of only the top level spark.

#+CAPTION:    Split Supervision Trees with Eager Scheduling
#+LABEL:      fig:split-supervision-trees
#+ATTR_LaTeX: :width 110mm
[[./img/chp6/lazy-eager-recovery/eager-recovery.pdf]]

The reason for the high level of thread rescheduling for all
benchmarks (Table \ref{tab:chaos-monkey-results}) is illustrated in
Figure \ref{fig:split-supervision-trees}. As before, node A decomposes
the top level function call in to 2 separate tasks. With random work
distribution, one is pushed eagerly to node B. Converting and
executing this task generates 2 more child tasks, one being pushed
back to A. This happens for a 3rd time i.e. a supervision tree is
split between nodes A and B. The failure of node B will result in the
necessary replication of the top level task on B, and the unnecessary
replication of the grey tasks on B. The supervision tree will be
regenerated, thus replicating these grey children for a 2nd and 3rd
time respectively.

The pattern of results for Sum Euler are mirrored in the Summatory
Liouville, Queens and Mandelbrot runtimes. That is, lazy scheduling
results in much lower replica numbers and hence shorter runtimes in
the presence of failure. There are some cases where failure costs are
very low, even when with a high failure rate. There are 2 runs of Sum
Euler that incurs 3 node failures, of the 10 nodes used at the
start. Their runtimes are both 139s, in comparison to fault-free
execution with ~parMap~ of 126.1s --- an increase of 11%. An execution
of Summatory Liouville incurs the failure of 7 nodes. The runtime is
76s, an increase of 35% in comparison to fault-free execution on
10 nodes. This increase is attributed to failure detection latency,
the recovery of 14 sparks, and a decrease in compute power of 30%
i.e. 3 out of 10 nodes. There are examples when the timing of failure
is larger than the execution time. For example, chaos monkey schedules
8 node failures for one run of Mandelbrot. However, the execution is
complete after 29s, so only one node will have failed by then, after
28 seconds.

The large recovery costs of using eager skeletons is due in-part to
their naive design. The HdpH-RS skeletons ~pushMapFT~,
~pushMapSlicedFT~, ~pushMapChunkedFT~, ~pushDnCFT~ and
~pushMapReduceRangeThreshFT~ are built on top of the HdpH-RS
~supervisedSpawnAt~ primitive. Task placement at all levels of the
supervision tree is random. It does not consider the location of
parent supervisors, or organise nodes as tree structures corresponding
to the supervision tree. These techniques could avoid unnecessary
replication and is left as future work.

** Evaluation Discussion

This chapter has shown how to write fault tolerant programs with
HdpH-RS, and has evaluated the performance of HdpH-RS in the absence
and presence of faults. The speedup results demonstrate that when task
size variability can be lowered with input slicing, eager preemptive
scheduling achieves shorter runtimes than lazy scheduling with work
stealing. However when failures are present, on-demand lazy scheduling
is more suitable for minimising recovery costs. Recovery costs of lazy
scheduling is low even when failure frequency is high e.g. 80% node
failure. The supervision costs incurred by using the fault tolerant
fishing protocol is negligible on the Beowulf cluster and on HECToR,
demonstrating that the HdpH-RS scheduler can be scaled to massively
parallel architectures.

Two benchmarks were used to assess the performance of single level
supervision with parallel-map skeletons and three were used to assess
hierarchically nested supervision with map-reduce and
divide-and-conquer skeletons. These were executed on 244 cores of a 32
node Beowulf cluster, and 1400 cores on HECToR.

The Sum Euler and Summatory Liouville benchmarks were implemented with
lazy and eager parallel-map skeletons and input slicing was used to
regulate task sizes. This flattening of task sizes diminished the
need for lazy work stealing. As a result, the fault tolerant eager
scheduling achieved better speedup's at scale. A speedup of 114 was
achieved using explicit scheduling of Sum Euler executed on Beowulf
using 224 cores, and 68 with lazy scheduling. A speedup of 146 was
achieved using explicit scheduling of Summatory Liouville, and 94 with
lazy scheduling. A speedup of 757 was achieved using explicit
scheduling of Summatory Liouville on HECToR using 1400 cores, and 340
with lazy scheduling.

The Mandelbrot benchmark is implemented with lazy and eager map-reduce
skeletons. There is no clear distinction between the lazy and eager
skeletons executed on Beowulf. The speedup of all four skeletons peaks
at either 196 or 224 cores --- between 57.4 and 60.2. A contrast
between lazy and eager scheduling is apparent in the HECToR speedup
measurements. Runtimes improve for the eager skeletons, continuing
up to 560 cores with a speedup of 88.5. A modest speedup from 560 to
1400 cores is observed thereafter. The lazy skeleton no longer
exhibits significant speedup after 140 cores, peaking with a speedup
of 50.5.

Fibonacci was implemented using the lazy divide-and-conquer
skeletons. The runtimes for ~parDnCFT~ and very similar to ~parDnC~ at
all scales. As with Mandelbrot, Fibonacci demonstrates that
hierarchically nested supervision costs are negligible. The speedup of
both the fault tolerant and non-fault tolerant skeletons peak at 196
core on Beowulf, at 54.4 and 52.1 respectively.

Four benchmarks are used to assess the resiliency of the parallel-map,
map-reduce and divide-and-conquer skeletons in the presence of
randomised Chaos Monkey failure. The executions with fault occurrence
were executed on Beowulf --- a platform that, unlike HECToR, supports
the UDP peer discovery in the fault detecting TCP-based HdpH-RS
transport layer. The key observation from these results is that lazy
scheduling reduces recovery costs in the presence of frequent
failure. Lazy skeletons generate sparks that are only scheduled
elsewhere if requested through on-demand work stealing. This laziness
minimises the migration of sparks, taking place only when nodes become
idle. Eager skeletons preemptively distribute work from the task
generating nodes, regardless of work load of other nodes. Lazy
scheduling therefore minimises the number of tasks that need to be
recovered when failures are detected.

For example, the Queens unit test generates $65234$ tasks. Using the
eager ~pushDnCFT~ skeleton results in the re-scheduling of $40696$
threads in an execution during which 8 of the initial 10 nodes
fail. This leads to an increased runtime of 649 seconds compared to
the mean 15 seconds of 5 failure-free ~pushDnC~ runtimes. In contrast,
an execution using the lazy ~parDnCFT~ skeleton in the presence of 9
node failures results in the re-scheduling of only $8$ sparks. This
leads to a much lower recovery overhead with the runtime of 52
seconds, compared to the mean 28 seconds of 5 failure-free ~parDnC~
runtimes.

* Conclusion

** Summary                                                          

New approaches to language design and system architecture are needed
to address the growing issue of fault tolerance for massively parallel
heterogeneous architectures. This thesis investigates the challenges
of providing reliable scalable symbolic computing. It has culminated
in the design, validation and implementation of a \textbf{R}eliable
\textbf{S}cheduling extension called HdpH-RS.

Chapter [[Related Work]] presents a critical review of fault tolerance in
distributed computing systems, defining dependability terms (Section
[[Dependability of Distributed Systems]]) and common existing approaches
such as checkpointing, rollback and fault tolerant MPI implementations
(Section [[Classifications of Fault Tolerance Implementations]]). New
approaches for scalable fault tolerant language design are needed, and
a critique of these existing approaches is given. The SymGridParII
middleware and the symbolic computing domain is introduced in Section
[[SymGridParII]]. Symbolic computations are challenging to parallelise as
they have complex data and control structures, and both dynamic and
highly irregular parallelism. The HdpH realisation of the SymGridParII
design is described in Section [[HdpH]]. The language is a shallowly embedded
parallel extension of Haskell that supports high-level implicit and
explicit parallelism. To handle the complex nature of symbolic
applications, HdpH supports dynamic and irregular parallelism.

The design of a supervised workpool construct
\cite{DBLP:conf/sfp/StewartTM12} (Appendix [[Supervised Workpools]]) is
influenced by supervision techniques in Erlang, and task replication
in Hadoop (Sections [[Erlang]] and [[MapReduce]]). The workpool hides task
scheduling, failure detection and task replication from the
programmer. For benchmark kernels the supervision overheads are low in
the absence of failure, between 2% and 7%, and the increased runtimes
are also acceptable in the presence of a single node failure in a ten
node architecture, between 8% and 10% (Section [[Supervised Workpool
Evaluation]]).

The design of HdpH-RS is an elaboration of the supervised workpools
prototype, most notably adding support for fault tolerant work
stealing. The concept of pairing one task with one value in the
supervised workpool is strengthened with the introduction of the spawn
family of primitives (Section [[HdpH-RS Programming Primitives]]). The
~spawn~ and ~spawnAt~ primitives are implemented using the existing
HdpH primitives ~new~, ~glob~, ~spark~, ~pushTo~, and ~rput~. These
spawn primitives were later added to the HdpH language
\cite{comlan-special-issue}. The APIs of two new fault tolerant
primitives ~supervisedSpawn~ and ~supervisedSpawnAt~ in HdpH-RS are
identical to the non-fault tolerant versions, providing opt-in fault
tolerance to the programmer.

To support these fault tolerance primitives, a reliable scheduler has
been designed and verified. The scheduler is designed to handle
single, simultaneous and random failures. The scheduling algorithm
(Section [[Fault Tolerant Scheduling Algorithm]]) is modeled as a Promela
abstraction in Section [[Promela Model of Fault Tolerant Scheduling]], and
verified with the SPIN model checker. The abstraction includes four
nodes. The immortal supervisor node is a node that creates a
supervised spark and a supervised future with ~supervisedSpawn~. Three
other nodes compete for the spark using work stealing and are mortal:
they can fail at any time. The supervised future is initially
empty. The key property of the model is that in all possible
intersections of mortal node failure, the supervised empty future will
nevertheless eventually be full. This property is expressed using
linear temporal logic formulae. The four nodes are translated in to a
finite automaton. The SPIN model checker is used to exhaustively
search the intersection of this automaton to validate that the key
reliability property holds in all unique model states. This it does
having exhaustively searched approximately 8.22 million unique states
of the Promela abstraction of the HdpH-RS fishing protocol, at a depth
of 124 from the initial state using 85Mb memory for states (Section
[[Model Checking Results]]).

The verified scheduler design has been implemented in Haskell. A new
transport layer for the HdpH-RS scheduler (Section [[Fault Detecting
Communications Layer]]) propagates TCP errors when connections are lost
between nodes. The transport layer was later merged in to the public
release 0.0 of HdpH \cite{hdph-0.0-release}. It informs all nodes of
remote node failure, and each node is responsible for ensuring the
safety of the futures it supervises. Using the spawn family (Section
[[HdpH-RS Primitives]]), 10 algorithmic skeletons have been developed
(Section [[Fault Tolerant Parallel Skeletons]]) to provide high level
parallel patterns of computation. Fault tolerant load-balancing and
task recovery is masked from the programmer.  In extending HdpH, 1
module is added for the fault tolerant strategies, and 14 modules are
modified. This amounts to an additional 1271 lines of Haskell code in
HdpH-RS, an increase of 52%. The increase is attributed to fault
detection, fault recovery and task supervision code.

The small-step operational semantics for HdpH-RS (Section [[Operational
Semantics]]) extends the HdpH operational semantics to include the spawn
family, and supervised scheduling and task recovery transitions. They
provide a concise and unambiguous description of the scheduling
transitions in the absence and presence of failure, and the states of
supervised sparks and supervised futures. The transition rules are
demonstrated with one fault-free execution, and three executions that
recover and evaluate tasks in the presence of faults (Section
[[Execution of Transition Rules]]). The execution of the operational
semantics in Figure
\ref{fig:supervisedSpawn-execution-faults-multiple-rputs} demonstrates
that scenarios that non-deterministically race pure computations is
indistinguishable from fault-free execution, thanks to the idempotent
side effect property.

The performance of HdpH-RS in the absence and presence of faults is
evaluated on 244 cores of a 32 node COTS architecture i.e. a Beowulf
cluster. The scalability of HdpH-RS is measured on 1400 cores of a HPC
architecture called HECToR (Chapter [[Fault Tolerant Programming &
Reliable Scheduling Evaluation]]). Speedup results show that when task
sizes are regular, eager scheduling achieves shorter runtimes than
lazy on-demand scheduling. A speedup of 757 was achieved using
explicit scheduling of Summatory Liouville on HECToR using 1400 cores,
and 340 with lazy scheduling. The scalability of hierarchically nested
supervision is demonstrated with Mandelbrot implemented with a
MapReduce parallel skeleton. Scaling Mandelbrot to 560 cores results
in a speedup of 89. The supervision overheads observed when comparing
fault tolerant and non-fault tolerant implementations are marginal on
the Beowulf cluster and negligible on HECToR at all scales. This
demonstrates that HdpH-RS scales to distributed-parallel architectures
using both flat and hierarchically nested supervision.

When failure occurrence is frequent, lazy on-demand scheduling is a
more appropriate work distribution strategy. Simultaneous failures
(Section [[Simultaneous Multiple Failures]]) and Chaos Monkey failures
(Section [[Chaos Monkey]]) are injected to measure recovery costs. Killing
5 nodes in a 20 node COTS architecture at varying stages of a
Summatory Liouville execution increases runtime up to 22% when
compared to failure free execution. When nodes are killed towards the
end of expected computation time, the recovery overheads are
negligible with lazy supervised work stealing, as most tasks have by
then been evaluated. An instance of injecting failure with Chaos
Monkey killed 80% of nodes whilst executing the Queens benchmark also
reveal the advantages of lazy on-demand work stealing when failure is
the common case and not the exception. The runtime using the two
remaining nodes of the initial ten was 52 seconds, compared with the
mean runtime of 28 seconds for failure-free runtime with those nodes.

** Limitations                                                      

HdpH-RS is limited in the kinds of recoverable computations, and also
the built-in assumptions of the communications layer. The recovery of
computations is restricted to expressions with idempotent side effects
i.e. side effects whose repetition cannot be observed. In the 5
benchmarks used in the evaluation (Chapter [[Fault Tolerant Programming
& Reliable Scheduling Evaluation]]) all computations are pure, and
therefore idempotent. This contrasts with Erlang, which supports the
recovery of non-idempotent stateful computation with the task restart
parameters of the supervision behaviour API in Erlang OTP.

Unlike the design of a fault tolerant GdH
\cite{Trinder00runtimesystem}, the communications layer in HdpH-RS
does not support the addition of nodes during the execution of a
single program. Nodes may be lost, and the program will nevertheless
be executed provided the root node does not fail. So there may be
fewer nodes in the distributed virtual machine at the end of program
execution, but not more.

The HdpH-RS communications layer instantiates a fully connected graph
of nodes at the start of program execution. Every node is connected
via a TCP connection to every other node, a requirement for the fault
detection mechanism in HdpH-RS. This topology will be less common in
future network infrastructures, where networks will be hierarchically
structured or where connections may be managed e.g. set up only on
demand.

** Future Work

**** Fault Tolerant Distributed Data Structures

The focus of this thesis is fault tolerant /big
computation/. This is in contrast to fault tolerant /big data/
solutions such as MapReduce implementation like Hadoop.  The use of
distributed data structures is currently out-of-scope for HdpH and
HdpH-RS. In order to be regarded a truly distributed programming
language, HdpH would need a need to support the persistence of
distributed data. Examples are MNesia
\cite{DBLP:conf/padl/MattssonNW99} for Erlang, or HDFS
\cite{DBLP:books/daglib/0029284} for Hadoop. Resilient distributed
data structures is left as future work.

A technique for limiting the cost of failure is memoization. This is
especially suited for divide and conquer programming, but is not
featured in the divide and conquer skeletons in HdpH-RS. One approach
is to use a globalised transposition table to store the values of
executed tasks \cite{DBLP:journals/ijhpca/WrzesinskaNMKB06}. When
tasks are recovered in the presence of failure, this table is first
used to check for a result, before rescheduling each task. Supervisors
near the top have descendent children spawning potentially many tasks,
some of which may have been executed at the point of failure. The use
of transposition tables can avoid the potentially costly re-scheduling
of these completed tasks. A distributed data structure is a
pre-requisite for a transposition table for the divide and conquer
skeletons in HdpH-RS.

**** Generalised Fault Detecting Transport Layer

The communications layer in HdpH-RS relies on a number of assumptions
about the underlying TCP based transport layer. The failure detection
exploits the TCP protocol, when transmission attempts to lost
endpoints propagates exceptions. In order for nodes to catch these
exceptions, a fully connected graph is needed. A generalised
communications layer in HdpH-RS could be designed to remove the
dependency on a connection-oriented protocol like TCP to support for
example, failure detection using passive heartbeats with
connectionless protocols like UDP.

**** Passive Fault Tolerant Fishing Protocol

A less restrictive fault tolerant fishing protocol for HdpH-RS could
reduce the number of additional RTS messages needed for supervised
work stealing. The fishing protocol in HdpH-RS is the culmination of
iterative design guided by the goal of eliminating unlikely, if
possible, race conditions identified by the SPIN model checker. The
goal for HdpH-RS was not therefore solely to achieve good runtime
performance, but to implement a formally verified fishing
protocols. Future work could explore an alternative fishing protocol
that demotes the supervision intervention to a wholly observational
role that can nevertheless be exhaustively verified with SPIN using
the key resiliency property defined in this thesis.

**** Data Structure Performance Tuning

Future versions of HdpH-RS could adopt two recent Haskell
technologies. One is a compare-and-swap primitive on top of which
lock-free data structures have been developed, including a lock-free
work stealing deque. The ~monad-par~ library will soon adopt
(September 2013) a Chase & Lev deque
\cite{DBLP:conf/spaa/ChaseL05} as a lock-free work stealing data
structure for threadpools. The author has collaborated with the
~atomic-primops~ developer whilst exploring its use in HdpH, resulting
a GHC bug being uncovered in GHC 7.6
\cite{atomic-primops-bug}. Adopting the Chase & Lev deque for
sparkpools in HdpH is a possibility beyond November 2013, once GHC 7.8
is released. The second is a new multithreaded IO manager for GHC
\cite{mio-ghc}, which has been shown to improve multithreaded
CloudHaskell performance on multicore hosts. It was motivated by
current bottlenecks in network-scale applications. The new IO manager
may lower multithreaded contention between HdpH-RS schedulers and
message handlers on future architectures with thousands of cores per
node.

**** Future Architectures

The scalability and recovery performance of HdpH-RS has been measured
on a COTS and an HPC architecture with assumptions about failures
e.g. that transient failure is nevertheless treated as a permanent
loss of service, and of the use cases of these architecture e.g. job
managers that deploy jobs on to reserved idle resources.

Resource sharing is increasingly common e.g. virtualised nodes on
physical hosts, so node load profiles may fluctuate unpredictably
during long-running massively scaled computation. An extension to
HdpH-RS could incorporate functionality to forcibly remove otherwise
healthy yet overloaded nodes to eliminate unresponsive compute
resources. The existing fault tolerance in HdpH-RS would handle such
node exits just as if connectivity was lost through permanent
failure. Furthermore, the demonstrated safety of racing pure tasks
could be used not only for tolerating faults, but also for replicating
tasks on unresponsive or overloaded nodes where the effect is
indistinguishable from that of no replication, thanks to the
idempotence property.

The dynamic scalability of cloud computing infrastructures has not
been addressed in this thesis. For example, when a HdpH-RS node fails
it can no longer participate in the execution of the job it was
detached from. Future HPC use cases may involve the dynamic scale-up
and scale-down of compute resources either as budgets allow or as
application needs change. Future work would be to adapt HdpH-RS to
support the dynamic allocation of nodes to provide an elastic fault
tolerant scalable task scheduler for scaling patterns in cloud
computing architectures.

\bibliographystyle{plain}
\bibliography{thesis}


#+LATEX: \appendix

* Appendix

** Supervised Workpools

This section presents a software level reliability mechanism, namely
supervised fault tolerant workpools implemented in HdpH. The workpool
is a feasibility study that influenced the designs of the HdpH-RS
scheduler and HdpH-RS fault tolerance primitives. The supervised
workpool concept of exposing fault tolerant API primitives later
influences the inception of the spawn family of primitives in HdpH-RS
(Section [[HdpH-RS Programming Primitives]]). Also, the fault detection
and task replication strategies used in the supervised workpool were
elaborated upon in HdpH-RS. To the best of the authors knowledge, this
was a novel construct at the time time of publication
\cite{DBLP:conf/sfp/StewartTM12} (June, 2012).

The design and implementation of a novel fault-tolerant workpool in
Haskell is presented (Sections [[Design of the Workpool]] and [[Workpool
Implementation]]) hiding task scheduling, failure detection and task
replication from the programmer. Moreover, workpools can be nested to
form fault-tolerant hierarchies, which is essential for scaling up to
massively parallel platforms. Two use cases in the presence of
failures are described in Section [[Use Case Scenarios]]. The
implementation of high-level fault tolerant abstractions on top of the
workpool are in Section [[Workpool High Level Fault Tolerant
Abstractions]]: generic fault tolerant skeletons for task parallelism
and nested parallelism, respectively. The concept of fault tolerant
parallel skeletons is expanded in HdpH-RS, which features 10 fault
tolerant skeletons (Section [[Fault Tolerant Parallel Skeletons]]).

The fault tolerant skeletons are evaluated using two
benchmarks. These benchmarks demonstrate fault tolerance --- 
computations do complete with a correct result in the presence of node
failures. Overheads of the fault tolerant skeletons are measured in
Section [[Supervised Workpool Evaluation]], both in terms of the cost of
book keeping, and in terms of the time to recover from failure.

The supervised workpools use a limited scheduling strategy. Tasks are
distributed preemptively, and not on-demand. Once tasks are received,
they are converted to threads immediately, sparks are not
supported. One elaboration in the HdpH-RS scheduler is a support for
supervised sparks, which brings with it a need for a fault tolerant
fishing protocol extension from HdpH.

*** Design of the Workpool

A workpool is an abstract control structure that takes units of work
as input, returning values as output. The scheduling of the work units
is coordinated by the workpool implementation. Workpools are a very
common pattern, and are often used for performance, rather than fault
tolerance. For example workpools can be used for limiting concurrent
connections to resources, to manage heavy load on compute nodes, and
to schedule critical application tasks in favour of non-critical
monitoring tasks \cite{erlang-workpools}. The supervised workpool
presented in this Section extends the workpool pattern by adding fault
tolerance to support reliable execution of HdpH
applications. The fault tolerant design is inspired by the supervision
behaviour and node monitoring aspects of Erlang (Section
[[Erlang]]), and combines this with Haskell's polymorphic, static typing.

Most workpools schedule work units dynamically, e.g. an idle worker
selects a task from the pool and executes it. For simplicity the
HdpH workpool uses static scheduling: each worker is given a
fixed set of work units (Section [[Workpool Scheduling]]). The supervised
workpool performs well for applications exhibiting regular
parallelism, and also for parallel programs with limited irregularity, as
shown in Section [[Supervised Workpool Evaluation]].

**** Concepts

Before describing the workpool in detail, parts of the HdpH API are
re-introduced in Table [[tab:workpool-terminology]], with terminology of
each primary concept with respect to the workpool. The concepts in
HdpH-RS (Section [[HdpH-RS Terminology]]) are a refinement of the
supervised workpool terms. First, in HdpH-RS there is less emphasis on
GIVar and their respective operations \texttt{glob} and \texttt{rput},
as these are demoted to internal HdpH-RS scheduling
functionality. Secondly, the definition of a /task/ in HdpH-RS is
refined to be a supervised spark or thread corresponding to a
supervised future, or a spark or thread corresponding to a future.

#+CAPTION: HdpH and workpool terminology
#+LABEL:   tab:workpool-terminology
#+ATTR_LaTeX: :mode table :align |l|p{10cm}|
|---------------------+-----------------------------------------------------------------------------------------------------------------------------|
| Concept             | Description                                                                                                                 |
|---------------------+-----------------------------------------------------------------------------------------------------------------------------|
| IVar                | A write-once mutable reference.                                                                                             |
| GIVar               | A global reference to an IVar, which is used to remotely write values to the IVar.                                          |
| Task                | Consists of an /expression/ and a /GIVar/. The expression is evaluated, and its value is written to the associated GIVar.   |
| Completed task      | When the associated GIVar in a /task/ contains the value of the task expression.                                            |
| Closure             | A Serializable expression or value. Tasks and values are serialised as closures, allowing them to be shipped to other nodes. |
| Supervisor thread   | The Haskell thread that has initialised the workpool.                                                                       |
| Process             | An OS process executing the GHC runtime system.                                                                             |
| Supervising process | The /process/ hosting the /supervisor thread/.                                                                              |
| Supervising node    | The node hosting the supervising process.                                                                                   |
| Worker node         | Every node that has been statically assigned a task from a given workpool.                                                  |
|---------------------+-----------------------------------------------------------------------------------------------------------------------------|

**** Workpool API

A fundamental principle in the HdpH supervised workpool is that there
is a one-to-one correspondence between a /task/ and an ~IVar~ --- each
task evaluates an expression to return a result which is written to
its associated ~IVar~. The /tasks/ are distributed as closures to
\emph{worker nodes}. The /supervisor thread/ is responsible for
creating and globalising \texttt{IVar}s, in addition to creating the
associated tasks and distributing them as closures. This one-to-one
binding between tasks and \texttt{IVar}s by the spawn family of
primitives in HdpH-RS (Section [[HdpH-RS Programming Primitives]]). Here
are the workpool types and the function for using it:

#+BEGIN_LATEX
{\footnotesize
\begin{haskellcodebare}{name=dummyapi}
type SupervisedTasks a  = [(Closure (IO ()), IVar a)]
supervisedWorkpoolEval :: SupervisedTasks a -> [NodeId] -> IO [a]
\end{haskellcodebare}
}
#+END_LATEX

The ~supervisedWorkpoolEval~ function takes as input a list of
tuples, pairing tasks with their associated \texttt{IVar}s, and a list
of \texttt{NodeId}s. The closured tasks are distributed to worker
nodes in a round robin fashion to the specified worker
\texttt{NodeId}s, and the workpool waits until all tasks are complete
i.e. all \texttt{IVar}s are full. If a node failure is identified
before tasks complete, the unevaluated tasks sent to the failed node
are reallocated to the remaining available nodes. Detailed
descriptions of the scheduling, node failure detection, and failure
recovery is in Section [[Workpool Implementation]].

**** Workpool Properties and Assumptions

The supervised workpool guarantees that given a list of /tasks/,
it will fully evaluate their result provided that:

1) The supervising node is alive throughout the evaluation of all
   tasks in the workpool.
2) All expressions are computable. For example, evaluating an
   expression should not throw uncaught exceptions, such as a division
   by 0; all programming exceptions such as non-exhaustive case
   statements must be handled within the expression; and so on.

The supervised workpool is non-deterministic, and hence is
monadic. This is useful in some cases such as racing the evaluation of
the same task on separate nodes. The write operations on
\texttt{IVar}s are relaxed in HdpH for fault tolerance, allowing two
tasks to attempt a write attempt. Either the first to complete
evaluation wins, or in the presence of failure the /surviving/ task
wins. The write semantics of \texttt{IVar}s are described in Section
[[Workpool Implementation]].

To recover determinism in the supervised workpool, expressions must be
/idempotent/. An idempotent expression may be executed more than once
which entails the same side effect as executing only once. E.g
inserting a given key/value pair to a mutable map - consecutive
inserts have no effect. Pure computations, because of their lack of
side effects, are of course idempotent.

Workpools are functions and may be freely nested and composed. There
is no restriction to the number of workpools hosted on a node, and
Section [[Workpool High Level Fault Tolerant Abstractions]] presents a
divide-and-conquer abstraction that uses this flexibility.

*** Use Case Scenarios

Figure \ref{fig:workpool-scenario1} shows a workpool scenario where six
closures are created, along with six associated \texttt{IVar}s. The
closures are allocated to three worker nodes: ~Node2, Node3~ and
~Node4~ from the supervising node, ~Node1~. Whilst these closures are
being evaluated, ~Node3~ fails, having completed only one of its two
tasks. As ~IVar i5~ had not been filled, closure ~c5~ is reallocated
to ~Node4~. No further node failures occur, and once all six
\texttt{IVar}s are full, the supervised workpool terminates. The
mechanisms for detecting node failure, for identifying completed
tasks, and the reallocation of closures are described in Section
[[Workpool Implementation]].

#+CAPTION:    Reallocating Closures Scenario 1
#+LABEL:      fig:workpool-scenario1
#+ATTR_LaTeX: :height 80mm :width 140mm
[[./img/supervised-workpools/use_cases/use-case1.pdf]]

The use case in Figure \ref{fig:workpool-scenario2} involves the
failure the /two/ nodes. The scenario is similar to Figure
\ref{fig:workpool-scenario1}, again involving four nodes, and initial task
placement is the same. This time, once ~Node2~ has evaluated ~c1~ and
~rput~ to ~i1~, it fails. The task ~c4~ has not been evaluated, and
therefore ~IVar~ ~i4~ is empty. The supervising node ~Node1~ detects
the failure of ~Node2~ and reallocated ~c4~ to the only remaining
available node, ~Node4~. Once all this node has evaluated all
allocated tasks (including ~c4~ from ~Node2~ and ~c5~ from ~Node3~,
all \texttt{IVar}s on ~Node1~ will be full, and STM will unblock,
terminating the workpool.

#+CAPTION:    Reallocating Closures Scenario 2
#+LABEL:      fig:workpool-scenario2
#+ATTR_LaTeX: :height 80mm :width 140mm
[[./img/supervised-workpools/use_cases/use-case2.pdf]]


*** Workpool Implementation

#+BEGIN_LATEX
\begin{Code}
\begin{haskellcode}{Types Signatures of HdpH Primitives}{lst:hdph-wp-primitives}
-- |HdpH primitives
type IVar a = TMVar a                              -- type synonym for IVar
data GIVar a                                       -- global handles to IVars
data Closure a                                     -- explicit, serialisable closures
pushTo :: Closure (IO ()) -> NodeId -> IO ()       -- explicit task placement
rput   :: GIVar (Closure a) -> Closure a -> IO ()  -- write a value to a remote IVar
get    :: IVar a -> IO a                           -- blocking get on an IVar
probe  :: IVar a -> STM Bool                       -- check if IVar is full or empty
\end{haskellcode}
\end{Code}
#+END_LATEX

The types of the relevant HdpH primitives are shown in Listing
\ref{lst:hdph-wp-primitives}. These primitives are used as the
foundation for the ~spawn~ family of primitives in HdpH-RS (Chapter
[[Designing a Fault Tolerant Programming Language for Distributed
Memory Scheduling]]). The complete fault tolerant workpool
implementation is available \cite{tfp-workpools-implementation}, and
the most important functions are shown in Listing
\ref{lst:workpool-implementation}. Two independent phases take place
in the workpool:

1) Line \ref{vrb:supervisedWorkpoolEval} shows the
   ~supervisedWorkpoolEval~ function which creates the workpool,
   distributes tasks, and then uses Haskell's Software Transactional
   Memory (STM) library \cite{STM}
   as a termination check described in item \ref{item:stm}. The
   ~distributeTasks~ function on line \ref{vrb:distributeTasks} uses
   ~pushTo~ (from Listing \ref{lst:hdph-wp-primitives}) to ship tasks
   to the worker nodes and creates ~taskLocations~, an instance of
   \texttt{TaskLookup~a} (line \ref{vrb:TaskLookup}). This is a
   mutable map from \texttt{NodeId}s to \texttt{SupervisedTasks~a} on
   line \ref{vrb:SupervisedTasks}, which is used for the book keeping
   of task locations. The ~monitorNodes~ function on lines
   \ref{vrb:monitorNodes} - \ref{vrb:reallocate} then monitors worker
   node availability. Should a worker node fail,
   ~blockWhileNodeHealthy~ (line \ref{vrb:blockHealthy}) exits, and
   ~reallocateIncompleteTasks~ on line \ref{vrb:reallocate} is used to
   identify incomplete tasks shipped to the failed node, using ~probe~
   (from Listing \ref{lst:hdph-wp-primitives}). These tasks are
   distributed to the remaining available nodes.

2) \label{item:stm} STM is used as a termination check. For every
   task, an ~IVar~ is created. A ~TVar~ is created in the workpool to
   store a list of values returned to these \texttt{IVar}s from each
   task execution. The ~getResult~ function on line
   \ref{vrb:getResult} runs a /blocking/ \texttt{get} on each
   \texttt{IVar}, which then writes this value as an element to the
   list in the ~TVar~. The ~waitForResults~ function on line
   \ref{vrb:waitForResults} is used to keep phase 1 of the supervised
   workpool active until the length of the list in the ~TVar~ equals
   the number of tasks added to the workpool.

#+BEGIN_LATEX
\begin{Code}
\begin{haskellcode}{Workpool Implementation and Use of STM as a Termination Check}{lst:workpool-implementation}
-- |Workpool types
type SupervisedTasks a  = [(Closure (IO ()), IVar a)] @\label{vrb:SupervisedTasks}@
type TaskLookup a       = MVar (Map NodeId (SupervisedTasks a)) @\label{vrb:TaskLookup}@

supervisedWorkpoolEval :: SupervisedTasks a -> [NodeId] -> IO [a] @\label{vrb:supervisedWorkpoolEval}@
supervisedWorkpoolEval tasks nodes = do
       -- PHASE 1
       -- Ship the work, and create an instance of 'TaskLookup a'.
   taskLocations <- distributeTasks tasks nodes @\label{vrb:distributeTasks}@
       -- Monitor utilized nodes; reallocate incomplete tasks when worker nodes fail
   monitorNodes taskLocations

       -- PHASE 2
       -- Use STM as a termination check. Until all tasks are evaluated,
       -- phase 1 remains active.
   fullIvars <- newTVarIO []
   mapM_ (forkIO . atomically . getResult fullIvars . snd) tasks
   results <- atomically $ waitForResults fullIvars (length tasks)

       -- Finally, return the results of the tasks
   return results

monitorNodes :: TaskLookup a -> IO () @\label{vrb:monitorNodes}@
monitorNodes taskLookup = do
   nodes <- fmap Map.keys $ readMVar taskLookup
   mapM_ (forkIO . monitorNode) nodes @\label{vrb:forkMonitorNode}@
   where
    monitorNode :: NodeId -> IO ()
    monitorNode node = do
      blockWhileNodeHealthy node -- Blocks while node is healthy (Used in Listing @\ref{lst:transport-layer-primitives}@) @\label{vrb:blockHealthy}@
      reallocateIncompleteTasks node taskLookup -- reallocate incomplete tasks
                                                -- shipped to 'node' @\label{vrb:reallocate}@

-- |Takes an IVar, runs a blocking 'get' call, and writes
--  the value to the list of values in a TVar
getResult :: TVar [a] -> IVar a -> STM () @\label{vrb:getResult}@
getResult values ivar = do
    v  <- get ivar
    vs <- readTVar values
    writeTVar results (vs ++ [v]) @\label{code:stm-writeTVar}@

-- |After each write to the TVar in 'evalTask', the length of the list
--  is checked. If it matches the number of tasks, STM releases the block.
waitForResults :: TVar [a] -> Int -> STM [a] @\label{vrb:waitForResults}@
waitForResults values i = do
    vs <- readTVar values
    if length vs == i then return vs else retry @\label{code:stm-retry}@
\end{haskellcode}
\end{Code}
#+END_LATEX

The restriction to idempotent tasks in the workpool (Section
[[Workpool Properties and Assumptions]]) enables the workpool to
freely duplicate and re-distribute tasks. Idempotence is permitted by
the write semantics of \texttt{IVar}s. The first write to an ~IVar~
succeeds, and subsequent writes are ignored --- successive ~rput~
attempts to the same ~IVar~ are non-fatal in HdpH. To support this,
the write-once semantics of \texttt{IVar}s in the ~Par~ monad
\cite{DBLP:conf/haskell/MarlowNJ11} are relaxed slightly in HdpH, to
support fault tolerance. This enables identical closures to be raced
on separate nodes. Should one of the nodes fail, the other evaluates
the closure and \texttt{rput}s the value to the associated
~IVar~. Should the node failure be intermittent, and a successive
~rput~ be attempted, it is silently ignored. It also enables
replication of closures residing on overloaded nodes to be duplicated
on to healthy nodes.

It would be unnecessary and costly to reallocate tasks if they had
been fully evaluated prior to the failure of the worker node it was
assigned to. For this purpose, the ~probe~ primitive is used to
identify which \texttt{IVar}s are full, indicating the evaluation
status of its associated task. As such, all \texttt{IVar}s that
correspond to tasks allocated to the failed worker node are
/probed/. Only tasks associated with empty \texttt{IVar}s are
reallocated as closures.

**** Node Failure Detection

A new transport layer for distributed Haskells
\cite{new-cloud-haskell} underlies the fault tolerant workpool. The
main advantage of adopting this library is the typed error messages at
the Haskell language level. The author contributed to this library as
a collaboration with Edsko de Vries and Duncan Coutts on the failure
semantics of the transport API \cite{nt-discussion}, and through
identifying subtle race conditions in the TCP implementation
\cite{nt-tcp-fix}. The implementation and testing phases coincided
with the migration of HdpH over to this TCP based transport API.

#+BEGIN_LATEX
\begin{Code}
\begin{haskellcode}{Detecting Node Failure in the \texttt{blockWhileNodeHealthy} Function}{lst:transport-layer-primitives}
  -- connection attempt
attempt <- @\textbf{connect myEndPoint remoteEndPointAddress}@ @\emph{<default args>}@
case attempt of
  (Left (TransportError ConnectFailed)) -> -- react to failure, in Listing @\ref{lst:workpool-implementation}@ line @\ref{vrb:blockHealthy}@
  (Right connection) ->                    -- otherwise, carry on.
\end{haskellcode}
\end{Code}
#+END_LATEX

The ~connect~ function from the transport layer is shown in Listing
\ref{lst:transport-layer-primitives}. It is used by the workpool to
detect node failure in the ~blockWhileNodeHealthy~ function on line
\ref{vrb:blockHealthy} of Listing
\ref{lst:workpool-implementation}. Each node creates an endpoint, and
endpoints are connected to send and receive messages between the
nodes. Node availability is determined by the outcome of connection
attempts using \texttt{connect} between the node hosting the
supervised workpool, and each worker node utilized by that
workpool. The transport layer ensures lightweight communications by
reusing the underlying TCP connection. One logical connection attempt
between the supervising node and worker nodes is made each second. If
~Right Connection~ is returned, then the worker node is healthy and no
action is taken. However, if
\texttt{Left~(TransportError~ConnectFailed)} is returned then the
worker node is deemed to have failed, and ~reallocateIncompleteTasks~
(Listing \ref{lst:workpool-implementation}, line \ref{vrb:reallocate})
re-distributes incomplete tasks originally shipped to this
node. Concurrency for monitoring node availability is achieved by
Haskell IO threads on line \ref{vrb:forkMonitorNode} in Listing
\ref{lst:workpool-implementation}.

An alternative to this design for node failure detection was
considered --- with periodic heartbeat messages sent from the worker
nodes to the process hosting the supervised workpool. However the
bottleneck of message delivery would be the same i.e. involving the
endpoint of the process hosting the workpool. Moreover, there are
dangers with timeout values for expecting heartbeat messages in
asynchronous messaging systems such as the one employed by
HdpH. Remote nodes may be wrongly judged to have failed e.g. when the
message queue on the workpool process is flooded, and heartbeat
messages are not popped from the message queue within the timeout
period. Our design avoids this danger by synchronously checking each
connection.

Each workpool hosted on a node must monitor the availability of worker
nodes. With nested or composed supervised workpools there is a risk
that the network will be saturated with ~connect~ requests to monitor
node availability. Our architecture avoids this by creating just one
Haskell thread per node that monitors availability of all other nodes,
irrespective of the number of workpools hosted on a node. Each
supervisor thread communicates with these monitoring threads to
identify node failure. See the complete implementation
\cite{tfp-workpools-implementation} for details.

*** Workpool Scheduling

One limitation of our supervised workpool is the naive scheduling of
tasks between nodes. The strategy is simple --- it eagerly distributes
the tasks in the workpool between the nodes in a round robin fashion
over the ordering of nodes allocated to the workpool. Recall the
workpool primitive from Section [[Design of the Workpool]]:

#+BEGIN_LATEX
{\footnotesize
\begin{haskellcodebare}{name=dummy}
supervisedWorkpoolEval :: SupervisedTasks a -> [NodeId] -> IO [a]
\end{haskellcodebare}
}
#+END_LATEX

It is up to the programmer to order the \texttt{NodeId}s in a sensible
sequence. The ordering in the algorithmic skeletons in Section
[[Workpool High Level Fault Tolerant Abstractions]] is sensitive to each
skeleton. A suitable node order must be selected for each of the
skeletons in Section [[Workpool High Level Fault Tolerant
Abstractions]]. For example, the list of nodes IDs passed to the
parallel map are simply sorted in order, and then given to the
workpool. In the divide and conquer skeleton, the \texttt{NodeId}s are
first of all randomised, and are then assigned to the workpool. This
avoids overloading nodes near the head of a sorted list of
\texttt{NodeId}s.

The workpool is eager and preemptive in its scheduling. HdpH-RS has
more sophisticated scheduling strategies than the workpool in this
chapter, though the recovery and failure detection techniques in
HdpH-RS are heavily influenced by the workpool.

*** Workpool High Level Fault Tolerant Abstractions

It is straight forward to implement algorithmic skeletons in HdpH
\cite{Maier_Trinder_IFL2011}. This present work extends these by
adding resilience to the execution of two generic parallel algorithmic
skeletons.

HdpH provides high level coordination abstractions: evaluation
strategies and algorithmic skeletons. The advantages of these
skeletons are that they provide a higher level of abstraction
\cite{strategies} that capture common parallel
patterns, and that the HdpH primitives for work distribution and
operations on \texttt{IVar}s are hidden away from the programmer.

The following shows how to use fault tolerant workpools to add
resilience to algorithmic skeletons. Listing
\ref{lst:workpool-skeletons} shows the type signatures of fault
tolerant versions of the following two generic algorithmic skeletons.

- pushMap :: A parallel skeleton that provides a parallel map
    operation, applying a function closure to the input list.
- pushDivideAndConquer :: Another parallel skeleton that allows a
  problem to be decomposed into sub-problems until they are sufficiently
  small, and then reassembled with a combining function.

\texttt{IVar}s are globalised and closures are created from tasks in
the skeleton code, and ~supervisedWorkpoolEval~ is used at a lower
level to distribute closures, and to provide the guarantees described
in Section [[Workpool Implementation]]. The tasks in the workpool are
eagerly scheduled into the threadpool of remote nodes. The two
algorithmic skeletons have different scheduling strategies ---
~pushMap~ schedules tasks in a round-robin fashion;
~pushDivideAndConquer~ schedules tasks randomly (but statically at the
beginning, not on-demand).

#+BEGIN_LATEX
\begin{Code}
\begin{haskellcode}{Fault Tolerant Algorithmic Parallel Skeletons}{lst:workpool-skeletons}
pushMap
   :: [NodeId]         -- available nodes
   -> Closure (a -> b) -- function closure
   -> [a]              -- input list
   -> IO [b]           -- output list

pushDivideAndConquer
   :: [NodeId]                                        -- available nodes
   -> Closure (Closure a -> Bool)                     -- trivial
   -> Closure (Closure a -> IO (Closure b))           -- simplySolve
   -> Closure (Closure a -> [Closure a])              -- decompose
   -> Closure (Closure a -> [Closure b] -> Closure b) -- combine
   -> Closure a                                       -- problem
   -> IO (Closure b)                                  -- output
\end{haskellcode}
\end{Code}
#+END_LATEX

*** Supervised Workpool Evaluation

This section present a performance evaluation of the workpools, in
both the absence and presence of faults. The fault tolerant parallel
skeletons from Section [[Workpool High Level Fault Tolerant Abstractions]]
are used to implement a data parallel benchmark and a divide and
conquer benchmark.

**** Data Parallel Benchmark

To demonstrate the \texttt{pushMap} data parallel skeleton (Listing
\ref{lst:workpool-skeletons}), Summatory Liouville
\cite{DBLP:journals/moc/BorweinFM08} has been implemented in HdpH,
adapted from existing Haskell code
\cite{DBLP:conf/popl/ZainHBTMA09}. The Liouville function
$\lambda(n)$ is the completely multiplicative function defined by
$\lambda(p) = -1$ for each prime $p$. $L(n)$ denotes the sum of the
values of the Liouville function $\lambda(n)$ up to $n$, where $L(n)
:= \sum_{k=1}^{n} \lambda(k)$. The \emph{scale-up} runtime results
measure Summatory Liouville $L(n)$ for
$n=[10^{8},2\cdot10^{8},3\cdot10^{8}..10^{9}]$. Each experiment is run
on 20 nodes with closures distributed in a round robin fashion, and
the chunk size per closure is $10^{6}$. For example, calculating
$L(10^{8})$ will generate 100 tasks, allocating 5 to each
node. On each node, a partial Summatory Liouville value is further
divided and evaluated in parallel, utilising multicore
support in the Haskell runtime \cite{DBLP:conf/icfp/MarlowJS09}.

**** Control Parallel Benchmark using Nested Workpools

The ~pushDivideAndConquer~ skeleton (Listing
\ref{lst:workpool-skeletons}) is demonstrated with the implementation
of Fibonacci. This example illustrates the flexibility of the
supervised workpool, which can be nested hierarchically in
divide-and-conquer trees. At the point when a closure is deemed too
computationally expensive, the problem is decomposed into
sub-problems, turned into closures themselves, and pushed to other
nodes. In the case of Fibonacci, costly tasks are decomposed into two
smaller tasks, though the ~pushDivideAndConquer~ skeleton permits any
number of decomposed tasks to be supervised.

A sequential threshold is used to determine when sequential evaluation
should be chosen. For example, if the sequential threshold is 8, and
the computation is \texttt{fib~10}, then \texttt{fib~9} will be
further decomposed, whilst \texttt{fib~8} will be evaluated sequentially.

The runtime results measure Fibonacci $Fib(n)$ for $n=[45..55]$, and
the sequential threshold for each $n$ is 40. Unlike the
~pushMap~ skeleton, closures are distributed to random nodes
from the set of available nodes to achieve fairer load balancing.

**** Benchmark Platform

The two applications were benchmarked on a Beowulf cluster. Each
Beowulf node comprises two Intel quad-core CPUs (Xeon E5504) at 2GHz,
sharing 12GB of RAM. Nodes are connected via Gigabit Ethernet and run
Linux (CentOS 5.7 64bit). HdpH version 0.3.2 was used and the benchmarks
were built with GHC 7.2.1. Benchmarks were run on 20 cluster nodes; to
limit variability only 6 cores per node were used. Reported runtime is
median wall clock time over 20 executions, and reported error is the
range of runtimes.

**** Supervised Workpool Performance

***** No Failure

The runtimes for Summatory Liouville are shown in Figure
\ref{fig:runtime-sumLiouville-nofailures}. The chunk size is fixed, increasing
the number of supervised closures as $n$ is increased in
$L(n)$. The overheads of the supervised workpool for Summatory
Liouville are shown in Figure \ref{fig:supervision-sumLiouville-nofailures}. The
runtime for Fibonacci are shown in Figure \ref{fig:runtime-fib-nofailures}.

#+CAPTION:    Runtime with no failures for Summatory Liouville $10^8$ to $10^9$
#+LABEL:      fig:runtime-sumLiouville-nofailures
#+ATTR_LaTeX: :width 100mm :height 70mm :placement [H]
[[./img/supervised-workpools/results/sumLiouville/sumLiouville-runtimes-nofailures.pdf]]

#+CAPTION:    Supervision overheads with no failures for Summatory Lioville $10^8$ to $10^9$
#+LABEL:      fig:supervision-sumLiouville-nofailures
#+ATTR_LaTeX: :width 90mm :placement [H]
[[./img/supervised-workpools/results/sumLiouville/overheads/sumLiouville-overheads-nofailures.pdf]]

#+CAPTION:    Runtime with no failure for Fibonacci
#+LABEL:      fig:runtime-fib-nofailures
#+ATTR_LaTeX: :width 100mm :height 70mm :placement [H]
[[./img/supervised-workpools/results/fib/fib-runtimes-nofailures.pdf]]

The supervision overheads for Summatory Liouville range between 2.5%
at $L(10^{8})$ and 7% at $L(5\cdot10^{8})$. As the problem size grows
to $L(10^{9})$, the number of generated closures increases with the
chunk size fixed at $10^{6}$. Despite this increase in supervised
closures, near constant overheads of between 6.7 and 8.4 seconds are
observed between $L(5\cdot10^{8})$ and $L(10^{9})$.

Overheads are not measurable for Fibonacci, as they are lower than
system variability, probably due to random work distribution.

The runtime for calculating $L(5\cdot10^{8})$ is used to verify the
scalability of the HdpH implementation of Summatory Liouville. The
median runtime on 20 nodes, each using 6 cores, is 95.69 seconds, and
on 1 node using 6 cores is 1711.5 seconds, giving a speed up of 17.9
on 20 nodes.

***** Recovery From Failure

To demonstrate the fault tolerance and to assess the efficacy of the
supervised workpool, recovery times have been measured when one node
dies during the computation of Summatory Liouville. Nested workpools
used by ~pushDivideAndConquer~ also tolerate faults. Due to the size
of the divide-and-conquer graph for large problems, they are harder to
analyse in any meaningful way.

To measure the recovery time, a number of parameters are fixed. The
computation is $L(3\cdot10^{8})$ with a chunk size of $10^{6}$, which
is initially deployed on 10 nodes, with one hosting the supervising
task. The ~pushMap~ skeleton is used to distribute closures in a round
robin fashion, so that 30 closures are sent to each node. An expected
runtime utilising 10 nodes is calculated from 5 failure-free
executions. From this, approximate timings are calculated for
injecting node failure. The Linux ~kill~ command is used to forcibly
terminate one running Haskell process prematurely.

#+CAPTION:    Recovery time with 1 node failure
#+LABEL:      fig:workpool-recovery-time
#+ATTR_LaTeX: :width 100mm :height 80mm :placement [H]
[[./img/supervised-workpools/results/sumLiouville/sumLiouvilleFaults/sumLiouville-reallocation.pdf]]

#+CAPTION:    Recovery time with 1 node failure /with reallocated tasks/
#+LABEL:      fig:workpool-recovery-time-reallocated-tasks
#+ATTR_LaTeX: :width 100mm :height 80mm :placement [H]
[[./img/supervised-workpools/results/sumLiouville/sumLiouvilleFaults/sumLiouville-reallocation-overlay.pdf]]

The results in Figure \ref{fig:workpool-recovery-time} show the
runtime of the Summatory Liouville calculation when node failure
occurs at approximately [10%,20%..90%] of expected execution time. Five
runtimes are observed at each timing point. Figure
\ref{fig:workpool-recovery-time} also shows five runtimes using 10 nodes
when /no/ failures occur, and additionally five runtimes using 9 nodes,
again with no failures. Figure
\ref{fig:workpool-recovery-time-reallocated-tasks} reports an
additional dataset --- the average number of closures that are
reallocated relative to when node failure occurs. As described in
Section [[Workpool Implementation]], only non-evaluated closures are
redistributed. The expectation is that the longer the injected node
failure is delayed, the fewer closures will need reallocating
elsewhere.

The data shows that at least for the first 30% of the execution, no
tasks are complete on the node, which can be attributed to the time
taken to distribute 300 closures. Fully evaluated closure values are
seen at 40%, where only 16 (of 30) are reallocated. This continues to
fall until the 90% mark, when 0 closures are reallocated, indicating
that all closures had already been fully evaluated on the responsible
node.

The motivation for observing failure-free runtimes using 9 and also 10
nodes is to evaluate the overheads of recovery time when node failure
occurs. Figure \ref{fig:workpool-recovery-time} shows that when a node dies
early on (in the first 30% of estimated total runtime), the
performance of the remaining 9 nodes is comparable with that of a
failure-free run on 9 nodes. Moreover, node failure occurring near the
end of a run (e.g. at 90% of estimated runtime) does not impact
runtime performance, i.e. is similar to that of a 10 node cluster that
experiences no failures at all.


*** Summary

This chapter presents a language based approach to fault tolerant
distributed-memory parallel computation in Haskell: a fault tolerant
workpool that hides task scheduling, failure detection and task
replication from the programmer. Fault tolerant versions of two
algorithmic skeletons are developed using the workpool. They provide
high level abstractions for fault tolerant parallel computation on
distributed-memory architectures. To the best of the authors knowledge
the supervised workpool is a novel construct.

The workpool and the skeletons guarantee the completion of tasks even
in the presence of multiple node failures, withstanding the failure of
all but the supervising node. The supervised workpool has acceptable
runtime overheads --- between 2.5\% and 7\% using a data parallel
skeleton. Moreover when a node fails, the recovery costs are
negligible.

This marks the end of a feasibility study motivated by the claim that
HdpH has the potential for fault tolerance
\cite{Maier_Trinder_IFL2011}. The limitations of the supervised
workpools, described in Section [[Workpool Scheduling]], are that
scheduling is preemptive and eager. There is no load balancing. The
ideas of propagated fault detection from the transport layer, and
freely replicating idempotent tasks are taken forward into HdpH-RS in
Chapter [[Designing a Fault Tolerant Programming Language for
Distributed Memory Scheduling]].

** Programming with Futures

Programming with futures \cite{DBLP:journals/toplas/Halstead85} is a
simple programming abstraction for parallel scheduling. A future can
be thought of as placeholder for a value that is set to contain a real
value once that value becomes known, by evaluating the
expression. Futures are created with the HdpH-RS spawn family of
primitives (Section [[HdpH-RS Programming Primitives]]). Fault tolerant
futures are implemented with HdpH-RS using a modified version of
\texttt{IVar}s from HdpH, described in Section [[Implementing
Futures]].

This section compares functional futures in monad-par
\cite{DBLP:conf/haskell/MarlowNJ11}, Erlang RPC
\cite{DBLP:books/daglib/0022920}, and the CloudHaskell Platform (CH-P)
\cite{ch-p} with HdpH and HdpH-RS. It shows that HdpH-RS is the only
language that provides futures whilst supporting load balancing on
distributed-memory architectures (Section [[Library Support for
Distributed Functional Futures]]. It also highlights the inconsistencies
between primitive naming conventions for programming with futures
(Section [[Primitive Names for Future Operations]]).

*** Library Support for Distributed Functional Futures

Table \ref{tab:futures-api-comparison} compares three features of
monad-par, CH-P, HdpH and HdpH-RS.

1) Whether the runtime system for these languages balance load
   between the different processing capabilities dynamically.
2) Whether the runtime system can be deployed over multiple nodes.
3) Support for serialising function closures (applies only to
   multiple-node deployments).

First, monad-par is designed for shared-memory execution with GHC
\cite{ghc} on one node, and load balancing between processor
cores. Erlang's RPC library supports multiple node distribution, but
does not support work stealing. CH-P is the same as Erlang RPC, but
does require additional Template Haskell code for explicit closure
creation. Finally, HdpH and HdpH-RS are designed for execution on
multiple nodes, and load balancing is between nodes /and/ between
cores on each node.

#+BEGIN_LATEX
\begin{table}
\begin{tabular}{|>{\footnotesize}l||>{\footnotesize}c >{\footnotesize}c |>{\footnotesize}c >{\footnotesize}c|}
\hline
API & Load Balancing & Multiple Nodes & Closure transmission & Serialisation \\
\hline
\hline
monad-par  & \tick          & \cross         & -                    & Not required              \\
Erlang RPC & \cross         & \tick          & \tick                & Built-in language support \\
CH-P       & \cross         & \tick          & \tick                & Template Haskell needed   \\
HdpH       & \tick          & \tick          & \tick                & Template Haskell needed   \\
HdpH-RS    & \tick          & \tick          & \tick                & Template Haskell needed   \\
\hline
\end{tabular}
\caption{Comparison of APIs for Programming with Futures}
\label{tab:futures-api-comparison}
\end{table}
#+END_LATEX

*** Primitive Names for Future Operations

Across these libraries (and many more
e.g. \cite{DBLP:conf/ppopp/BlumofeJKLRZ95}), primitives that serve the
same purpose unfortunately do not share identical naming
conventions. Worse, there are identical primitives names that have
different meanings. This section uses the six APIs from Section
[[Library Support for Distributed Functional Futures]] to compare
primitive names, and the presence or absence of key task and future
creation primitives in each API.

Table \ref{tab:task-creation} shows the primitives across the six
libraries used for task creation. One common name for task creation is
/spawn/. The ~spawn~ primitive in monad-par and HdpH-RS creates a
future and a /future task/ primed for lazy work stealing. The same
purpose is served by ~async~ in CH-P. The ~spawn/3~ and ~spawn/4~
Erlang primitives are different to both, this time meaning eager task
creation and with no enforced relationship to futures. /Arbitrary/
tasks are passed to ~spawn~ in Erlang --- there is no obligation to
return a value to the parent process.

#+BEGIN_LATEX
\begin{table}
\begin{center}
\begin{tabular}{|>{\small}p{2.7cm}||>{\small}c >{\small}c >{\small}c|>{\small}c|>{\small}c >{\small}c|}
\hline
Placement & monad-par & HdpH & HdpH-RS & Erlang & CH & CH-P \\
\hline
\hline
Local only       & \texttt{fork} & \texttt{fork} & \texttt{fork}  & \texttt{spawn/3} & \texttt{forkProcess} &      \\
Lazy             &           & \texttt{spark}  &         &           &               &      \\
Eager            &           & \texttt{pushTo} &         & \texttt{spawn/4} & \texttt{spawn}       &      \\
Eager \emph{blocking} &           &          &         & \texttt{call/4}  & \texttt{call}        &      \\
\hline
\end{tabular}
\end{center}
\caption{Task Creation}
\label{tab:task-creation}
\end{table}
#+END_LATEX

Table \ref{tab:future-creation} shows the primitives across the six
libraries used for future creation. The Erlang RPC library is for
programming with futures, the model of monad-par and HdpH. The
function ~rpc:async_call/4~ eagerly places future tasks, returning a
~Key~, synonymous with \texttt{IVar}s in monad-par or HdpH-RS. The
functions ~rpc:yield/1~, ~rpc:nb_yield/1~ and ~rpc:nb_yield/2~ are
used to read values from keys.


#+BEGIN_LATEX
\begin{table}
\begin{center}
\begin{tabular}{|>{\small}p{2.7cm}||>{\small}c >{\small}c >{\small}c|>{\small}c|>{\small}c >{\small}c|}
\hline
Placement & monad-par & HdpH & HdpH-RS & Erlang & CH & CH-P \\
\hline
\hline
Local only & \texttt{spawn}   &      &           &                &    & \texttt{async}    \\
Lazy       &           &      & \texttt{spawn}   &                &    &            \\
Eager      &           &      & \texttt{spawnAt} & \texttt{async\_call/4} &    & \texttt{asyncSTM} \\
\hline
\end{tabular}
\end{center}
\caption{Future Creation}
\label{tab:future-creation}
\end{table}
#+END_LATEX

The Fibonacci micro-benchmark is used to compare the use of the
Erlang, HdpH-RS and CH-P libraries for /future task/ creation for
distributed-memory scheduling. The decomposition of Fibonacci is the
same in all three cases. The scheduling in Erlang and CH-P is
explicit, whilst in HdpH-RS it is lazy.

Listing \ref{lst:erlang-async-fib} shows an Erlang Fibonacci that uses
~rpc:async_call~ on line \ref{code:erlang-async}. Listing
\ref{lst:hdphrs-spawn-fib} shows a HdpH-RS Fibonacci that uses a
~spawn~ call on line \ref{code:hdphrs-spawn}. Listing
\ref{lst:ch-p-async-fib} shows a CH-P Fibonacci that uses an
~asyncSTM~ on line \ref{code:ch-p-asyncSTM}.

#+BEGIN_LATEX
\begin{Code}
\begin{erlangcode}{Fibonacci with Asynchronous Remote Function Calls in Erlang with RPC}{lst:erlang-async-fib}
-module(fib_rpc).
-export([fib/1,random_node/0]).

%% Compute Fibonacci with futures
fib(0) -> 0;
fib(1) -> 1;
fib(X) ->
  Key = rpc:async_call(random_node(),fib_rpc,fib,[X-1]), @\label{code:erlang-async}@
  Y = fib(X-2),
  Z = rpc:yield(Key),
  Y + Z.

%% Select random node (maybe our own)
random_node() ->
  I = random:uniform(length(nodes()) + 1),
  Nodes = nodes() ++ [node()],
  lists:nth(I,Nodes).
\end{erlangcode}
\end{Code}
#+END_LATEX

#+BEGIN_LATEX
\begin{Code}
\begin{haskellcode}{Fibonacci with HdpH-RS}{lst:hdphrs-spawn-fib}
fib :: Int -> Par Integer
fib x
    | x == 0 = return 0
    | x == 1 = return 1
    | otherwise = do
        v <- spawn $(mkClosure [| hdphFibRemote (x) |]) @\label{code:hdphrs-spawn}@
        y <- fib (x - 2)
        clo_x <- get v
        force $ unClosure clo_x + y

hdphFibRemote :: Int -> Par (Closure Integer)
hdphFibRemote n =
  fib (n-1) >>= force . toClosure
\end{haskellcode}
\end{Code}
#+END_LATEX

#+BEGIN_LATEX
\begin{Code}
\begin{haskellcode}{Fibonacci with Async CloudHaskell-Platform API}{lst:ch-p-async-fib}
randomElement :: [a] -> IO a
randomElement xs = randomIO >>= \ix -> return (xs !! (ix `mod` length xs)) @\label{code:randomElement}@

remotableDecl [
  [d| fib :: ([NodeId],Int) -> Process Integer ;
      fib (_,0) = return 0
      fib (_,1) = return 1
      fib (nids,n) = do
        node <- liftIO $ randomElement nids
        let tsk = remoteTask ($(functionTDict 'fib)) node ($(mkClosure 'fib) (nids,n-2))
        future <- asyncSTM tsk @\label{code:ch-p-asyncSTM}@
        y <- fib (nids,n-1)
        (AsyncDone z) <- wait future
        return $ y + z
    |]
  ]
\end{haskellcode}
\end{Code}
#+END_LATEX


The ~mkClosure~ Template Haskell in HdpH-RS is
required to allow the ~fib~ function to be transmitted over the
network. The CH-P code uncovered a race condition when
setting up monitors for evaluating \texttt{AsyncTask}s, which was
added as a test case upstream by the author \cite{chp-async-fix}. The
difference between HdpH-RS and CH-P is that in CH-P a
~NodeId~ must be given to ~asyncSTM~ in CH-P, to eagerly schedule a
\texttt{fib~(n-2)} task, selected with ~randomElement~ on line
\ref{code:randomElement}. There are more intrusive Template Haskell
requirements in CloudHaskell, as it does not share a common closure
representation with HdpH. In CloudHaskell, the ~remotableDecl~
boilerplate code is needed when a programmer wants to refer to
\texttt{\$(mkClosure~'f)} within the definition of ~f~ itself.

The CH-P API implementation uses Haskell's STM to
monitor the evaluation status of \texttt{Async}'s on line
\ref{code:ch-p-asyncSTM} in Listing \ref{lst:ch-p-async-fib}. The same
technique is used for blocking on \texttt{IVar}s in the supervised
workpools implementation for HdpH in Chapter [[Supervised Workpools]].

It is a similar story for operations on futures. In HdpH and HdpH-RS,
a blocking wait on a future is ~get~, which is also true for
monad-par. In Erlang RPC it is ~rpc:yield/4~, and in CH-P it is
~wait~. The non-blocking version in HdpH and HdpH-RS is ~tryGet~, in
Erlang RPC it is ~rpc:nb_yield/4~ and in CH-P it is ~waitTimeout~. The
operations on futures are shown in Table \ref{tab:future-operations}.

#+BEGIN_LATEX
\begin{table}
\begin{center}
{\small
\begin{tabular}{|l||c|c|c|c|c|c|}
\hline
 & monad-par & HdpH & HdpH-RS & Erlang & CH & CH-P \\
\hline
\hline
Blocking read     & \texttt{get}     & \texttt{get}    & \texttt{get}    & \texttt{yield/1}    &    & \texttt{wait}        \\
Non-blocking read &           & \texttt{tryGet} & \texttt{tryGet} & \texttt{nb\_yield/1} &    & \texttt{poll}       \\
Timeout read      &           &          &          & \texttt{nb\_yield/2} &    & \texttt{waitTimeout} \\
Check fullness    &           & \texttt{probe}  & \texttt{probe}  &              &    & \texttt{check}       \\
\hline
\end{tabular}
}
\end{center}
\caption{Operations on Futures}
\label{tab:future-operations}
\end{table}
#+END_LATEX

This section has expanded on the introduction of the spawn
family in Section [[HdpH-RS Programming Primitives]]. It shows that HdpH
balances load between processing elements. In contrast to the
monad-par library which defines single-node multicores as a
processing element, HdpH uses distributed-memory multicore as
processing elements. Moreover, HdpH-RS shares the futures programming
model with the Erlang RPC library and the CloudHaskell Platform on
distributed-memory. Only the HdpH-RS runtime supports load-balancing
for evaluating futures.

The second distinction in this section is the naming conventions for
creating futures. The /spawn/ convention in HdpH-RS was influenced by
the monad-par API. It also uses \texttt{IVar}s as futures. As HdpH was
designed as a distributed-memory extension of this library, the spawn
name is shared. In contrast, the CloudHaskell Platform primitive for
eager task placement is ~asyncSTM~. The Erlang primitive for
eager task placement is ~async_call/4~. The ~spawn~ and
~supervisedSpawn~ HdpH-RS primitives are unique in their function, in this
comparison of three functional libraries. They are used to create
supervised sparks that are lazily scheduled with load balancing on
distributed-memory.

** Promela Model Implementation

The Promela model used in Chapter [[The Validation of Reliable
Distributed Scheduling for HdpH-RS]] is in Listing
\ref{lst:promela-model-all}.

#+BEGIN_LATEX
\begin{promelacode}{Promela Model of HdpH-RS Scheduler}{lst:promela-model-all}
mtype = {FISH,DEADNODE,SCHEDULE,REQ,AUTH,DENIED,ACK,NOWORK,OBSOLETE,RESULT};
mtype = {ONNODE,INTRANSITION};

typedef Location
{
  int from;
  int to;
  int at=3;
}

typedef Sparkpool {
  int spark_count=0;
  int spark=0; /* sequence number */
}

typedef Spark {
  int highestReplica=0;
  Location location;
  mtype context=ONNODE; 
  int age=0;
}

typedef WorkerNode {
  Sparkpool sparkpool;
  int waitingFishReplyFrom;
  bool waitingSchedAuth=false;
  bool resultSent=false;
  bool dead=false;
  int lastTried;
};

typedef SupervisorNode {
  Sparkpool sparkpool;
  bool waitingSchedAuth=false;
  bool resultSent=false;
  bit ivar=0;
};

#define null -1
#define maxLife 100

WorkerNode worker[3];
SupervisorNode supervisor;
Spark spark;
chan chans[4] = [10] of {mtype, int , int , int } ;

inline report_death(me){
  chans[0] ! DEADNODE(me, null, null) ; 
  chans[1] ! DEADNODE(me, null, null) ;
  chans[2] ! DEADNODE(me, null, null) ;
  chans[3] ! DEADNODE(me, null, null) ; /* supervisor */
}

active proctype Supervisor() {
  int thiefID, victimID, deadNodeID, seq, authorizedSeq, deniedSeq;
  
  supervisor.sparkpool.spark_count = 1;
  run Worker(0); run Worker(1); run Worker(2); 

SUPERVISOR_RECEIVE:
  
  if  :: (supervisor.sparkpool.spark_count > 0 && spark.age > maxLife) ->
         atomic {
           supervisor.resultSent = 1;
           supervisor.ivar = 1;  /* write to IVar locally */
           goto EVALUATION_COMPLETE;
         }
      :: else ->
         if
           :: (supervisor.sparkpool.spark_count > 0) ->
              atomic {
                supervisor.resultSent = 1;
                supervisor.ivar = 1;  /* write to IVar locally */
                goto EVALUATION_COMPLETE;
              }
           :: chans[3] ? DENIED(thiefID, deniedSeq,null) ->
              supervisor.waitingSchedAuth = false;
              chans[thiefID] ! NOWORK(3, null, null) ;
           :: chans[3] ? FISH(thiefID, null,null) -> /* React to FISH request */
              if    /* We have the spark */
                :: (supervisor.sparkpool.spark_count > 0 && !supervisor.waitingSchedAuth) ->
                   supervisor.waitingSchedAuth = true;
                   chans[3] ! REQ(3, thiefID, supervisor.sparkpool.spark);
                ::  else -> chans[thiefID] ! NOWORK(3, null,null) ; /*We don't have the spark */
              fi; 
           :: chans[3] ? AUTH(thiefID, authorizedSeq, null) -> /* React to FISH request */
              d_step {
                supervisor.waitingSchedAuth = false;
                supervisor.sparkpool.spark_count--;
              }
              chans[thiefID] ! SCHEDULE(3, supervisor.sparkpool.spark ,null);
           :: chans[3] ? REQ(victimID, thiefID, seq) ->
              if
                :: seq == spark.highestReplica ->
                   if
                     :: spark.context == ONNODE && ! worker[thiefID].dead->
                        d_step {
                          spark.context  = INTRANSITION;
                          spark.location.from = victimID ;
                          spark.location.to = thiefID ;
                        }
                        chans[victimID] ! AUTH(thiefID, seq, null);
                     :: else ->
                        chans[victimID] ! DENIED(thiefID, seq, null);
                   fi
                :: else ->
                   chans[victimID] ! OBSOLETE(thiefID, null, null); /* obsolete sequence number */
              fi
           :: chans[3] ? ACK(thiefID, seq, null) ->
              if
                :: seq == spark.highestReplica ->
                   d_step {
                     spark.context = ONNODE;
                     spark.location.at = thiefID ;
                   }
                :: else -> skip ;
              fi             
           :: atomic {chans[3] ? RESULT(null, null, null); supervisor.ivar = 1; goto EVALUATION_COMPLETE;}	              
           :: chans[3] ? DEADNODE(deadNodeID, null, null) ->
              bool should_replicate;
              d_step {
                should_replicate = false;
                if
                  :: spark.context == ONNODE \
                     && spark.location.at == deadNodeID -> should_replicate = true;
                  :: spark.context  == INTRANSITION \
                     && (spark.location.from == deadNodeID \
                         || spark.location.to == deadNodeID) -> should_replicate = true;
                  :: else -> skip;
                fi;
                
                if
                  :: should_replicate ->
                     spark.age++;
                     supervisor.sparkpool.spark_count++;
                     spark.highestReplica++;
                     supervisor.sparkpool.spark = spark.highestReplica ;
                     spark.context = ONNODE;
                     spark.location.at = 3 ;
                  :: else -> skip;
                fi;
              }
         fi;
  fi;
  
  if
    :: (supervisor.ivar == 0) -> goto SUPERVISOR_RECEIVE;
    :: else -> skip;
  fi;    
  
EVALUATION_COMPLETE:
  
}  /* END OF SUPERVISOR */

proctype Worker(int me) {
  int thiefID, victimID, deadNodeID, seq, authorisedSeq, deniedSeq;
  
WORKER_RECEIVE:

  if
    :: (worker[me].sparkpool.spark_count > 0 && spark.age > maxLife) -> 
       atomic {
         worker[me].resultSent = true;
         chans[3]  ! RESULT(null,null,null);
         goto END;
       }
            
    :: else ->
       if
         ::  skip ->  /* die */
            worker[me].dead = true;
            report_death(me);
            goto END;

         :: (worker[me].sparkpool.spark_count > 0) ->
              chans[3] ! RESULT(null,null,null);
            
         :: (worker[me].sparkpool.spark_count == 0 && (worker[me].waitingFishReplyFrom == -1) \
             && spark.age < (maxLife+1)) -> 
            /* Lets go fishing */
            int chosenVictimID;
            d_step {
              if
                :: (0 != me) && !worker[0].dead  && (worker[me].lastTried -  0) -> chosenVictimID = 0;
                :: (1 != me) && !worker[1].dead  && (worker[me].lastTried -  1) -> chosenVictimID = 1;
                :: (2 != me) && !worker[2].dead  && (worker[me].lastTried -  2) -> chosenVictimID = 2;
                :: skip -> chosenVictimID = 3; /* supervisor */
              fi;
              worker[me].lastTried=chosenVictimID;
              worker[me].waitingFishReplyFrom = chosenVictimID;
            };
            chans[chosenVictimID] ! FISH(me, null, null) ;
            
         :: chans[me] ? NOWORK(victimID, null, null) ->
              worker[me].waitingFishReplyFrom = -1;  /* can fish again */

         :: chans[me] ? FISH(thiefID, null, null) -> /* React to FISH request */
            if   /* We have the spark */
              :: (worker[me].sparkpool.spark_count > 0 && ! worker[me].waitingSchedAuth) ->
                 worker[me].waitingSchedAuth = true;
                 chans[3] ! REQ(me, thiefID, worker[me].sparkpool.spark);
              ::  else -> chans[thiefID] ! NOWORK(me, null, null) ; /*We don't have the spark */
            fi
            
         :: chans[me] ? AUTH(thiefID, authorisedSeq, null) ->  /* React to schedule authorisation */
            d_step {
              worker[me].waitingSchedAuth = false;
              worker[me].sparkpool.spark_count--;
              worker[me].waitingFishReplyFrom = -1;
            }
            chans[thiefID] ! SCHEDULE(me, worker[me].sparkpool.spark, null);
            
         :: chans[me] ? DENIED(thiefID, deniedSeq, null) ->
            worker[me].waitingSchedAuth = false;
            chans[thiefID] ! NOWORK(me, null, null) ;
            
         :: chans[me] ? OBSOLETE(thiefID, null, null) ->
            d_step {
              worker[me].waitingSchedAuth = false;
              worker[me].sparkpool.spark_count--;
              worker[me].waitingFishReplyFrom = -1;
            }
            chans[thiefID] ! NOWORK(me, null, null) ;
            
         :: chans[me] ? SCHEDULE(victimID, seq, null) -> /* We're being sent the spark */
            d_step {
              worker[me].sparkpool.spark_count++;
              worker[me].sparkpool.spark = seq ;
              spark.age++;
            }
            chans[3] ! ACK(me, seq, null) ; /* Send ACK To supervisor */
            
         :: chans[me] ? DEADNODE(deadNodeID, null, null) ->
            d_step {
              if
                :: worker[me].waitingFishReplyFrom > deadNodeID ->
                   worker[me].waitingFishReplyFrom = -1 ;
                :: else -> skip ;
              fi ;
            };
       fi ;
  fi;
  
  if
    :: (supervisor.ivar == 1) -> goto END;
    :: else -> goto WORKER_RECEIVE;
  fi;    
  
END:
  
} /* END OF WORKER */

/* propositional symbols for LTL formulae */

#define ivar_full ( supervisor.ivar == 1 )
#define ivar_empty ( supervisor.ivar == 0 )
#define all_workers_alive ( !worker[0].dead && !worker[1].dead && !worker[2].dead )
#define all_workers_dead ( worker[0].dead && worker[1].dead && worker[2].dead )
#define any_result_sent ( supervisor.resultSent \
                          || worker[0].resultSent 
                          || worker[1].resultSent \
                          || worker[2].resultSent )

/* SPIN generated never claims corresponding to LTL formulae */

never  {    /* ! [] all_workers_alive */
T0_init:
  do
    :: d_step { (! ((all_workers_alive))) -> assert(!(! ((all_workers_alive)))) }
    :: (1) -> goto T0_init
  od;
accept_all:
  skip
}

never  {    /* ! [] (ivar_empty U any_result_sent) */
  skip;
T0_init:
  do
    :: (! ((any_result_sent))) -> goto accept_S4
    :: d_step { (! ((any_result_sent)) && ! ((ivar_empty))) ->
                assert(!(! ((any_result_sent)) && ! ((ivar_empty)))) }
    :: (1) -> goto T0_init
  od;
accept_S4:
  do
    :: (! ((any_result_sent))) -> goto accept_S4
    :: d_step { (! ((any_result_sent)) && ! ((ivar_empty))) ->
                assert(!(! ((any_result_sent)) && ! ((ivar_empty)))) }
  od;
accept_all:
  skip
}

never  {    /* ! <> [] ivar_full */
T0_init:
  do
    :: (! ((ivar_full))) -> goto accept_S9
    :: (1) -> goto T0_init
  od;
accept_S9:
  do
    :: (1) -> goto T0_init
  od;
}
\end{promelacode}
#+END_LATEX

** Feeding Promela Bug Fix to Implementation

This section shows the Promela fix (Section [[Bug Fix in Promela Model]])
and Haskell fix (Section [[Bug Fix in Haskell]]) to the scheduling bug
described in Section [[Identifying Scheduling Bugs]].

*** Bug Fix in Promela Model

#+BEGIN_LATEX
\begin{lstlisting}[captionpos=b,basicstyle=\ttfamily\scriptsize]
commit 7f23a46035cbb9a12aeadb84ab8dbcb0c28e7a48
Author: Rob Stewart <robstewart57@gmail.com>
Date:   Thu Jun 27 23:21:20 2013 +0100

    Additional condition when supervisor receives REQ.
    
    Now the supervisor checks that the thief in a REQ message is still
    alive before it authorises with the scheduling with AUTH. This has
    been fed back in to the HdpH-RS implementation git commit
    d2c5c7e58257ae11443c00203a9cc984b13601ad

diff --git a/spin_models/hdph_scheduler.pml b/spin_models/hdph_scheduler.pml
index 3bb033f..e94ce63 100644
--- a/spin_models/hdph_scheduler.pml
+++ b/spin_models/hdph_scheduler.pml
  :: chans[3] ? REQ(sendingNode, fishingNodeID, seq) ->
     if
       :: seq == spark.highestReplica ->
          if
-          :: spark.context == ONNODE ->
+          :: spark.context == ONNODE && ! worker[fishingNodeID].dead->
               atomic {
                 spark.context  = INTRANSITION;
                 spark.location.from = sendingNode ;
                 spark.location.to = worker[fishingNodeID].inChan;
               }
               sendingNode ! AUTH(worker[fishingNodeID].inChan, seq);
            :: else ->
               sendingNode ! DENIED(worker[fishingNodeID].inChan, seq);
\end{lstlisting}
#+END_LATEX


*** Bug Fix in Haskell

#+BEGIN_LATEX
\begin{lstlisting}[captionpos=b,basicstyle=\ttfamily\scriptsize]
commit d2c5c7e58257ae11443c00203a9cc984b13601ad
Author: Rob Stewart <robstewart57@gmail.com>
Date: Thu Jun 27 23:05:45 2013 +0100

    Added guard in handleREQ.
    
    Improvements (a simplification) to Promela model has enabled me to
    attempt to verify more properties. Attempting to verify a new
    property has uncovered a subtle deadlock in the fault tolerant
    fishing. A thief chooses a random victim and the victim requests a
    SCHEDULE to the supervisor of the spark. The supervisor may
    receive a DEADNODE message about the thief before the REQ. When
    the REQ is received by the supervisor, the existence of the thief
    in the VM is not checked, so the supervisor may nevertheless
    return AUTH to the victim. The victim will blindly send the
    message to the dead node, and the spark may be lost. The
    supervisor will have no future DEADNODE message about the thief to
    react to i.e. the spark will not get recreated.
    
    Now, the spark supervisor checks that the thief is in the VM
    before sending an AUTH to the victim. The spark location record
    now is InTransition. If the supervisor /now/ receives a DEADNODE
    message, it *will* re-create the spark, with an incremented
    replica number.

diff --git a/hdph/src/Control/Parallel/HdpH/Internal/Sparkpool.hs
           b/hdph/src/Control/Parallel/HdpH/Internal/Sparkpool.hs
index 480e938..205151d 100644
--- a/hdph/src/Control/Parallel/HdpH/Internal/Sparkpool.hs
+++ b/hdph/src/Control/Parallel/HdpH/Internal/Sparkpool.hs
@@ -608,25 +608,35 @@ handleREQ (REQ taskRef seqN from to) = do
            show obsoleteMsg ++ " ->> " ++ show from
          void $ liftCommM $ Comm.send from $ encodeLazy obsoleteMsg
         else do
-         loc <- liftIO $ fromJust <$> locationOfTask taskRef
-         case loc of

+         nodes <- liftCommM Comm.allNodes
+         -- check fisher hasn't died in the meantime (from verified model)
+         if (elem to nodes)
+          then do
            loc <- liftIO $ fromJust <$> locationOfTask taskRef
            case loc of

+          else do
+           let deniedMsg = DENIED to
+           debug dbgMsgSend $
+              show deniedMsg ++ " ->> " ++ show from
+           void $ liftCommM $ Comm.send from $ encodeLazy deniedMsg
\end{lstlisting}
#+END_LATEX



** Network Transport Event Error Codes

All typed error events that can be received from the network-transport
\cite{network-transport} layer is in Listing
\ref{lst:transport-events}. The API for error event codes was
conceived by the WellTyped company \cite{welltyped} as part of the
CloudHaskell 2.0 development \cite{new-cloud-haskell}. The
~ErrorConnectionLost~ error is used in HdpH-RS to propagate ~DEADNODE~
messages to the scheduler.

#+BEGIN_LATEX
\begin{haskellcode}{Error Events in Transport Layer}{lst:transport-events}
-- | Error codes used when reporting errors to endpoints (through receive)
data EventErrorCode =
    -- | Failure of the entire endpoint
    EventEndPointFailed
    -- | Transport-wide fatal error
  | EventTransportFailed
    -- | We lost connection to another endpoint
    --
    -- Although "Network.Transport" provides multiple independent lightweight
    -- connections between endpoints, those connections cannot fail
    -- independently: once one connection has failed, all connections, in
    -- both directions, must now be considered to have failed; they fail as a
    -- "bundle" of connections, with only a single "bundle" of connections per
    -- endpoint at any point in time.
    --
    -- That is, suppose there are multiple connections in either direction
    -- between endpoints A and B, and A receives a notification that it has
    -- lost contact with B. Then A must not be able to send any further
    -- messages to B on existing connections.
    --
    -- Although B may not realise immediately that its connection to A has
    -- been broken, messages sent by B on existing connections should not be
    -- delivered, and B must eventually get an EventConnectionLost message,
    -- too.
    --
    -- Moreover, this event must be posted before A has successfully
    -- reconnected (in other words, if B notices a reconnection attempt from A,
    -- it must post the EventConnectionLost before acknowledging the connection
    -- from A) so that B will not receive events about new connections or
    -- incoming messages from A without realising that it got disconnected.
    --
    -- If B attempts to establish another connection to A before it realised
    -- that it got disconnected from A then it's okay for this connection
    -- attempt to fail, and the EventConnectionLost to be posted at that point,
    -- or for the EventConnectionLost to be posted and for the new connection
    -- to be considered the first connection of the "new bundle".
  | EventConnectionLost EndPointAddress\end{haskellcode}
#+END_LATEX

** Handling Dead Node Notifications

The handling of ~DEADNODE~ messages in the HdpH-RS scheduler is in
Listing \ref{lst:deadnode-handler-impl}. The implementations of
~replicateSpark~ and ~replicateThread~ (lines
\ref{code:replicateSpark} and \ref{code:replicateThread}) is in
Appendix [[Replicating Sparks and Threads]].

#+BEGIN_LATEX
\begin{haskellcode}{Handler for \texttt{DEADNODE} Messages}{lst:deadnode-handler-impl}
handleDEADNODE :: Msg RTS -> RTS ()
handleDEADNODE (DEADNODE deadNode) = do
    -- remove node from virtual machine                                                                                                                      
    liftCommM $ Comm.rmNode deadNode

    -- 1) if waiting for FISH response from dead node, reset                                                                                                 
    maybe_fishingReplyNode <- liftSparkM waitingFishingReplyFrom
    when (isJust maybe_fishingReplyNode) $ do
      let fishingReplyNode = fromJust maybe_fishingReplyNode
      when (fishingReplyNode == deadNode) $ do
        liftSparkM clearFishingFlag

    -- 2) If spark in guard post was destined for                                                                                                            
    --    the failed node, put spark back in sparkpool                                                                                                       
    liftSparkM $ popGuardPostToSparkpool deadNode

    -- 3a) identify all empty vulnerable futures                                                                                                             
    emptyIVars <- liftIO $ vulnerableEmptyFutures deadNode :: RTS [(Int,IVar m a)]
    (emptyIVarsSparked,emptyIVarsPushed) <- partitionM wasSparked emptyIVars

    -- 3b) create replicas                                                                                                                                   
    replicatedSparks  <- mapM (liftIO . replicateSpark) emptyIVarsSparked @\label{code:replicateSpark}@
    replicatedThreads <- mapM (liftIO . replicateThread) emptyIVarsPushed @\label{code:replicateThread}@

    -- 3c) schedule replicas                                                                                                                                 
    mapM_ (liftSparkM . putSpark . Left . toClosure . fromJust) replicatedSparks
    mapM_ (execThread . mkThread . unClosure . fromJust) replicatedThreads

    -- for RTS stats                                                                                                                                         
    replicateM_ (length emptyIVarsSparked) $ liftSparkM $ getSparkRecCtr  >>= incCtr
    replicateM_ (length emptyIVarsPushed)  $ liftSparkM $ getThreadRecCtr >>= incCtr

  where
    wasSparked (_,v) = do
      e <- liftIO $ readIORef v
      let (Empty _ maybe_st) = e
          st = fromJust maybe_st
      return $ scheduling st == Sparked
\end{haskellcode}
#+END_LATEX

** Replicating Sparks and Threads

Listing \ref{lst:task-replication} shows the implementation of
~replicateSpark~ and ~replicateThread~. These are used by
~handleDEADNODE~ in Listing \ref{lst:deadnode-handler-impl} of
Appendix [[Handling Dead Node Notifications]] to recover at-risk tasks
when node failure is detected.


#+BEGIN_LATEX
\begin{haskellcode}{Replicating Sparks \& Threads in Presence of Failure}{lst:task-replication}
replicateSpark :: (Int,IVar m a) -> IO (Maybe (SupervisedSpark m))
replicateSpark (indexRef,v) = do
  me <- myNode
  atomicModifyIORef v $ \e ->
    case e of
      Full _ -> (e,Nothing) -- cannot duplicate task, IVar full, task garbage collected
      Empty b maybe_st ->
        let ivarSt = fromMaybe
             (error "cannot duplicate non-supervised task")
             maybe_st
            newTaskLocation =  OnNode me
            newReplica = (newestReplica ivarSt) + 1
            supervisedTask = SupervisedSpark
              { clo = task ivarSt
              , thisReplica = newReplica
              , remoteRef = TaskRef indexRef me
              }
            newIVarSt = ivarSt { newestReplica = newReplica , location = newTaskLocation}
        in (Empty b (Just newIVarSt),Just supervisedTask)

replicateThread :: (Int,IVar m a) -> IO (Maybe (Closure (ParM m ())))
replicateThread (indexRef,v) = do
  me <- myNode
  atomicModifyIORef v $ \e ->
    case e of
      Full _ -> (e,Nothing) -- cannot duplicate task, IVar full, task garbage collected
      Empty b maybe_st ->
        let ivarSt = fromMaybe
             (error "cannot duplicate non-supervised task")
             maybe_st
            newTaskLocation =  OnNode me
            newReplica = (newestReplica ivarSt) + 1
            threadCopy = task ivarSt
            newIVarSt = ivarSt { newestReplica = newReplica , location = newTaskLocation}
        in (Empty b (Just newIVarSt),Just threadCopy)
\end{haskellcode}
#+END_LATEX

** Propagating Failures from Transport Layer

The implementation of the ~receive~ function in the ~Comm~ module of
HdpH-RS is in Listing \ref{lst:propagating-failure}.

#+BEGIN_LATEX
\begin{haskellcode}{Propagating Failure Events}{lst:propagating-failure}
receive :: IO Msg
receive = do
     ep <- myEndPoint
     event <- NT.receive ep
     case event of
        -- HdpH-RS payload message
        NT.Received _ msg ->  return ((force . decodeLazy . Lazy.fromChunks) msg) @\label{code:nt-received}@

        -- special event from the transport layer
        NT.ErrorEvent (NT.TransportError e _) ->
         case e of
          (NT.EventConnectionLost ep) -> do
               mainEP <- mainEndpointAddr
               if mainEP == ep then do
                 -- check if the root node has died. If it has, give up.
                 uncleanShutdown
                 return Shutdown @\label{code:nt-shutdown}@

                else do
                 -- propagate DEADNODE to the scheduler
                 remoteConnections <- connectionLookup
                 let x = Map.filterWithKey (\node _ -> ep == node) remoteConnections
                     deadNode = head $ Map.keys x
                     msg = Payload $ encodeLazy (Payload.DEADNODE deadNode)
                 return msg @\label{code:nt-deadnode}@

          _ -> receive -- loop
        -- unused events: [ConnectionClosed,ConnectionOpened,
        --                 ReceivedMulticast,EndPointClosed]
        _ -> receive
\end{haskellcode}
#+END_LATEX

** HdpH-RS Skeleton API

The HdpH-RS algorithmic skeletons API is in Listing
\ref{lst:hdphrs-skel-api}. They are evaluated in Chapter [[Fault
Tolerant Programming & Reliable Scheduling Evaluation]]. They are
adaptions of HdpH skeletons, which are available online
\cite{hdph-code}. The use of ~spawn~ and ~spawnAt~ in the HdpH
skeletons is replaced with ~supervisedSpawn~ and ~supervisedSpawnAt~
respectively in the HdpH-RS versions.

#+BEGIN_LATEX
\begin{haskellcode}{HdpH-RS Skeleton API}{lst:hdphrs-skel-api}
------------------------
-- parallel-map family

parMap
  :: (ToClosure a, ForceCC b)
  => Closure (a -> b) -- function to apply
  -> [a]              -- list of inputs
  -> Par [b]          -- list of outputs

pushMap
  :: (ToClosure a, ForceCC b)
  => Closure (a -> b)  -- function to apply
  -> [a]               -- list of inputs
  -> Par [b]           -- list of outputs

parMapForkM
  :: (NFData b
  => (a -> Par b)  -- function to apply
  -> [a]           -- list of inputs
  -> Par [b]       -- list of outputs

parMapChunked
  :: (ToClosure a, ForceCC b)
  => Int              -- how many chunks
  -> Closure (a -> b) -- function to apply
  -> [a]              -- list of inputs
  -> Par [b]          -- list of outputs

pushMapChunked
  :: (ToClosure a, ForceCC b)
  => Int              -- how many chunks
  -> Closure (a -> b) -- function to apply
  -> [a]              -- list of inputs
  -> Par [b]          -- list of outputs

parMapSliced
  :: (ToClosure a, ForceCC b)
  => Int               -- how many slices
  -> Closure (a -> b)  -- function to apply
  -> [a]               -- list of inputs
  -> Par [b]           -- list of outputs

pushMapSliced
  :: (ToClosure a, ForceCC b)
  => Int               -- how many slices
  -> Closure (a -> b)  -- function to apply
  -> [a]               -- list of inputs
  -> Par [b]           -- list of outputs

------------------------
-- divide-and-conquer family

forkDivideAndConquer
  :: (NFData b)
  => (a -> Bool)      -- isTrivial
  -> (a -> [a])       -- decompose problem
  -> (a -> [b] -> b)  -- combine solutions
  -> (a -> Par b)     -- trivial algorithm
  -> a                -- problem
  -> Par b            -- result

parDivideAndConquer
  :: Closure (Closure a -> Bool)            -- is trivial
  -> Closure (Closure a -> [Closure a])     -- decompose problem
  -> Closure (Closure a -> [Closure b] -> Closure b)  -- combine solutions
  -> Closure (Closure a -> Par (Closure b)) --trivial algorithm
  -> Closure a                              -- problem
  -> Par (Closure b)                        -- result

pushDivideAndConquer
  :: Closure (Closure a -> Bool)            -- is trivial
  -> Closure (Closure a -> [Closure a])     -- decompose problem
  -> Closure (Closure a -> [Closure b] -> Closure b) -- combine solutions
  -> Closure (Closure a -> Par (Closure b)) -- trivial algorithm
  -> Closure a                              -- problem
  -> Par (Closure b)                        -- result

------------------------
-- map-reduce family

parMapReduceRangeThresh
  :: Closure Int                                         -- threshold
  -> Closure InclusiveRange                              -- range over which to calculate
  -> Closure (Closure Int -> Par (Closure a))            -- compute one result
  -> Closure (Closure a -> Closure a -> Par (Closure a)) -- compute two results (associate)
  -> Closure a                                           -- initial value
  -> Par (Closure a)

pushMapReduceRangeThresh
  :: Closure Int                                         -- threshold
  -> Closure InclusiveRange                              -- range over which to calculate
  -> Closure (Closure Int -> Par (Closure a))            -- compute one result
  -> Closure (Closure a -> Closure a -> Par (Closure a)) -- compute two results (associate)
  -> Closure a                                           -- initial value
  -> Par (Closure a)
\end{haskellcode}
#+END_LATEX

** Using Chaos Monkey in Unit Testing

The ~chaosMonkeyUnitTest~ is on line \ref{code:chaosMonkeyUnitTest} of
Listing \ref{lst:chaosmonkey-unit-test}.

#+BEGIN_LATEX
\begin{haskellcode}{Using Chaos Monkey in Unit Testing}{lst:chaosmonkey-unit-test}
-- | example use of chaos monkey unit testing with Sum Euler.
main = do
    conf <- parseCmdOpts
    chaosMonkeyUnitTest @\label{code:chaosMonkeyUnitTest-used}@
       conf -- user defined RTS options
       "sumeuler-pushMapFT" -- label for test
       759924264 -- expected value
       (ft_push_farm_sum_totient_chunked 0 50000 500) -- Par computation to run
                                                      -- in presence of chaos monkey
  where       
    -- using fault tolerant explicit pushMap skeleton
    ft_push_farm_sum_totient_chunked :: Int -> Int -> Int -> Par Integer                                                                  
    ft_push_farm_sum_totient_chunked lower upper chunksize =
        sum <$> FT.pushMapNF $(mkClosure [| sum_totient |]) chunked_list
    chunked_list = chunk chunksize [upper, upper - 1 @..@ lower] :: [[Int]]

-- | helper function in 'Control.Parallel.HdpH' to run
--   a Par computation unit test with chaos monkey enabled.
chaosMonkeyUnitTest :: (Eq a, Show a) @\label{code:chaosMonkeyUnitTest}@
                    => RTSConf -- user defined RTS configuration
                    -> String  -- label identifier for unit test
                    -> a       -- expected value
                    -> Par a   -- Par computation to execute with failure
                    -> IO ()
chaosMonkeyUnitTest conf label expected f = do
    let tests = TestList $ [runTest label expected f]
    void $ runTestTT tests -- run HUnit test
  where
    runTest :: (Show a,Eq a) => String -> a -> Par a -> Test
    runTest label expected f =
      let chaosMonkeyConf = conf {chaosMonkey = True, maxFish = 10 , minSched = 11}
          test = do
           result <- evaluate =<< runParIO chaosMonkeyConf f -- run HdpH-RS computation 'f'
           case result of
             Nothing -> assert True -- non-root nodes do not perform unit test
             Just x -> putStrLn (label++" result: "++show x) >>
                       assertEqual label x expected -- compare result vs expected value
      in TestLabel label (TestCase test)
\end{haskellcode}
#+END_LATEX
** Benchmark Implementations
*** Fibonacci

#+BEGIN_LATEX
\begin{haskellcode}{Fibonacci Benchmark Implementations}{lst:fib-impl}
-- | sequential Fibonacci
fib :: Int -> Integer
fib n | n <= 1    = 1
      | otherwise = fib (n-1) + fib (n-2)

-- | lazy divide-and-conquer skeleton
spark_skel_fib :: Int -> Int -> Par Integer
spark_skel_fib seqThreshold n = unClosure <$> skel (toClosure n)
  where
    skel = parDivideAndConquer
             $(mkClosure [| dnc_trivial_abs (seqThreshold) |])
             $(mkClosure [| dnc_decompose |])
             $(mkClosure [| dnc_combine |])
             $(mkClosure [| dnc_f |])

-- | eager divide-and-conquer skeleton
push_skel_fib :: [NodeId] -> Int -> Int -> Par Integer                                                                                                                                                         
push_skel_fib nodes seqThreshold n = unClosure <$> skel (toClosure n)
  where
    skel = pushDivideAndConquer
             nodes
             $(mkClosure [| dnc_trivial_abs (seqThreshold) |])
             $(mkClosure [| dnc_decompose |])
             $(mkClosure [| dnc_combine |])
             $(mkClosure [| dnc_f |])

-- | fault tolerant lazy divide-and-conquer skeleton
ft_spark_skel_fib :: Int -> Int -> Par Integer
ft_spark_skel_fib seqThreshold n = unClosure <$> skel (toClosure n)
  where
    skel = FT.parDivideAndConquer
             $(mkClosure [| dnc_trivial_abs (seqThreshold) |])
             $(mkClosure [| dnc_decompose |])
             $(mkClosure [| dnc_combine |])
             $(mkClosure [| dnc_f |])

-- | fault tolerant eager divide-and-conquer skeleton
ft_push_skel_fib :: Int -> Int -> Par Integer
ft_push_skel_fib seqThreshold n = unClosure <$> skel (toClosure n)
  where
    skel = FT.pushDivideAndConquer
             $(mkClosure [| dnc_trivial_abs (seqThreshold) |])
             $(mkClosure [| dnc_decompose |])
             $(mkClosure [| dnc_combine |])
             $(mkClosure [| dnc_f |])

-- | is trivial case
dnc_trivial_abs :: (Int) -> (Closure Int -> Bool)
dnc_trivial_abs (seqThreshold) =
  \ clo_n -> unClosure clo_n <= max 1 seqThreshold

-- | decompose problem
dnc_decompose =
  \ clo_n -> let n = unClosure clo_n in [toClosure (n-1), toClosure (n-2)]

-- | combine solutions
dnc_combine =
  \ _ clos -> toClosure $ sum $ map unClosure clos

-- | trivial sequential case
dnc_f =
  \ clo_n -> toClosure <$> (force $ fib $ unClosure clo_n)
\end{haskellcode}
#+END_LATEX

*** Sum Euler

#+BEGIN_LATEX
\begin{haskellcode}{Sum Euler Benchmark Implementations}{lst:sumeuler-impl}
-- | sequential Euler's totient function
totient :: Int -> Integer
totient n = toInteger $ length $ filter (\ k -> gcd n k == 1) [1 .. n]

-- | lazy parallel-map skeleton with chunking
chunkfarm_sum_totient :: Int -> Int -> Int -> Par Integer
chunkfarm_sum_totient lower upper chunksize =
  sum <$> parMapChunkedNF chunksize $(mkClosure [| totient |]) list
    where
      list = [upper, upper - 1 @..@ lower] :: [Int]

-- | eager parallel-map skeleton with chunking
chunkfarm_push_sum_totient :: Int -> Int -> Int -> Par Integer
chunkfarm_push_sum_totient lower upper chunksize = do
  nodes <- allNodes
  sum <$> pushMapChunkedNF nodes chunksize $(mkClosure [| totient |]) list
    where
      list = [upper, upper - 1 @..@ lower] :: [Int]

-- | fault tolerant lazy parallel-map skeleton with chunking
ft_chunkfarm_sum_totient :: Int -> Int -> Int -> Par Integer
ft_chunkfarm_sum_totient lower upper chunksize =
  sum <$> FT.parMapChunkedNF chunksize $(mkClosure [| totient |]) list
    where
      list = [upper, upper - 1 @..@ lower] :: [Int]

-- | fault tolerant eager parallel-map skeleton with chunking
ft_chunkfarm_push_sum_totient :: Int -> Int -> Int -> Par Integer
ft_chunkfarm_push_sum_totient lower upper chunksize =
  sum <$> FT.pushMapChunkedNF chunksize $(mkClosure [| totient |]) list
    where
      list = [upper, upper - 1 @..@ lower] :: [Int]
\end{haskellcode}
#+END_LATEX

*** Summatory Liouville

#+BEGIN_LATEX
\begin{haskellcode}{Summatory Liouville Benchmark Implementations}{lst:summatoryliouville-impl}
-- | Liouville function
liouville :: Integer -> Int
liouville n
  | n == 1 = 1
  | length (primeFactors n) `mod` 2 == 0 = 1
  | otherwise = -1

-- | Summatory Liouville function from 1 to specified Integer
summatoryLiouville :: Integer -> Integer
summatoryLiouville x = sumLEvalChunk (1,x)

-- | sequential sum of Liouville values between two Integers.
sumLEvalChunk :: (Integer,Integer) -> Integer
sumLEvalChunk (lower,upper) =
     let chunkSize = 1000
         smp_chunks = chunk chunkSize [lower,lower+1..upper] :: [[Integer]]
         tuples      = map (head Control.Arrow.&&& last) smp_chunks
     in sum $ map (\(lower,upper) -> sumLEvalChunk' lower upper 0) tuples

-- | accumulative Summatory Liouville helper
sumLEvalChunk' :: Integer -> Integer -> Integer -> Integer
sumLEvalChunk' lower upper total
  | lower > upper = total
  | otherwise = let s = toInteger $ liouville lower
                in sumLEvalChunk' (lower+1) upper (total+s)

-- | lazy parallel-map skeleton with slicing
farmParSumLiouvilleSliced :: Integer -> Int -> Par Integer
farmParSumLiouvilleSliced x chunkSize = do
  let chunked = chunkedList x chunkSize
  sum <$> parMapSlicedNF chunkSize
           $(mkClosure [|sumLEvalChunk|])
           chunked

-- | eager parallel-map skeleton with slicing
farmPushSumLiouvilleSliced :: Integer -> Int -> Par Integer
farmPushSumLiouvilleSliced x chunkSize = do
  let chunked = chunkedList x chunkSize
  nodes <- allNodes
  sum <$> pushMapSlicedNF nodes chunkSize
           $(mkClosure [|sumLEvalChunk|])
           chunked

-- | fault tolerant lazy parallel-map skeleton with slicing
ft_farmParSumLiouvilleSliced :: Integer -> Int -> Par Integer
ft_farmParSumLiouvilleSliced x chunkSize = do
  let chunked = chunkedList x chunkSize
  sum <$> FT.parMapSlicedNF chunkSize
           $(mkClosure [|sumLEvalChunk|])
           chunked

-- | fault tolerant eager parallel-map skeleton with slicing
ft_farmPushSumLiouvilleSliced :: Integer -> Int -> Par Integer
ft_farmPushSumLiouvilleSliced x chunkSize = do
  let chunked = chunkedList x chunkSize
  sum <$> FT.pushMapSlicedNF chunkSize
           $(mkClosure [|sumLEvalChunk|])
           chunked
\end{haskellcode}
#+END_LATEX

*** Queens

#+BEGIN_LATEX
\begin{haskellcode}{Queens Benchmark Implementations}{lst:queens-impl}
-- | monad-par implementation of Queens
monadpar_queens :: Int -> Int -> MonadPar.Par [[Int]]
monadpar_queens nq threshold = step 0 []
  where
    step :: Int -> [Int] -> MonadPar.Par [[Int]]
    step !n b
       | n >= threshold = return (iterate (gen nq) [b] !! (nq - n))
       | otherwise = do
          rs <- MonadPar.C.parMapM (step (n+1)) (gen nq [b])
          return (concat rs)

-- | compute Queens solutions using accumulator
dist_queens = dist_queens' 0 []

-- | Porting monad-par solution to HdpH-RS
dist_queens' :: Int -> [Int] -> Int -> Int -> Par [[Int]]
dist_queens' !n b nq threshold
  | n >= threshold = force $ iterate (gen nq) [b] !! (nq - n)
  | otherwise = do
      let n' = n+1
      vs <- mapM (\b' -> spawn
                           $(mkClosure [| dist_queens_abs (n',b',nq,threshold) |]))
                           (gen nq [b])
      rs <- mapM (get) vs
      force $ concatMap unClosure rs

-- | function closure
dist_queens_abs :: (Int,[Int],Int,Int) -> Par (Closure [[Int]])
dist_queens_abs (n,b,nq,threshold) =
  dist_queens' n b nq threshold >>= return . toClosure

-- | HdpH-RS solution using threads for execution on one-node
dist_skel_fork_queens :: Int -> Int -> Par [[Int]]
dist_skel_fork_queens nq threshold = skel (0,nq,[])
  where
    skel = forkDivideAndConquer trivial decompose combine f
    trivial (n',_,_) = n' >= threshold
    decompose (n',nq',b') = map (\b'' -> (n'+1,nq',b'')) (gen nq' [b'])
    combine _ a = concat a
    f (n',nq',b') = force (iterate (gen nq') [b'] !! (nq' - n'))

-- | compute Queens solutions using accumulator with fault tolerance
ft_dist_queens = ft_dist_queens' 0 []

-- | Porting monad-par solution to HdpH-RS with fault fault tolerance
ft_dist_queens' :: Int -> [Int] -> Int -> Int -> Par [[Int]]
ft_dist_queens' !n b nq threshold
  | n >= threshold = force $ iterate (gen nq) [b] !! (nq - n)
  | otherwise = do
      let n' = n+1
      vs <- mapM (\b' -> supervisedSpawn
                           $(mkClosure [| dist_queens_abs (n',b',nq,threshold) |]))
                           (gen nq [b])
      rs <- mapM (get) vs
      force $ concatMap unClosure rs

-- | function closure
ft_dist_queens_abs :: (Int,[Int],Int,Int) -> Par (Closure [[Int]])
ft_dist_queens_abs (n,b,nq,threshold) =
  ft_dist_queens' n b nq threshold >>= return . toClosure

-- | lazy divide-and-conquer skeleton
dist_skel_par_queens :: Int -> Int -> Par [[Int]]
dist_skel_par_queens nq threshold = unClosure <$> skel (toClosure (0,nq,[]))
  where
    skel = parDivideAndConquer
             $(mkClosure [| dnc_trivial_abs (threshold) |])
             $(mkClosure [| dnc_decompose |])
             $(mkClosure [| dnc_combine |])
             $(mkClosure [| dnc_f |])

-- | eager divide-and-conquer skeleton
dist_skel_push_queens :: Int -> Int -> Par [[Int]]
dist_skel_push_queens nq threshold = unClosure <$> skel (toClosure (0,nq,[]))
  where
    skel x_clo = do
            nodes <- allNodes
            pushDivideAndConquer
             nodes
             $(mkClosure [| dnc_trivial_abs (threshold) |])
             $(mkClosure [| dnc_decompose |])
             $(mkClosure [| dnc_combine |])
             $(mkClosure [| dnc_f |])
             x_clo

-- | fault tolerant lazy divide-and-conquer skeleton
ft_dist_skel_par_queens :: Int -> Int -> Par [[Int]]
ft_dist_skel_par_queens nq threshold = unClosure <$> skel (toClosure (0,nq,[]))
  where
    skel = FT.parDivideAndConquer
             $(mkClosure [| dnc_trivial_abs (threshold) |])
             $(mkClosure [| dnc_decompose |])
             $(mkClosure [| dnc_combine |])
             $(mkClosure [| dnc_f |])

-- | fault tolerance eager divide-and-conquer skeleton
ft_dist_skel_push_queens :: Int -> Int -> Par [[Int]]
ft_dist_skel_push_queens nq threshold = unClosure <$> skel (toClosure (0,nq,[]))
  where
    skel = FT.pushDivideAndConquer
             $(mkClosure [| dnc_trivial_abs (threshold) |])
             $(mkClosure [| dnc_decompose |])
             $(mkClosure [| dnc_combine |])
             $(mkClosure [| dnc_f |])

-- | is trivial case
dnc_trivial_abs :: (Int) -> (Closure  (Int,Int,[Int]) -> Bool)
dnc_trivial_abs threshold =
  \ clo_n -> let (!n,_nq,_b) = unClosure clo_n in n >= threshold

-- | decompose problem
dnc_decompose :: Closure (Int, Int, [Int]) -> [Closure (Int, Int, [Int])]
dnc_decompose =
  \ clo_n -> let (n,nq,b) = unClosure clo_n
             in map (\b' -> toClosure (n+1,nq,b')) (gen nq [b])

-- | combine solutions
dnc_combine ::  a -> [Closure [[Int]]] -> Closure [[Int]]
dnc_combine =
  \ _ clos -> toClosure $ concatMap unClosure clos

-- | trivial sequential case
dnc_f :: Closure (Int, Int, [Int]) -> Par (Closure [[Int]])
dnc_f =
  \ clo_n -> do
       let (n,nq,b) = unClosure clo_n
       toClosure <$> force (iterate (gen nq) [b] !! (nq - n))
\end{haskellcode}
#+END_LATEX

*** Mandelbrot

#+BEGIN_LATEX
\begin{haskellcode}{Mandelbrot Benchmark Implementations}{lst:mandelbrot-impl}
-- | sequential implementation of Mandelbrot
mandel :: Int -> Complex Double -> Int
mandel max_depth c = loop 0 0
  where
   fn = magnitude
   loop i !z
    | i == max_depth = i
    | fn(z) >= 2.0   = i
    | otherwise      = loop (i+1) (z*z + c)

-- | lazy map-reduce skeleton
runMandelPar
  :: Double -> Double -> Double -> Double -> Int -> Int -> Int -> Int -> Par VecTree
runMandelPar minX minY maxX maxY winX winY maxDepth threshold =
    unClosure <$> skel (toClosure (Leaf V.empty))
  where
    skel = parMapReduceRangeThresh
            (toClosure threshold)
            (toClosure (InclusiveRange 0 (winY-1)))
            $(mkClosure [| map_f (minX,minY,maxX,maxY,winX,winY,maxDepth) |])
            $(mkClosure [| reduce_f |])

-- | eager map-reduce skeleton
runMandelPush
  :: Double -> Double -> Double -> Double -> Int -> Int -> Int -> Int -> Par VecTree
runMandelPush minX minY maxX maxY winX winY maxDepth threshold =
    unClosure <$> skel (toClosure (Leaf V.empty))
  where
    skel = pushMapReduceRangeThresh
            (toClosure threshold)
            (toClosure (InclusiveRange 0 (winY-1)))
            $(mkClosure [| map_f (minX,minY,maxX,maxY,winX,winY,maxDepth) |])
            $(mkClosure [| reduce_f |])

-- | fault tolerant lazy map-reduce skeleton
runMandelParFT
  :: Double -> Double -> Double -> Double -> Int -> Int -> Int -> Int -> Par VecTree
runMandelParFT minX minY maxX maxY winX winY maxDepth threshold =
    unClosure <$> skel (toClosure (Leaf V.empty))
  where
    skel = FT.parMapReduceRangeThresh
            (toClosure threshold)
            (toClosure (FT.InclusiveRange 0 (winY-1)))
            $(mkClosure [| map_f (minX,minY,maxX,maxY,winX,winY,maxDepth) |])
            $(mkClosure [| reduce_f |])

-- | eager map-reduce skeleton
runMandelPushFT
  :: Double -> Double -> Double -> Double -> Int -> Int -> Int -> Int -> Par VecTree
runMandelPushFT minX minY maxX maxY winX winY maxDepth threshold =
    unClosure <$> skel (toClosure (Leaf V.empty))
  where
    skel = FT.pushMapReduceRangeThresh
            (toClosure threshold)
            (toClosure (FT.InclusiveRange 0 (winY-1)))
            $(mkClosure [| map_f (minX,minY,maxX,maxY,winX,winY,maxDepth) |])
            $(mkClosure [| reduce_f |])

------------------------------------------------                                                                                                                                                               
-- Map and Reduce function closure implementations                                                                                                                                                             

-- | implementation of map function
map_f :: (Double,Double,Double,Double,Int,Int,Int)
      -> Closure Int
      -> Par (Closure VecTree)
map_f (minX,minY,maxX,maxY,winX,winY,maxDepth) = \y_clo -> do
    let y = unClosure y_clo
    let vec = V.generate winX (\x -> mandelStep y x)
    seq (vec V.! 0) $ return (toClosure (Leaf vec))
  where
    mandelStep i j = mandel maxDepth (calcZ i j)
    calcZ i j = ((fromIntegral j * r_scale) / fromIntegral winY + minY) :+
           ((fromIntegral i * c_scale) / fromIntegral winX + minX)
    r_scale =  maxY - minY  :: Double
    c_scale =   maxX - minX  :: Double

-- | implementation of reduce function
reduce_f :: Closure VecTree -> Closure VecTree -> Par (Closure VecTree)
reduce_f = \a_clo b_clo -> return $ toClosure (MkNode (unClosure a_clo) (unClosure b_clo))
\end{haskellcode}
#+END_LATEX
